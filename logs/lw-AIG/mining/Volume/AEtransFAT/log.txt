Namespace(seed=15, model='AEtransGAT', dataset='mining/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Volume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 124], edge_attr=[124, 2], x=[30, 14887], y=[1, 1], num_nodes=32)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2adbb940>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.9034;  Loss pred: 3.9034; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 3.9436;  Loss pred: 3.9436; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 3.9638;  Loss pred: 3.9638; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 3.9749;  Loss pred: 3.9749; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 3.8939;  Loss pred: 3.8939; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 3.8372;  Loss pred: 3.8372; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 3.7923;  Loss pred: 3.7923; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 3.7693;  Loss pred: 3.7693; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 3.6630;  Loss pred: 3.6630; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 3.6197;  Loss pred: 3.6197; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 3.6107;  Loss pred: 3.6107; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 3.4444;  Loss pred: 3.4444; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 3.4441;  Loss pred: 3.4441; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 3.3748;  Loss pred: 3.3748; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 3.4170;  Loss pred: 3.4170; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 3.3191;  Loss pred: 3.3191; Loss self: 0.0000; time: 4.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.80s
Epoch 17/1000, LR 0.000270
Train loss: 3.2873;  Loss pred: 3.2873; Loss self: 0.0000; time: 4.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.96s
Epoch 18/1000, LR 0.000270
Train loss: 3.1990;  Loss pred: 3.1990; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 3.1289;  Loss pred: 3.1289; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 20/1000, LR 0.000270
Train loss: 3.1205;  Loss pred: 3.1205; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 017,   Train_Loss: 3.1990,   Val_Loss: 0.6934,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6934,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6932


[0.049124801997095346, 0.049941986915655434, 0.05001039104536176, 0.05000575305894017, 0.04985263501293957, 0.054238114040344954, 0.049600558006204665, 0.04968592105433345, 0.04960505000781268, 0.0642840500222519, 0.06257685902528465, 0.04993117100093514, 0.05009627097751945, 0.05076947109773755, 0.04965444398112595, 0.8010479350341484, 0.9624789939261973, 0.051789202028885484, 0.05047673103399575, 0.050415270030498505]
[0.001116472772661258, 0.001135045157173987, 0.0011365997964854944, 0.0011364943877031858, 0.001133014432112263, 0.00123268441000784, 0.0011272854092319242, 0.0011292254785075784, 0.001127387500177561, 0.0014610011368693615, 0.001422201341483742, 0.001134799340930344, 0.0011385516131254421, 0.0011538516158576715, 0.0011285100904801352, 0.018205634887139735, 0.021874522589231758, 0.0011770273188383064, 0.0011471984325908124, 0.0011458015916022387]
[895.6779103679977, 881.0222163228996, 879.817155600522, 879.8987578116985, 882.6012905552438, 811.2376467823099, 887.0867943561426, 885.5627321849246, 887.0064639199054, 684.4621641724411, 703.1353232705635, 881.2130602580089, 878.3088868978864, 866.6625641085471, 886.1241103963373, 54.9280487167404, 45.71528342713515, 849.5979523966976, 871.6887781494069, 872.7514495783199]
Elapsed: 0.1347792804648634~0.2503323882437933
Time per graph: 0.003063165465110532~0.00568937246008621
Speed: 774.2249294636864~247.88115676316627
Total Time: 0.0507
best val loss: 0.6933856010437012 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.05s
test Score 0.5000
Epoch Time List: [0.5091898591490462, 0.21777532389387488, 0.22003529395442456, 0.22011822497006506, 0.2203031408134848, 0.22476634592749178, 0.2191435620188713, 0.21867023990489542, 0.21942473202943802, 0.24040018604137003, 0.23323823499958962, 0.22454964509233832, 0.22136349906213582, 0.22192379494663328, 0.22051425103563815, 5.942992649972439, 5.878618599846959, 1.239305712049827, 0.2249449238879606, 0.2225545340916142]
Total Epoch List: [20]
Total Time List: [0.050746947061270475]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2ad70a60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.6898;  Loss pred: 2.6898; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.6630;  Loss pred: 2.6630; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.4884 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.5373;  Loss pred: 2.5373; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.4884 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.6076;  Loss pred: 2.6076; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4884 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.5993;  Loss pred: 2.5993; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4884 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.5531;  Loss pred: 2.5531; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.5499;  Loss pred: 2.5499; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.5684;  Loss pred: 2.5684; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 2.4852;  Loss pred: 2.4852; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.4860;  Loss pred: 2.4860; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 2.3904;  Loss pred: 2.3904; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 2.3948;  Loss pred: 2.3948; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 2.3552;  Loss pred: 2.3552; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 2.3085;  Loss pred: 2.3085; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 2.2835;  Loss pred: 2.2835; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 2.2654;  Loss pred: 2.2654; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 2.2132;  Loss pred: 2.2132; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 2.1842;  Loss pred: 2.1842; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 2.1579;  Loss pred: 2.1579; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 2.1412;  Loss pred: 2.1412; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 2.0976;  Loss pred: 2.0976; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 2.0638;  Loss pred: 2.0638; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 2.0078;  Loss pred: 2.0078; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 2.0172;  Loss pred: 2.0172; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 1.9696;  Loss pred: 1.9696; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 1.9084;  Loss pred: 1.9084; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 1.9119;  Loss pred: 1.9119; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 1.8700;  Loss pred: 1.8700; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 1.8878;  Loss pred: 1.8878; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 1.8594;  Loss pred: 1.8594; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 1.8267;  Loss pred: 1.8267; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 1.7737;  Loss pred: 1.7737; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 1.7457;  Loss pred: 1.7457; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 1.7553;  Loss pred: 1.7553; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 1.6958;  Loss pred: 1.6958; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 1.7163;  Loss pred: 1.7163; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 1.6907;  Loss pred: 1.6907; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 1.6904;  Loss pred: 1.6904; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 1.6366;  Loss pred: 1.6366; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 1.6138;  Loss pred: 1.6138; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 1.6032;  Loss pred: 1.6032; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 1.6220;  Loss pred: 1.6220; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 1.5981;  Loss pred: 1.5981; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 1.5616;  Loss pred: 1.5616; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 1.5468;  Loss pred: 1.5468; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 1.5464;  Loss pred: 1.5464; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 1.4920;  Loss pred: 1.4920; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 1.4833;  Loss pred: 1.4833; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 1.4655;  Loss pred: 1.4655; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 1.4480;  Loss pred: 1.4480; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 1.4352;  Loss pred: 1.4352; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 1.4213;  Loss pred: 1.4213; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 1.4143;  Loss pred: 1.4143; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 1.3997;  Loss pred: 1.3997; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 1.3923;  Loss pred: 1.3923; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 1.3746;  Loss pred: 1.3746; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 1.3772;  Loss pred: 1.3772; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 1.3730;  Loss pred: 1.3730; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 1.3384;  Loss pred: 1.3384; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 1.3245;  Loss pred: 1.3245; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 1.3162;  Loss pred: 1.3162; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 1.3130;  Loss pred: 1.3130; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 1.3039;  Loss pred: 1.3039; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4884 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 1.2924;  Loss pred: 1.2924; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 1.89s
Epoch 65/1000, LR 0.000268
Train loss: 1.2869;  Loss pred: 1.2869; Loss self: 0.0000; time: 4.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 1.2676;  Loss pred: 1.2676; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4884 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 1.2631;  Loss pred: 1.2631; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4884 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 1.2472;  Loss pred: 1.2472; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.2522;  Loss pred: 1.2522; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 1.2457;  Loss pred: 1.2457; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 1.2283;  Loss pred: 1.2283; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 1.2237;  Loss pred: 1.2237; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4884 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 1.2104;  Loss pred: 1.2104; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4884 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 1.2154;  Loss pred: 1.2154; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4884 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 1.1997;  Loss pred: 1.1997; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.4884 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 1.1905;  Loss pred: 1.1905; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4884 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 1.1826;  Loss pred: 1.1826; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4884 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 1.1819;  Loss pred: 1.1819; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4884 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 1.1790;  Loss pred: 1.1790; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4884 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 1.1732;  Loss pred: 1.1732; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.4884 time: 0.05s
Epoch 81/1000, LR 0.000267
Train loss: 1.1621;  Loss pred: 1.1621; Loss self: 0.0000; time: 0.15s
Val loss: 0.6837 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.4884 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 1.1548;  Loss pred: 1.1548; Loss self: 0.0000; time: 0.13s
Val loss: 0.6833 score: 0.5455 time: 0.05s
Test loss: 0.6862 score: 0.5116 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 1.1509;  Loss pred: 1.1509; Loss self: 0.0000; time: 0.14s
Val loss: 0.6829 score: 0.5682 time: 0.04s
Test loss: 0.6859 score: 0.5116 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.1451;  Loss pred: 1.1451; Loss self: 0.0000; time: 0.14s
Val loss: 0.6824 score: 0.5909 time: 0.04s
Test loss: 0.6855 score: 0.5349 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 1.1393;  Loss pred: 1.1393; Loss self: 0.0000; time: 0.14s
Val loss: 0.6819 score: 0.6136 time: 0.04s
Test loss: 0.6851 score: 0.5349 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 1.1267;  Loss pred: 1.1267; Loss self: 0.0000; time: 0.13s
Val loss: 0.6815 score: 0.6136 time: 0.04s
Test loss: 0.6847 score: 0.5349 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 1.1276;  Loss pred: 1.1276; Loss self: 0.0000; time: 0.13s
Val loss: 0.6810 score: 0.6136 time: 0.04s
Test loss: 0.6843 score: 0.5349 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 1.1190;  Loss pred: 1.1190; Loss self: 0.0000; time: 0.13s
Val loss: 0.6804 score: 0.6591 time: 0.04s
Test loss: 0.6839 score: 0.5581 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 1.1209;  Loss pred: 1.1209; Loss self: 0.0000; time: 0.14s
Val loss: 0.6799 score: 0.6591 time: 0.05s
Test loss: 0.6835 score: 0.5814 time: 0.04s
Epoch 90/1000, LR 0.000266
Train loss: 1.1098;  Loss pred: 1.1098; Loss self: 0.0000; time: 0.14s
Val loss: 0.6793 score: 0.6818 time: 0.04s
Test loss: 0.6831 score: 0.5814 time: 0.05s
Epoch 91/1000, LR 0.000266
Train loss: 1.1045;  Loss pred: 1.1045; Loss self: 0.0000; time: 0.14s
Val loss: 0.6788 score: 0.6818 time: 0.05s
Test loss: 0.6827 score: 0.5814 time: 0.04s
Epoch 92/1000, LR 0.000266
Train loss: 1.1034;  Loss pred: 1.1034; Loss self: 0.0000; time: 0.15s
Val loss: 0.6782 score: 0.6818 time: 0.04s
Test loss: 0.6822 score: 0.6047 time: 0.04s
Epoch 93/1000, LR 0.000265
Train loss: 1.1012;  Loss pred: 1.1012; Loss self: 0.0000; time: 0.14s
Val loss: 0.6776 score: 0.6818 time: 0.04s
Test loss: 0.6818 score: 0.6047 time: 0.04s
Epoch 94/1000, LR 0.000265
Train loss: 1.0900;  Loss pred: 1.0900; Loss self: 0.0000; time: 0.13s
Val loss: 0.6770 score: 0.6818 time: 0.04s
Test loss: 0.6813 score: 0.6047 time: 0.04s
Epoch 95/1000, LR 0.000265
Train loss: 1.0935;  Loss pred: 1.0935; Loss self: 0.0000; time: 0.13s
Val loss: 0.6763 score: 0.6818 time: 0.04s
Test loss: 0.6808 score: 0.6047 time: 0.04s
Epoch 96/1000, LR 0.000265
Train loss: 1.0904;  Loss pred: 1.0904; Loss self: 0.0000; time: 0.14s
Val loss: 0.6757 score: 0.6818 time: 0.04s
Test loss: 0.6803 score: 0.6047 time: 0.05s
Epoch 97/1000, LR 0.000265
Train loss: 1.0836;  Loss pred: 1.0836; Loss self: 0.0000; time: 0.13s
Val loss: 0.6750 score: 0.6818 time: 0.04s
Test loss: 0.6798 score: 0.6047 time: 0.04s
Epoch 98/1000, LR 0.000265
Train loss: 1.0886;  Loss pred: 1.0886; Loss self: 0.0000; time: 0.14s
Val loss: 0.6743 score: 0.6818 time: 0.05s
Test loss: 0.6793 score: 0.6047 time: 0.04s
Epoch 99/1000, LR 0.000265
Train loss: 1.0836;  Loss pred: 1.0836; Loss self: 0.0000; time: 0.14s
Val loss: 0.6736 score: 0.6818 time: 0.04s
Test loss: 0.6788 score: 0.6047 time: 0.04s
Epoch 100/1000, LR 0.000265
Train loss: 1.0705;  Loss pred: 1.0705; Loss self: 0.0000; time: 0.13s
Val loss: 0.6728 score: 0.7045 time: 0.04s
Test loss: 0.6782 score: 0.6279 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 1.0680;  Loss pred: 1.0680; Loss self: 0.0000; time: 0.13s
Val loss: 0.6721 score: 0.7273 time: 0.04s
Test loss: 0.6776 score: 0.6279 time: 0.04s
Epoch 102/1000, LR 0.000264
Train loss: 1.0632;  Loss pred: 1.0632; Loss self: 0.0000; time: 0.13s
Val loss: 0.6713 score: 0.7273 time: 0.04s
Test loss: 0.6770 score: 0.6279 time: 0.04s
Epoch 103/1000, LR 0.000264
Train loss: 1.0601;  Loss pred: 1.0601; Loss self: 0.0000; time: 0.14s
Val loss: 0.6704 score: 0.7500 time: 0.04s
Test loss: 0.6763 score: 0.6744 time: 0.04s
Epoch 104/1000, LR 0.000264
Train loss: 1.0575;  Loss pred: 1.0575; Loss self: 0.0000; time: 0.13s
Val loss: 0.6695 score: 0.7500 time: 0.04s
Test loss: 0.6756 score: 0.6744 time: 0.04s
Epoch 105/1000, LR 0.000264
Train loss: 1.0570;  Loss pred: 1.0570; Loss self: 0.0000; time: 0.13s
Val loss: 0.6686 score: 0.7500 time: 0.05s
Test loss: 0.6749 score: 0.6744 time: 0.05s
Epoch 106/1000, LR 0.000264
Train loss: 1.0490;  Loss pred: 1.0490; Loss self: 0.0000; time: 0.18s
Val loss: 0.6677 score: 0.7500 time: 0.05s
Test loss: 0.6742 score: 0.6744 time: 0.05s
Epoch 107/1000, LR 0.000264
Train loss: 1.0498;  Loss pred: 1.0498; Loss self: 0.0000; time: 0.14s
Val loss: 0.6668 score: 0.7500 time: 0.04s
Test loss: 0.6735 score: 0.6744 time: 0.05s
Epoch 108/1000, LR 0.000264
Train loss: 1.0429;  Loss pred: 1.0429; Loss self: 0.0000; time: 0.78s
Val loss: 0.6659 score: 0.7500 time: 0.24s
Test loss: 0.6728 score: 0.6744 time: 0.48s
Epoch 109/1000, LR 0.000264
Train loss: 1.0463;  Loss pred: 1.0463; Loss self: 0.0000; time: 2.30s
Val loss: 0.6649 score: 0.7500 time: 1.63s
Test loss: 0.6720 score: 0.6744 time: 0.83s
Epoch 110/1000, LR 0.000263
Train loss: 1.0411;  Loss pred: 1.0411; Loss self: 0.0000; time: 1.98s
Val loss: 0.6639 score: 0.7727 time: 0.77s
Test loss: 0.6712 score: 0.6744 time: 0.05s
Epoch 111/1000, LR 0.000263
Train loss: 1.0365;  Loss pred: 1.0365; Loss self: 0.0000; time: 0.14s
Val loss: 0.6628 score: 0.7727 time: 0.05s
Test loss: 0.6704 score: 0.6744 time: 0.04s
Epoch 112/1000, LR 0.000263
Train loss: 1.0366;  Loss pred: 1.0366; Loss self: 0.0000; time: 0.13s
Val loss: 0.6618 score: 0.7727 time: 0.04s
Test loss: 0.6696 score: 0.6744 time: 0.04s
Epoch 113/1000, LR 0.000263
Train loss: 1.0291;  Loss pred: 1.0291; Loss self: 0.0000; time: 0.13s
Val loss: 0.6607 score: 0.7727 time: 0.05s
Test loss: 0.6688 score: 0.6977 time: 0.05s
Epoch 114/1000, LR 0.000263
Train loss: 1.0268;  Loss pred: 1.0268; Loss self: 0.0000; time: 0.15s
Val loss: 0.6596 score: 0.7727 time: 0.05s
Test loss: 0.6679 score: 0.6977 time: 0.05s
Epoch 115/1000, LR 0.000263
Train loss: 1.0218;  Loss pred: 1.0218; Loss self: 0.0000; time: 0.14s
Val loss: 0.6584 score: 0.7727 time: 0.05s
Test loss: 0.6670 score: 0.6977 time: 0.05s
Epoch 116/1000, LR 0.000263
Train loss: 1.0173;  Loss pred: 1.0173; Loss self: 0.0000; time: 0.14s
Val loss: 0.6572 score: 0.7955 time: 0.05s
Test loss: 0.6661 score: 0.6977 time: 0.05s
Epoch 117/1000, LR 0.000262
Train loss: 1.0138;  Loss pred: 1.0138; Loss self: 0.0000; time: 0.14s
Val loss: 0.6560 score: 0.7955 time: 0.05s
Test loss: 0.6652 score: 0.7209 time: 0.05s
Epoch 118/1000, LR 0.000262
Train loss: 1.0192;  Loss pred: 1.0192; Loss self: 0.0000; time: 0.14s
Val loss: 0.6548 score: 0.7955 time: 0.05s
Test loss: 0.6642 score: 0.7209 time: 0.05s
Epoch 119/1000, LR 0.000262
Train loss: 1.0123;  Loss pred: 1.0123; Loss self: 0.0000; time: 0.14s
Val loss: 0.6535 score: 0.7955 time: 0.05s
Test loss: 0.6632 score: 0.7209 time: 0.05s
Epoch 120/1000, LR 0.000262
Train loss: 1.0123;  Loss pred: 1.0123; Loss self: 0.0000; time: 0.14s
Val loss: 0.6522 score: 0.7955 time: 0.05s
Test loss: 0.6622 score: 0.7209 time: 0.05s
Epoch 121/1000, LR 0.000262
Train loss: 1.0072;  Loss pred: 1.0072; Loss self: 0.0000; time: 0.14s
Val loss: 0.6508 score: 0.7955 time: 0.05s
Test loss: 0.6611 score: 0.7209 time: 0.05s
Epoch 122/1000, LR 0.000262
Train loss: 1.0044;  Loss pred: 1.0044; Loss self: 0.0000; time: 0.14s
Val loss: 0.6494 score: 0.8182 time: 0.04s
Test loss: 0.6600 score: 0.7209 time: 0.04s
Epoch 123/1000, LR 0.000262
Train loss: 0.9983;  Loss pred: 0.9983; Loss self: 0.0000; time: 0.13s
Val loss: 0.6480 score: 0.8182 time: 0.04s
Test loss: 0.6589 score: 0.7442 time: 0.04s
Epoch 124/1000, LR 0.000261
Train loss: 0.9973;  Loss pred: 0.9973; Loss self: 0.0000; time: 0.13s
Val loss: 0.6465 score: 0.8182 time: 0.04s
Test loss: 0.6577 score: 0.7674 time: 0.04s
Epoch 125/1000, LR 0.000261
Train loss: 0.9913;  Loss pred: 0.9913; Loss self: 0.0000; time: 0.13s
Val loss: 0.6450 score: 0.8182 time: 0.04s
Test loss: 0.6565 score: 0.7907 time: 0.04s
Epoch 126/1000, LR 0.000261
Train loss: 0.9930;  Loss pred: 0.9930; Loss self: 0.0000; time: 0.13s
Val loss: 0.6435 score: 0.8182 time: 0.04s
Test loss: 0.6553 score: 0.8140 time: 0.04s
Epoch 127/1000, LR 0.000261
Train loss: 0.9906;  Loss pred: 0.9906; Loss self: 0.0000; time: 0.13s
Val loss: 0.6420 score: 0.8182 time: 0.04s
Test loss: 0.6541 score: 0.8140 time: 0.04s
Epoch 128/1000, LR 0.000261
Train loss: 0.9850;  Loss pred: 0.9850; Loss self: 0.0000; time: 0.13s
Val loss: 0.6404 score: 0.8182 time: 0.04s
Test loss: 0.6529 score: 0.8140 time: 0.04s
Epoch 129/1000, LR 0.000261
Train loss: 0.9850;  Loss pred: 0.9850; Loss self: 0.0000; time: 0.13s
Val loss: 0.6388 score: 0.8182 time: 0.04s
Test loss: 0.6517 score: 0.8140 time: 0.04s
Epoch 130/1000, LR 0.000260
Train loss: 0.9791;  Loss pred: 0.9791; Loss self: 0.0000; time: 0.13s
Val loss: 0.6372 score: 0.8182 time: 0.04s
Test loss: 0.6504 score: 0.8140 time: 0.04s
Epoch 131/1000, LR 0.000260
Train loss: 0.9778;  Loss pred: 0.9778; Loss self: 0.0000; time: 0.13s
Val loss: 0.6356 score: 0.8182 time: 0.04s
Test loss: 0.6491 score: 0.8140 time: 0.04s
Epoch 132/1000, LR 0.000260
Train loss: 0.9763;  Loss pred: 0.9763; Loss self: 0.0000; time: 0.13s
Val loss: 0.6339 score: 0.8182 time: 0.04s
Test loss: 0.6478 score: 0.8140 time: 0.04s
Epoch 133/1000, LR 0.000260
Train loss: 0.9714;  Loss pred: 0.9714; Loss self: 0.0000; time: 0.13s
Val loss: 0.6322 score: 0.8182 time: 0.04s
Test loss: 0.6465 score: 0.8140 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 0.9677;  Loss pred: 0.9677; Loss self: 0.0000; time: 0.13s
Val loss: 0.6304 score: 0.8182 time: 0.04s
Test loss: 0.6452 score: 0.8140 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9653;  Loss pred: 0.9653; Loss self: 0.0000; time: 0.13s
Val loss: 0.6286 score: 0.8182 time: 0.04s
Test loss: 0.6438 score: 0.8140 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9620;  Loss pred: 0.9620; Loss self: 0.0000; time: 0.13s
Val loss: 0.6269 score: 0.8182 time: 0.04s
Test loss: 0.6424 score: 0.8140 time: 0.04s
Epoch 137/1000, LR 0.000259
Train loss: 0.9605;  Loss pred: 0.9605; Loss self: 0.0000; time: 0.13s
Val loss: 0.6250 score: 0.8182 time: 0.04s
Test loss: 0.6410 score: 0.8140 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9567;  Loss pred: 0.9567; Loss self: 0.0000; time: 0.13s
Val loss: 0.6231 score: 0.8182 time: 0.04s
Test loss: 0.6395 score: 0.8140 time: 0.04s
Epoch 139/1000, LR 0.000259
Train loss: 0.9519;  Loss pred: 0.9519; Loss self: 0.0000; time: 0.13s
Val loss: 0.6212 score: 0.8182 time: 0.04s
Test loss: 0.6380 score: 0.8140 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9503;  Loss pred: 0.9503; Loss self: 0.0000; time: 0.13s
Val loss: 0.6192 score: 0.8182 time: 0.04s
Test loss: 0.6364 score: 0.8140 time: 0.04s
Epoch 141/1000, LR 0.000259
Train loss: 0.9469;  Loss pred: 0.9469; Loss self: 0.0000; time: 0.13s
Val loss: 0.6172 score: 0.8182 time: 0.04s
Test loss: 0.6348 score: 0.8140 time: 0.04s
Epoch 142/1000, LR 0.000259
Train loss: 0.9409;  Loss pred: 0.9409; Loss self: 0.0000; time: 0.13s
Val loss: 0.6151 score: 0.8182 time: 0.04s
Test loss: 0.6332 score: 0.8140 time: 0.04s
Epoch 143/1000, LR 0.000258
Train loss: 0.9424;  Loss pred: 0.9424; Loss self: 0.0000; time: 0.13s
Val loss: 0.6130 score: 0.8182 time: 0.04s
Test loss: 0.6315 score: 0.8140 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9417;  Loss pred: 0.9417; Loss self: 0.0000; time: 0.13s
Val loss: 0.6109 score: 0.8182 time: 0.04s
Test loss: 0.6298 score: 0.8140 time: 0.04s
Epoch 145/1000, LR 0.000258
Train loss: 0.9326;  Loss pred: 0.9326; Loss self: 0.0000; time: 0.13s
Val loss: 0.6087 score: 0.8182 time: 0.04s
Test loss: 0.6281 score: 0.7907 time: 0.04s
Epoch 146/1000, LR 0.000258
Train loss: 0.9345;  Loss pred: 0.9345; Loss self: 0.0000; time: 0.13s
Val loss: 0.6065 score: 0.8182 time: 0.04s
Test loss: 0.6263 score: 0.7907 time: 0.04s
Epoch 147/1000, LR 0.000258
Train loss: 0.9291;  Loss pred: 0.9291; Loss self: 0.0000; time: 0.13s
Val loss: 0.6043 score: 0.8182 time: 0.04s
Test loss: 0.6245 score: 0.7907 time: 0.04s
Epoch 148/1000, LR 0.000257
Train loss: 0.9274;  Loss pred: 0.9274; Loss self: 0.0000; time: 0.13s
Val loss: 0.6020 score: 0.8182 time: 0.04s
Test loss: 0.6226 score: 0.7907 time: 0.04s
Epoch 149/1000, LR 0.000257
Train loss: 0.9245;  Loss pred: 0.9245; Loss self: 0.0000; time: 0.13s
Val loss: 0.5997 score: 0.8182 time: 0.04s
Test loss: 0.6208 score: 0.7907 time: 0.04s
Epoch 150/1000, LR 0.000257
Train loss: 0.9212;  Loss pred: 0.9212; Loss self: 0.0000; time: 0.13s
Val loss: 0.5974 score: 0.8182 time: 0.04s
Test loss: 0.6189 score: 0.7907 time: 0.04s
Epoch 151/1000, LR 0.000257
Train loss: 0.9166;  Loss pred: 0.9166; Loss self: 0.0000; time: 0.13s
Val loss: 0.5950 score: 0.8182 time: 0.04s
Test loss: 0.6170 score: 0.7907 time: 0.04s
Epoch 152/1000, LR 0.000257
Train loss: 0.9130;  Loss pred: 0.9130; Loss self: 0.0000; time: 0.13s
Val loss: 0.5926 score: 0.8182 time: 0.04s
Test loss: 0.6151 score: 0.7907 time: 0.04s
Epoch 153/1000, LR 0.000257
Train loss: 0.9099;  Loss pred: 0.9099; Loss self: 0.0000; time: 0.13s
Val loss: 0.5902 score: 0.8182 time: 0.04s
Test loss: 0.6131 score: 0.7907 time: 0.04s
Epoch 154/1000, LR 0.000256
Train loss: 0.9109;  Loss pred: 0.9109; Loss self: 0.0000; time: 0.13s
Val loss: 0.5878 score: 0.8182 time: 0.04s
Test loss: 0.6111 score: 0.7907 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9033;  Loss pred: 0.9033; Loss self: 0.0000; time: 3.12s
Val loss: 0.5854 score: 0.8409 time: 0.31s
Test loss: 0.6091 score: 0.7907 time: 0.53s
Epoch 156/1000, LR 0.000256
Train loss: 0.9038;  Loss pred: 0.9038; Loss self: 0.0000; time: 1.38s
Val loss: 0.5829 score: 0.8409 time: 0.36s
Test loss: 0.6071 score: 0.8140 time: 0.27s
Epoch 157/1000, LR 0.000256
Train loss: 0.8993;  Loss pred: 0.8993; Loss self: 0.0000; time: 0.65s
Val loss: 0.5804 score: 0.8409 time: 0.21s
Test loss: 0.6051 score: 0.8140 time: 0.37s
Epoch 158/1000, LR 0.000256
Train loss: 0.8985;  Loss pred: 0.8985; Loss self: 0.0000; time: 0.95s
Val loss: 0.5779 score: 0.8409 time: 0.11s
Test loss: 0.6031 score: 0.8140 time: 0.18s
Epoch 159/1000, LR 0.000255
Train loss: 0.8958;  Loss pred: 0.8958; Loss self: 0.0000; time: 0.16s
Val loss: 0.5754 score: 0.8409 time: 0.05s
Test loss: 0.6011 score: 0.8140 time: 0.06s
Epoch 160/1000, LR 0.000255
Train loss: 0.8888;  Loss pred: 0.8888; Loss self: 0.0000; time: 0.15s
Val loss: 0.5730 score: 0.8409 time: 0.06s
Test loss: 0.5991 score: 0.8140 time: 0.05s
Epoch 161/1000, LR 0.000255
Train loss: 0.8847;  Loss pred: 0.8847; Loss self: 0.0000; time: 0.14s
Val loss: 0.5705 score: 0.8409 time: 0.05s
Test loss: 0.5971 score: 0.8140 time: 0.04s
Epoch 162/1000, LR 0.000255
Train loss: 0.8816;  Loss pred: 0.8816; Loss self: 0.0000; time: 0.13s
Val loss: 0.5679 score: 0.8409 time: 0.04s
Test loss: 0.5950 score: 0.8140 time: 0.04s
Epoch 163/1000, LR 0.000255
Train loss: 0.8786;  Loss pred: 0.8786; Loss self: 0.0000; time: 0.13s
Val loss: 0.5654 score: 0.8409 time: 0.05s
Test loss: 0.5929 score: 0.7907 time: 0.04s
Epoch 164/1000, LR 0.000254
Train loss: 0.8768;  Loss pred: 0.8768; Loss self: 0.0000; time: 0.13s
Val loss: 0.5628 score: 0.8409 time: 0.05s
Test loss: 0.5908 score: 0.8140 time: 0.04s
Epoch 165/1000, LR 0.000254
Train loss: 0.8742;  Loss pred: 0.8742; Loss self: 0.0000; time: 0.13s
Val loss: 0.5601 score: 0.8409 time: 0.05s
Test loss: 0.5886 score: 0.8140 time: 0.04s
Epoch 166/1000, LR 0.000254
Train loss: 0.8715;  Loss pred: 0.8715; Loss self: 0.0000; time: 0.13s
Val loss: 0.5574 score: 0.8409 time: 0.04s
Test loss: 0.5864 score: 0.8140 time: 0.05s
Epoch 167/1000, LR 0.000254
Train loss: 0.8663;  Loss pred: 0.8663; Loss self: 0.0000; time: 0.13s
Val loss: 0.5547 score: 0.8409 time: 0.05s
Test loss: 0.5841 score: 0.8140 time: 0.04s
Epoch 168/1000, LR 0.000254
Train loss: 0.8602;  Loss pred: 0.8602; Loss self: 0.0000; time: 0.13s
Val loss: 0.5520 score: 0.8409 time: 0.04s
Test loss: 0.5818 score: 0.8140 time: 0.04s
Epoch 169/1000, LR 0.000253
Train loss: 0.8624;  Loss pred: 0.8624; Loss self: 0.0000; time: 0.14s
Val loss: 0.5492 score: 0.8409 time: 0.05s
Test loss: 0.5794 score: 0.8140 time: 0.04s
Epoch 170/1000, LR 0.000253
Train loss: 0.8562;  Loss pred: 0.8562; Loss self: 0.0000; time: 0.13s
Val loss: 0.5464 score: 0.8409 time: 0.04s
Test loss: 0.5770 score: 0.8140 time: 0.04s
Epoch 171/1000, LR 0.000253
Train loss: 0.8535;  Loss pred: 0.8535; Loss self: 0.0000; time: 0.13s
Val loss: 0.5436 score: 0.8409 time: 0.04s
Test loss: 0.5746 score: 0.8140 time: 0.04s
Epoch 172/1000, LR 0.000253
Train loss: 0.8544;  Loss pred: 0.8544; Loss self: 0.0000; time: 0.13s
Val loss: 0.5408 score: 0.8409 time: 0.04s
Test loss: 0.5722 score: 0.8140 time: 0.04s
Epoch 173/1000, LR 0.000253
Train loss: 0.8497;  Loss pred: 0.8497; Loss self: 0.0000; time: 0.13s
Val loss: 0.5381 score: 0.8409 time: 0.04s
Test loss: 0.5699 score: 0.8140 time: 0.04s
Epoch 174/1000, LR 0.000252
Train loss: 0.8461;  Loss pred: 0.8461; Loss self: 0.0000; time: 0.13s
Val loss: 0.5354 score: 0.8409 time: 0.04s
Test loss: 0.5676 score: 0.8140 time: 0.05s
Epoch 175/1000, LR 0.000252
Train loss: 0.8400;  Loss pred: 0.8400; Loss self: 0.0000; time: 0.13s
Val loss: 0.5328 score: 0.8409 time: 0.04s
Test loss: 0.5655 score: 0.8140 time: 0.04s
Epoch 176/1000, LR 0.000252
Train loss: 0.8394;  Loss pred: 0.8394; Loss self: 0.0000; time: 0.13s
Val loss: 0.5301 score: 0.8409 time: 0.04s
Test loss: 0.5632 score: 0.8140 time: 0.05s
Epoch 177/1000, LR 0.000252
Train loss: 0.8365;  Loss pred: 0.8365; Loss self: 0.0000; time: 0.13s
Val loss: 0.5274 score: 0.8409 time: 0.04s
Test loss: 0.5610 score: 0.8140 time: 0.04s
Epoch 178/1000, LR 0.000251
Train loss: 0.8325;  Loss pred: 0.8325; Loss self: 0.0000; time: 0.13s
Val loss: 0.5248 score: 0.8409 time: 0.04s
Test loss: 0.5587 score: 0.8140 time: 0.04s
Epoch 179/1000, LR 0.000251
Train loss: 0.8280;  Loss pred: 0.8280; Loss self: 0.0000; time: 0.13s
Val loss: 0.5221 score: 0.8409 time: 0.04s
Test loss: 0.5564 score: 0.8140 time: 0.04s
Epoch 180/1000, LR 0.000251
Train loss: 0.8284;  Loss pred: 0.8284; Loss self: 0.0000; time: 0.14s
Val loss: 0.5193 score: 0.8409 time: 0.04s
Test loss: 0.5541 score: 0.8140 time: 0.04s
Epoch 181/1000, LR 0.000251
Train loss: 0.8197;  Loss pred: 0.8197; Loss self: 0.0000; time: 0.14s
Val loss: 0.5166 score: 0.8409 time: 0.05s
Test loss: 0.5517 score: 0.8140 time: 0.05s
Epoch 182/1000, LR 0.000251
Train loss: 0.8172;  Loss pred: 0.8172; Loss self: 0.0000; time: 0.13s
Val loss: 0.5138 score: 0.8409 time: 0.05s
Test loss: 0.5493 score: 0.8140 time: 0.04s
Epoch 183/1000, LR 0.000250
Train loss: 0.8175;  Loss pred: 0.8175; Loss self: 0.0000; time: 0.13s
Val loss: 0.5110 score: 0.8636 time: 0.05s
Test loss: 0.5468 score: 0.8140 time: 0.04s
Epoch 184/1000, LR 0.000250
Train loss: 0.8138;  Loss pred: 0.8138; Loss self: 0.0000; time: 0.13s
Val loss: 0.5083 score: 0.8636 time: 0.04s
Test loss: 0.5444 score: 0.8140 time: 0.04s
Epoch 185/1000, LR 0.000250
Train loss: 0.8117;  Loss pred: 0.8117; Loss self: 0.0000; time: 0.13s
Val loss: 0.5055 score: 0.8636 time: 0.04s
Test loss: 0.5419 score: 0.8140 time: 0.04s
Epoch 186/1000, LR 0.000250
Train loss: 0.8088;  Loss pred: 0.8088; Loss self: 0.0000; time: 0.14s
Val loss: 0.5028 score: 0.8636 time: 0.04s
Test loss: 0.5395 score: 0.8140 time: 0.05s
Epoch 187/1000, LR 0.000249
Train loss: 0.8033;  Loss pred: 0.8033; Loss self: 0.0000; time: 0.13s
Val loss: 0.5001 score: 0.8636 time: 0.04s
Test loss: 0.5372 score: 0.8140 time: 0.04s
Epoch 188/1000, LR 0.000249
Train loss: 0.8009;  Loss pred: 0.8009; Loss self: 0.0000; time: 0.13s
Val loss: 0.4975 score: 0.8636 time: 0.04s
Test loss: 0.5349 score: 0.8140 time: 0.04s
Epoch 189/1000, LR 0.000249
Train loss: 0.7975;  Loss pred: 0.7975; Loss self: 0.0000; time: 0.13s
Val loss: 0.4949 score: 0.8636 time: 0.04s
Test loss: 0.5326 score: 0.8140 time: 0.04s
Epoch 190/1000, LR 0.000249
Train loss: 0.7950;  Loss pred: 0.7950; Loss self: 0.0000; time: 0.14s
Val loss: 0.4923 score: 0.8636 time: 0.04s
Test loss: 0.5304 score: 0.8140 time: 0.04s
Epoch 191/1000, LR 0.000249
Train loss: 0.7909;  Loss pred: 0.7909; Loss self: 0.0000; time: 0.13s
Val loss: 0.4898 score: 0.8636 time: 0.04s
Test loss: 0.5282 score: 0.8140 time: 0.04s
Epoch 192/1000, LR 0.000248
Train loss: 0.7884;  Loss pred: 0.7884; Loss self: 0.0000; time: 0.13s
Val loss: 0.4872 score: 0.8636 time: 0.05s
Test loss: 0.5259 score: 0.8140 time: 0.04s
Epoch 193/1000, LR 0.000248
Train loss: 0.7877;  Loss pred: 0.7877; Loss self: 0.0000; time: 0.13s
Val loss: 0.4847 score: 0.8636 time: 0.04s
Test loss: 0.5237 score: 0.8140 time: 0.04s
Epoch 194/1000, LR 0.000248
Train loss: 0.7841;  Loss pred: 0.7841; Loss self: 0.0000; time: 0.13s
Val loss: 0.4822 score: 0.8636 time: 0.04s
Test loss: 0.5214 score: 0.8140 time: 0.05s
Epoch 195/1000, LR 0.000248
Train loss: 0.7809;  Loss pred: 0.7809; Loss self: 0.0000; time: 0.13s
Val loss: 0.4796 score: 0.8636 time: 0.49s
Test loss: 0.5191 score: 0.8140 time: 0.51s
Epoch 196/1000, LR 0.000247
Train loss: 0.7758;  Loss pred: 0.7758; Loss self: 0.0000; time: 1.31s
Val loss: 0.4770 score: 0.8636 time: 0.29s
Test loss: 0.5167 score: 0.8140 time: 0.50s
Epoch 197/1000, LR 0.000247
Train loss: 0.7740;  Loss pred: 0.7740; Loss self: 0.0000; time: 0.84s
Val loss: 0.4744 score: 0.8636 time: 1.03s
Test loss: 0.5144 score: 0.8140 time: 0.66s
Epoch 198/1000, LR 0.000247
Train loss: 0.7707;  Loss pred: 0.7707; Loss self: 0.0000; time: 1.33s
Val loss: 0.4719 score: 0.8636 time: 1.44s
Test loss: 0.5121 score: 0.8140 time: 0.10s
Epoch 199/1000, LR 0.000247
Train loss: 0.7662;  Loss pred: 0.7662; Loss self: 0.0000; time: 1.50s
Val loss: 0.4694 score: 0.8636 time: 0.06s
Test loss: 0.5098 score: 0.8140 time: 0.05s
Epoch 200/1000, LR 0.000246
Train loss: 0.7658;  Loss pred: 0.7658; Loss self: 0.0000; time: 0.13s
Val loss: 0.4670 score: 0.8636 time: 0.04s
Test loss: 0.5075 score: 0.8140 time: 0.04s
Epoch 201/1000, LR 0.000246
Train loss: 0.7598;  Loss pred: 0.7598; Loss self: 0.0000; time: 0.13s
Val loss: 0.4646 score: 0.8636 time: 0.05s
Test loss: 0.5053 score: 0.8140 time: 0.04s
Epoch 202/1000, LR 0.000246
Train loss: 0.7596;  Loss pred: 0.7596; Loss self: 0.0000; time: 0.13s
Val loss: 0.4622 score: 0.8636 time: 0.04s
Test loss: 0.5031 score: 0.8140 time: 0.04s
Epoch 203/1000, LR 0.000246
Train loss: 0.7552;  Loss pred: 0.7552; Loss self: 0.0000; time: 0.13s
Val loss: 0.4599 score: 0.8636 time: 0.04s
Test loss: 0.5010 score: 0.8140 time: 0.04s
Epoch 204/1000, LR 0.000245
Train loss: 0.7554;  Loss pred: 0.7554; Loss self: 0.0000; time: 0.14s
Val loss: 0.4576 score: 0.8636 time: 0.04s
Test loss: 0.4989 score: 0.8140 time: 0.04s
Epoch 205/1000, LR 0.000245
Train loss: 0.7511;  Loss pred: 0.7511; Loss self: 0.0000; time: 0.13s
Val loss: 0.4553 score: 0.8636 time: 0.04s
Test loss: 0.4968 score: 0.8140 time: 0.04s
Epoch 206/1000, LR 0.000245
Train loss: 0.7511;  Loss pred: 0.7511; Loss self: 0.0000; time: 0.13s
Val loss: 0.4530 score: 0.8636 time: 0.05s
Test loss: 0.4947 score: 0.8140 time: 0.04s
Epoch 207/1000, LR 0.000245
Train loss: 0.7453;  Loss pred: 0.7453; Loss self: 0.0000; time: 0.14s
Val loss: 0.4508 score: 0.8636 time: 0.04s
Test loss: 0.4926 score: 0.8140 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.7452;  Loss pred: 0.7452; Loss self: 0.0000; time: 0.13s
Val loss: 0.4487 score: 0.8636 time: 0.04s
Test loss: 0.4907 score: 0.8140 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.7377;  Loss pred: 0.7377; Loss self: 0.0000; time: 0.14s
Val loss: 0.4465 score: 0.8636 time: 0.04s
Test loss: 0.4886 score: 0.8140 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.7370;  Loss pred: 0.7370; Loss self: 0.0000; time: 0.13s
Val loss: 0.4444 score: 0.8636 time: 0.05s
Test loss: 0.4867 score: 0.8140 time: 0.05s
Epoch 211/1000, LR 0.000244
Train loss: 0.7353;  Loss pred: 0.7353; Loss self: 0.0000; time: 0.14s
Val loss: 0.4423 score: 0.8636 time: 0.05s
Test loss: 0.4847 score: 0.8140 time: 0.05s
Epoch 212/1000, LR 0.000243
Train loss: 0.7335;  Loss pred: 0.7335; Loss self: 0.0000; time: 0.15s
Val loss: 0.4401 score: 0.8636 time: 0.05s
Test loss: 0.4826 score: 0.8140 time: 0.05s
Epoch 213/1000, LR 0.000243
Train loss: 0.7294;  Loss pred: 0.7294; Loss self: 0.0000; time: 0.14s
Val loss: 0.4379 score: 0.8636 time: 0.05s
Test loss: 0.4804 score: 0.8140 time: 0.05s
Epoch 214/1000, LR 0.000243
Train loss: 0.7289;  Loss pred: 0.7289; Loss self: 0.0000; time: 0.14s
Val loss: 0.4357 score: 0.8636 time: 0.05s
Test loss: 0.4782 score: 0.8140 time: 0.05s
Epoch 215/1000, LR 0.000243
Train loss: 0.7282;  Loss pred: 0.7282; Loss self: 0.0000; time: 0.15s
Val loss: 0.4334 score: 0.8636 time: 0.05s
Test loss: 0.4760 score: 0.8140 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.7224;  Loss pred: 0.7224; Loss self: 0.0000; time: 0.14s
Val loss: 0.4312 score: 0.8636 time: 0.05s
Test loss: 0.4738 score: 0.8140 time: 0.05s
Epoch 217/1000, LR 0.000242
Train loss: 0.7271;  Loss pred: 0.7271; Loss self: 0.0000; time: 0.15s
Val loss: 0.4292 score: 0.8636 time: 0.05s
Test loss: 0.4717 score: 0.8140 time: 0.05s
Epoch 218/1000, LR 0.000242
Train loss: 0.7189;  Loss pred: 0.7189; Loss self: 0.0000; time: 0.14s
Val loss: 0.4272 score: 0.8636 time: 0.05s
Test loss: 0.4699 score: 0.8140 time: 0.05s
Epoch 219/1000, LR 0.000242
Train loss: 0.7179;  Loss pred: 0.7179; Loss self: 0.0000; time: 0.15s
Val loss: 0.4255 score: 0.8636 time: 0.05s
Test loss: 0.4682 score: 0.8140 time: 0.05s
Epoch 220/1000, LR 0.000241
Train loss: 0.7169;  Loss pred: 0.7169; Loss self: 0.0000; time: 0.14s
Val loss: 0.4238 score: 0.8636 time: 0.05s
Test loss: 0.4667 score: 0.8140 time: 0.05s
Epoch 221/1000, LR 0.000241
Train loss: 0.7125;  Loss pred: 0.7125; Loss self: 0.0000; time: 0.15s
Val loss: 0.4223 score: 0.8636 time: 0.05s
Test loss: 0.4653 score: 0.8140 time: 0.05s
Epoch 222/1000, LR 0.000241
Train loss: 0.7092;  Loss pred: 0.7092; Loss self: 0.0000; time: 0.14s
Val loss: 0.4207 score: 0.8636 time: 0.05s
Test loss: 0.4639 score: 0.8140 time: 0.05s
Epoch 223/1000, LR 0.000241
Train loss: 0.7091;  Loss pred: 0.7091; Loss self: 0.0000; time: 0.15s
Val loss: 0.4191 score: 0.8636 time: 0.05s
Test loss: 0.4625 score: 0.8140 time: 0.05s
Epoch 224/1000, LR 0.000240
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.15s
Val loss: 0.4175 score: 0.8636 time: 0.05s
Test loss: 0.4609 score: 0.8140 time: 0.05s
Epoch 225/1000, LR 0.000240
Train loss: 0.7042;  Loss pred: 0.7042; Loss self: 0.0000; time: 2.62s
Val loss: 0.4158 score: 0.8636 time: 0.35s
Test loss: 0.4592 score: 0.8140 time: 1.28s
Epoch 226/1000, LR 0.000240
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 3.29s
Val loss: 0.4140 score: 0.8636 time: 0.36s
Test loss: 0.4574 score: 0.8140 time: 0.33s
Epoch 227/1000, LR 0.000240
Train loss: 0.7015;  Loss pred: 0.7015; Loss self: 0.0000; time: 0.72s
Val loss: 0.4121 score: 0.8636 time: 0.46s
Test loss: 0.4555 score: 0.8140 time: 0.05s
Epoch 228/1000, LR 0.000239
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.15s
Val loss: 0.4103 score: 0.8636 time: 0.05s
Test loss: 0.4537 score: 0.8140 time: 0.05s
Epoch 229/1000, LR 0.000239
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.14s
Val loss: 0.4086 score: 0.8636 time: 0.05s
Test loss: 0.4519 score: 0.8140 time: 0.05s
Epoch 230/1000, LR 0.000239
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.14s
Val loss: 0.4069 score: 0.8636 time: 0.04s
Test loss: 0.4501 score: 0.8140 time: 0.05s
Epoch 231/1000, LR 0.000238
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.14s
Val loss: 0.4052 score: 0.8636 time: 0.05s
Test loss: 0.4485 score: 0.8140 time: 0.05s
Epoch 232/1000, LR 0.000238
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.14s
Val loss: 0.4036 score: 0.8636 time: 0.05s
Test loss: 0.4468 score: 0.8140 time: 0.05s
Epoch 233/1000, LR 0.000238
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.14s
Val loss: 0.4021 score: 0.8636 time: 0.04s
Test loss: 0.4453 score: 0.8140 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.17s
Val loss: 0.4006 score: 0.8636 time: 0.04s
Test loss: 0.4438 score: 0.8140 time: 0.04s
Epoch 235/1000, LR 0.000237
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.13s
Val loss: 0.3992 score: 0.8636 time: 0.04s
Test loss: 0.4423 score: 0.8140 time: 0.04s
Epoch 236/1000, LR 0.000237
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.13s
Val loss: 0.3979 score: 0.8636 time: 0.04s
Test loss: 0.4411 score: 0.8140 time: 0.05s
Epoch 237/1000, LR 0.000237
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.14s
Val loss: 0.3966 score: 0.8636 time: 0.04s
Test loss: 0.4398 score: 0.8140 time: 0.05s
Epoch 238/1000, LR 0.000236
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.14s
Val loss: 0.3954 score: 0.8636 time: 0.04s
Test loss: 0.4386 score: 0.8140 time: 0.05s
Epoch 239/1000, LR 0.000236
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.14s
Val loss: 0.3943 score: 0.8636 time: 0.05s
Test loss: 0.4375 score: 0.8140 time: 0.05s
Epoch 240/1000, LR 0.000236
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.14s
Val loss: 0.3931 score: 0.8636 time: 0.04s
Test loss: 0.4363 score: 0.8140 time: 0.04s
Epoch 241/1000, LR 0.000236
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.18s
Val loss: 0.3920 score: 0.8636 time: 0.05s
Test loss: 0.4352 score: 0.8140 time: 0.05s
Epoch 242/1000, LR 0.000235
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.13s
Val loss: 0.3909 score: 0.8636 time: 0.05s
Test loss: 0.4342 score: 0.8140 time: 0.04s
Epoch 243/1000, LR 0.000235
Train loss: 0.6698;  Loss pred: 0.6698; Loss self: 0.0000; time: 0.14s
Val loss: 0.3897 score: 0.8636 time: 0.04s
Test loss: 0.4329 score: 0.8140 time: 0.05s
Epoch 244/1000, LR 0.000235
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.13s
Val loss: 0.3885 score: 0.8636 time: 0.04s
Test loss: 0.4316 score: 0.8140 time: 0.05s
Epoch 245/1000, LR 0.000234
Train loss: 0.6671;  Loss pred: 0.6671; Loss self: 0.0000; time: 0.13s
Val loss: 0.3872 score: 0.8636 time: 0.05s
Test loss: 0.4303 score: 0.8140 time: 0.04s
Epoch 246/1000, LR 0.000234
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.13s
Val loss: 0.3858 score: 0.8636 time: 0.04s
Test loss: 0.4288 score: 0.8140 time: 0.04s
Epoch 247/1000, LR 0.000234
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.13s
Val loss: 0.3843 score: 0.8636 time: 0.04s
Test loss: 0.4272 score: 0.8140 time: 0.05s
Epoch 248/1000, LR 0.000234
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.19s
Val loss: 0.3829 score: 0.8636 time: 0.04s
Test loss: 0.4257 score: 0.8140 time: 0.04s
Epoch 249/1000, LR 0.000233
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.13s
Val loss: 0.3816 score: 0.8636 time: 0.04s
Test loss: 0.4243 score: 0.8140 time: 0.05s
Epoch 250/1000, LR 0.000233
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.13s
Val loss: 0.3805 score: 0.8636 time: 0.04s
Test loss: 0.4232 score: 0.8140 time: 0.04s
Epoch 251/1000, LR 0.000233
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.13s
Val loss: 0.3796 score: 0.8636 time: 0.04s
Test loss: 0.4223 score: 0.8140 time: 0.04s
Epoch 252/1000, LR 0.000232
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.13s
Val loss: 0.3788 score: 0.8636 time: 0.04s
Test loss: 0.4215 score: 0.8140 time: 0.04s
Epoch 253/1000, LR 0.000232
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.13s
Val loss: 0.3780 score: 0.8636 time: 0.04s
Test loss: 0.4207 score: 0.8140 time: 0.04s
Epoch 254/1000, LR 0.000232
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.13s
Val loss: 0.3773 score: 0.8636 time: 0.04s
Test loss: 0.4200 score: 0.8140 time: 0.04s
Epoch 255/1000, LR 0.000232
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.13s
Val loss: 0.3765 score: 0.8636 time: 0.04s
Test loss: 0.4192 score: 0.8140 time: 0.04s
Epoch 256/1000, LR 0.000231
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.13s
Val loss: 0.3755 score: 0.8636 time: 0.05s
Test loss: 0.4182 score: 0.8140 time: 0.04s
Epoch 257/1000, LR 0.000231
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.13s
Val loss: 0.3745 score: 0.8636 time: 0.04s
Test loss: 0.4171 score: 0.8140 time: 0.04s
Epoch 258/1000, LR 0.000231
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.13s
Val loss: 0.3733 score: 0.8636 time: 0.05s
Test loss: 0.4159 score: 0.8140 time: 0.05s
Epoch 259/1000, LR 0.000230
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.14s
Val loss: 0.3723 score: 0.8636 time: 0.05s
Test loss: 0.4148 score: 0.8140 time: 0.05s
Epoch 260/1000, LR 0.000230
Train loss: 0.6445;  Loss pred: 0.6445; Loss self: 0.0000; time: 0.14s
Val loss: 0.3712 score: 0.8636 time: 0.05s
Test loss: 0.4136 score: 0.8140 time: 0.05s
Epoch 261/1000, LR 0.000230
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 0.14s
Val loss: 0.3704 score: 0.8636 time: 0.05s
Test loss: 0.4128 score: 0.8140 time: 0.04s
Epoch 262/1000, LR 0.000229
Train loss: 0.6426;  Loss pred: 0.6426; Loss self: 0.0000; time: 0.13s
Val loss: 0.3697 score: 0.8636 time: 0.05s
Test loss: 0.4121 score: 0.8140 time: 0.04s
Epoch 263/1000, LR 0.000229
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.13s
Val loss: 0.3689 score: 0.8636 time: 0.04s
Test loss: 0.4112 score: 0.8140 time: 0.04s
Epoch 264/1000, LR 0.000229
Train loss: 0.6380;  Loss pred: 0.6380; Loss self: 0.0000; time: 0.13s
Val loss: 0.3682 score: 0.8636 time: 0.04s
Test loss: 0.4105 score: 0.8140 time: 0.04s
Epoch 265/1000, LR 0.000228
Train loss: 0.6411;  Loss pred: 0.6411; Loss self: 0.0000; time: 0.13s
Val loss: 0.3675 score: 0.8636 time: 0.05s
Test loss: 0.4098 score: 0.8140 time: 0.05s
Epoch 266/1000, LR 0.000228
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.13s
Val loss: 0.3668 score: 0.8636 time: 0.04s
Test loss: 0.4090 score: 0.8140 time: 0.04s
Epoch 267/1000, LR 0.000228
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.13s
Val loss: 0.3660 score: 0.8636 time: 0.04s
Test loss: 0.4083 score: 0.8140 time: 0.05s
Epoch 268/1000, LR 0.000228
Train loss: 0.6352;  Loss pred: 0.6352; Loss self: 0.0000; time: 0.13s
Val loss: 0.3653 score: 0.8636 time: 0.04s
Test loss: 0.4075 score: 0.8140 time: 0.04s
Epoch 269/1000, LR 0.000227
Train loss: 0.6368;  Loss pred: 0.6368; Loss self: 0.0000; time: 0.13s
Val loss: 0.3643 score: 0.8636 time: 0.04s
Test loss: 0.4065 score: 0.8140 time: 0.04s
Epoch 270/1000, LR 0.000227
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.13s
Val loss: 0.3634 score: 0.8636 time: 0.04s
Test loss: 0.4055 score: 0.8140 time: 0.04s
Epoch 271/1000, LR 0.000227
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.13s
Val loss: 0.3625 score: 0.8409 time: 0.05s
Test loss: 0.4046 score: 0.8140 time: 0.05s
Epoch 272/1000, LR 0.000226
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 0.13s
Val loss: 0.3616 score: 0.8409 time: 0.04s
Test loss: 0.4036 score: 0.8140 time: 0.04s
Epoch 273/1000, LR 0.000226
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.13s
Val loss: 0.3608 score: 0.8409 time: 0.04s
Test loss: 0.4027 score: 0.8140 time: 0.04s
Epoch 274/1000, LR 0.000226
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.13s
Val loss: 0.3602 score: 0.8409 time: 0.05s
Test loss: 0.4021 score: 0.8140 time: 0.04s
Epoch 275/1000, LR 0.000225
Train loss: 0.6276;  Loss pred: 0.6276; Loss self: 0.0000; time: 0.13s
Val loss: 0.3597 score: 0.8409 time: 0.04s
Test loss: 0.4016 score: 0.8140 time: 0.04s
Epoch 276/1000, LR 0.000225
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.13s
Val loss: 0.3591 score: 0.8409 time: 0.05s
Test loss: 0.4010 score: 0.8140 time: 0.04s
Epoch 277/1000, LR 0.000225
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.13s
Val loss: 0.3586 score: 0.8409 time: 0.04s
Test loss: 0.4004 score: 0.8140 time: 0.04s
Epoch 278/1000, LR 0.000224
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.13s
Val loss: 0.3581 score: 0.8409 time: 0.04s
Test loss: 0.4000 score: 0.8140 time: 0.04s
Epoch 279/1000, LR 0.000224
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.13s
Val loss: 0.3578 score: 0.8409 time: 0.04s
Test loss: 0.3996 score: 0.8140 time: 0.04s
Epoch 280/1000, LR 0.000224
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.13s
Val loss: 0.3574 score: 0.8409 time: 0.05s
Test loss: 0.3992 score: 0.8140 time: 0.04s
Epoch 281/1000, LR 0.000223
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 0.13s
Val loss: 0.3571 score: 0.8409 time: 0.05s
Test loss: 0.3989 score: 0.8140 time: 0.04s
Epoch 282/1000, LR 0.000223
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.13s
Val loss: 0.3567 score: 0.8409 time: 0.04s
Test loss: 0.3984 score: 0.8140 time: 0.04s
Epoch 283/1000, LR 0.000223
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.13s
Val loss: 0.3561 score: 0.8409 time: 0.05s
Test loss: 0.3978 score: 0.8140 time: 0.05s
Epoch 284/1000, LR 0.000222
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.14s
Val loss: 0.3555 score: 0.8409 time: 0.05s
Test loss: 0.3972 score: 0.8140 time: 0.05s
Epoch 285/1000, LR 0.000222
Train loss: 0.6214;  Loss pred: 0.6214; Loss self: 0.0000; time: 0.14s
Val loss: 0.3550 score: 0.8409 time: 0.05s
Test loss: 0.3966 score: 0.8140 time: 0.05s
Epoch 286/1000, LR 0.000222
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.15s
Val loss: 0.3544 score: 0.8409 time: 0.05s
Test loss: 0.3961 score: 0.8140 time: 0.04s
Epoch 287/1000, LR 0.000221
Train loss: 0.6149;  Loss pred: 0.6149; Loss self: 0.0000; time: 0.13s
Val loss: 0.3539 score: 0.8409 time: 0.05s
Test loss: 0.3954 score: 0.8140 time: 0.04s
Epoch 288/1000, LR 0.000221
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.13s
Val loss: 0.3533 score: 0.8409 time: 0.04s
Test loss: 0.3948 score: 0.8140 time: 0.04s
Epoch 289/1000, LR 0.000221
Train loss: 0.6146;  Loss pred: 0.6146; Loss self: 0.0000; time: 0.13s
Val loss: 0.3526 score: 0.8409 time: 0.04s
Test loss: 0.3941 score: 0.8140 time: 0.04s
Epoch 290/1000, LR 0.000220
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.13s
Val loss: 0.3517 score: 0.8409 time: 0.05s
Test loss: 0.3932 score: 0.8140 time: 0.04s
Epoch 291/1000, LR 0.000220
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 0.13s
Val loss: 0.3506 score: 0.8409 time: 0.04s
Test loss: 0.3920 score: 0.8140 time: 0.05s
Epoch 292/1000, LR 0.000220
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 0.13s
Val loss: 0.3494 score: 0.8409 time: 0.04s
Test loss: 0.3908 score: 0.8140 time: 0.04s
Epoch 293/1000, LR 0.000219
Train loss: 0.6136;  Loss pred: 0.6136; Loss self: 0.0000; time: 0.13s
Val loss: 0.3483 score: 0.8409 time: 0.04s
Test loss: 0.3897 score: 0.8372 time: 0.04s
Epoch 294/1000, LR 0.000219
Train loss: 0.6087;  Loss pred: 0.6087; Loss self: 0.0000; time: 0.13s
Val loss: 0.3475 score: 0.8409 time: 0.04s
Test loss: 0.3888 score: 0.8372 time: 0.04s
Epoch 295/1000, LR 0.000219
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.13s
Val loss: 0.3470 score: 0.8409 time: 0.04s
Test loss: 0.3883 score: 0.8372 time: 0.05s
Epoch 296/1000, LR 0.000218
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.13s
Val loss: 0.3469 score: 0.8409 time: 0.04s
Test loss: 0.3882 score: 0.8372 time: 0.04s
Epoch 297/1000, LR 0.000218
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.13s
Val loss: 0.3470 score: 0.8409 time: 0.04s
Test loss: 0.3882 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 298/1000, LR 0.000218
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.13s
Val loss: 0.3471 score: 0.8409 time: 0.04s
Test loss: 0.3884 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 295,   Train_Loss: 0.6100,   Val_Loss: 0.3469,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3469,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.3882


[0.049124801997095346, 0.049941986915655434, 0.05001039104536176, 0.05000575305894017, 0.04985263501293957, 0.054238114040344954, 0.049600558006204665, 0.04968592105433345, 0.04960505000781268, 0.0642840500222519, 0.06257685902528465, 0.04993117100093514, 0.05009627097751945, 0.05076947109773755, 0.04965444398112595, 0.8010479350341484, 0.9624789939261973, 0.051789202028885484, 0.05047673103399575, 0.050415270030498505, 0.05067001504357904, 0.05001178605016321, 0.04764000000432134, 0.047977045993320644, 0.04809597891289741, 0.0474908200558275, 0.04752930800896138, 0.04752216988708824, 0.047899198019877076, 0.04856589005794376, 0.051210662932135165, 0.05107054894324392, 0.046711631934158504, 0.04709814197849482, 0.04699607996735722, 0.046965755987912416, 0.04966601904015988, 0.04975360899697989, 0.05004150199238211, 0.04985224304255098, 0.04978581296745688, 0.0508447689935565, 0.04706937610171735, 0.04705664608627558, 0.04685477900784463, 0.04691502603236586, 0.04677742498461157, 0.04700617108028382, 0.04665140598081052, 0.053004335961304605, 0.05278286000248045, 0.04802707000635564, 0.0478996749734506, 0.04794751200824976, 0.048388471943326294, 0.04811339802108705, 0.0477440869435668, 0.04795489693060517, 0.04782837606035173, 0.04762062197551131, 0.04770031000953168, 0.04795510298572481, 0.04779111500829458, 0.04784899204969406, 0.04762001894414425, 0.04776116099674255, 0.047549761016853154, 0.04703010700177401, 0.047518166014924645, 0.04813079896848649, 0.048055576044134796, 0.04836353694554418, 0.04764750809408724, 0.0478049679659307, 0.047793634003028274, 0.0478635779581964, 0.04739577102009207, 0.06904093804769218, 0.045805745059624314, 0.047070655040442944, 0.047864739899523556, 0.04799157695379108, 0.04841345804743469, 1.8975935449125245, 0.238157146028243, 0.049851948977448046, 0.0491288109915331, 0.04924885800573975, 0.048684756038710475, 0.04887815297115594, 0.04877692903392017, 0.04888593906071037, 0.04875342105515301, 0.04913148994091898, 0.04882066999562085, 0.049533621058799326, 0.048941275919787586, 0.048621080932207406, 0.0485653430223465, 0.05313878401648253, 0.04899783292785287, 0.04929594194982201, 0.04857413598801941, 0.04874859796836972, 0.04839485802222043, 0.05144445307087153, 0.05231440195348114, 0.04802079498767853, 0.0489595599938184, 0.04990462400019169, 0.04849367099814117, 0.049193792976439, 0.04886886395979673, 0.05212921090424061, 0.04833870008587837, 0.05190294003114104, 0.048708730028010905, 0.04849714203737676, 0.04861010401509702, 0.05082948796916753, 0.04828699503559619, 0.048461950034834445, 0.048673023004084826, 0.04848626104649156, 0.06151098199188709, 0.04967881890479475, 0.05181039799936116, 0.48195687495172024, 0.8349372680531815, 0.053846960072405636, 0.04857496509794146, 0.04813641600776464, 0.052818552940152586, 0.05324833106715232, 0.052457384997978806, 0.05157791904639453, 0.05136637797113508, 0.05129920702893287, 0.05185841000638902, 0.05212930601555854, 0.05144456902053207, 0.04729048500303179, 0.04736585996579379, 0.04687475995160639, 0.04701954999472946, 0.047303516999818385, 0.04714460810646415, 0.04686443007085472, 0.04723995900712907, 0.04728995088953525, 0.0468773830216378, 0.04715531296096742, 0.04735453799366951, 0.04729887400753796, 0.04722528194542974, 0.047347089974209666, 0.04667689302004874, 0.04740952793508768, 0.04719024198129773, 0.04684369906317443, 0.047050187014974654, 0.046783454017713666, 0.04715968098025769, 0.04710568499285728, 0.04697849100921303, 0.046985063003376126, 0.046888803015463054, 0.04709876992274076, 0.047866786015219986, 0.04752150899730623, 0.04790825804229826, 0.04741957597434521, 0.04693337494973093, 0.05054649000521749, 0.5350280939601362, 0.27575918298680335, 0.37262281007133424, 0.1894765020115301, 0.059384661028161645, 0.051867075031623244, 0.0477505000308156, 0.04703712300397456, 0.047551673953421414, 0.04757282801438123, 0.046930763055570424, 0.05065486300736666, 0.046898209024220705, 0.04671567096374929, 0.046756108989939094, 0.04697242297697812, 0.04701685800682753, 0.04711323999799788, 0.0471840399550274, 0.04972679994534701, 0.046576492954045534, 0.04959269997198135, 0.04670298506971449, 0.04666732205078006, 0.04904814308974892, 0.04735283798072487, 0.051444451906718314, 0.04683102294802666, 0.04747437103651464, 0.04656289704144001, 0.04657274798955768, 0.050008572987280786, 0.04673309996724129, 0.046351082040928304, 0.04658880503848195, 0.04906335403211415, 0.04719901899807155, 0.047980961040593684, 0.04731344908941537, 0.05051254702266306, 0.5141356769017875, 0.5073090570513159, 0.6691882180748507, 0.106114515918307, 0.050323400064371526, 0.04785369802266359, 0.050361806992441416, 0.047354949987493455, 0.046856747940182686, 0.046914911014027894, 0.04820989305153489, 0.0475221510278061, 0.0473379340255633, 0.04769759194459766, 0.04767030104994774, 0.05463073798455298, 0.05096585093997419, 0.056320720934309065, 0.051504626986570656, 0.05107509403023869, 0.05086481897160411, 0.051120689953677356, 0.05436924600508064, 0.05097848200239241, 0.051157547044567764, 0.05150128901004791, 0.051047891029156744, 0.05237820802722126, 0.05134552204981446, 0.05144684296101332, 1.2866976069053635, 0.33832589199300855, 0.058182683074846864, 0.05075906997080892, 0.05330034904181957, 0.05056838097516447, 0.05066430906299502, 0.055238969973288476, 0.04630017408635467, 0.04715985897928476, 0.04657499794848263, 0.05397687992081046, 0.050615767017006874, 0.05087436793837696, 0.05006104602944106, 0.04718430095817894, 0.05004300607834011, 0.04723262705374509, 0.0503092099679634, 0.05036686302628368, 0.047206401941366494, 0.04649776499718428, 0.05532745295204222, 0.04697813105303794, 0.05057423701509833, 0.0473900840152055, 0.047477280953899026, 0.04718189500272274, 0.04762252198997885, 0.04733392200432718, 0.04740707506425679, 0.046993626980111, 0.04679046897217631, 0.05228281510062516, 0.05486566899344325, 0.05143624893389642, 0.04751908092293888, 0.04716942005325109, 0.047253772034309804, 0.04712383798323572, 0.05183814198244363, 0.04636877006851137, 0.050918597960844636, 0.05047950299922377, 0.047259313985705376, 0.04688339005224407, 0.0503449160605669, 0.0469193389872089, 0.04621793294791132, 0.047027976950630546, 0.04685000400058925, 0.046889129909686744, 0.04692025505937636, 0.0469191640149802, 0.04676727997139096, 0.04758582101203501, 0.046972851967439055, 0.047232512035407126, 0.05052566702943295, 0.05062980600632727, 0.054490074981004, 0.04707917501218617, 0.047178345965221524, 0.047151644015684724, 0.04683390795253217, 0.04739416902884841, 0.05126973893493414, 0.04694108199328184, 0.04704286903142929, 0.047076240996830165, 0.05029943096451461, 0.04692779702600092, 0.049902954953722656, 0.04769393999595195]
[0.001116472772661258, 0.001135045157173987, 0.0011365997964854944, 0.0011364943877031858, 0.001133014432112263, 0.00123268441000784, 0.0011272854092319242, 0.0011292254785075784, 0.001127387500177561, 0.0014610011368693615, 0.001422201341483742, 0.001134799340930344, 0.0011385516131254421, 0.0011538516158576715, 0.0011285100904801352, 0.018205634887139735, 0.021874522589231758, 0.0011770273188383064, 0.0011471984325908124, 0.0011458015916022387, 0.0011783724428739311, 0.0011630647918642607, 0.0011079069768446823, 0.0011157452556586197, 0.001118511137509242, 0.0011044376757169186, 0.0011053327443944507, 0.0011051667415601916, 0.00111393483767156, 0.0011294393036731106, 0.0011909456495845386, 0.0011876871847266027, 0.0010863170217246163, 0.0010953056274068563, 0.0010929320922641213, 0.0010922268834398236, 0.0011550236986083693, 0.0011570606743483696, 0.001163755860287956, 0.0011593544893616506, 0.001157809603894346, 0.0011824364882222441, 0.0010946366535283106, 0.0010943406066575715, 0.001089646023438247, 0.0010910471170317642, 0.0010878470926653854, 0.001093166769308926, 0.0010849164181583842, 0.0012326589758442931, 0.001227508372150708, 0.0011169086047989685, 0.0011139459296151302, 0.001115058418796506, 0.0011253133010075882, 0.0011189162330485362, 0.0011103276033387628, 0.0011152301611768644, 0.001112287815357017, 0.001107456325011891, 0.001109309535105388, 0.001115234953156391, 0.0011114212792626647, 0.0011127672569696294, 0.0011074423010266105, 0.00111072467434285, 0.001105808395740771, 0.0010937234186459072, 0.0011050736282540614, 0.0011193209062438719, 0.0011175715359101116, 0.0011247334173382368, 0.0011080815835834243, 0.0011117434410681558, 0.0011114798605355413, 0.0011131064641441023, 0.001102227233025397, 0.001605603210411446, 0.0010652498851075423, 0.0010946663962893707, 0.0011131334860354316, 0.0011160831849718856, 0.0011258943731961555, 0.04413008243982615, 0.005538538279726581, 0.0011593476506383267, 0.0011425304881751883, 0.00114532227920325, 0.0011322036288072203, 0.0011367012318873474, 0.0011343471868353527, 0.0011368823037374504, 0.0011338004896547212, 0.0011425927893236973, 0.0011353644185028103, 0.0011519446757860309, 0.0011381692074369206, 0.0011307228123769164, 0.0011294265819150348, 0.0012357856748019193, 0.0011394844866942527, 0.0011464172546470234, 0.0011296310694888234, 0.0011336883248458075, 0.0011254618144702427, 0.0011963826295551519, 0.001216613998918166, 0.0011167626741320589, 0.001138594418460893, 0.0011605726511672485, 0.0011277597906544459, 0.0011440416971264884, 0.0011364852083673657, 0.001212307230331177, 0.0011241558159506598, 0.00120704511700328, 0.0011327611634421142, 0.0011278405124971339, 0.0011304675352348144, 0.0011820811155620356, 0.0011229533729208417, 0.0011270220938333591, 0.0011319307675368565, 0.0011275874661974782, 0.0014304879532996998, 0.0011553213698789477, 0.0012048929767293293, 0.011208299417481866, 0.01941714576867864, 0.0012522548854047822, 0.0011296503511149176, 0.001119451535064294, 0.0012283384404686648, 0.0012383332806314494, 0.0012199391859995072, 0.0011994864894510355, 0.001194566929561281, 0.0011930048146263459, 0.0012060095350323028, 0.0012123094422222917, 0.0011963853260588853, 0.0010997787210007394, 0.0011015316271114834, 0.0010901106965489858, 0.0010934779068541734, 0.0011000817906934508, 0.00109638623503405, 0.0010898704667640633, 0.001098603697840211, 0.0010997662997566336, 0.0010901716981776232, 0.0010966351851387772, 0.0011012683254341747, 0.0010999738141287897, 0.0010982623708239475, 0.0011010951156792946, 0.0010855091400011334, 0.0011025471612811089, 0.0010974474879371565, 0.001089388350306382, 0.001094190395697085, 0.0010879873027375272, 0.001096736766982737, 0.001095481046345518, 0.0010925230467258844, 0.0010926758837994449, 0.0010904372794293734, 0.001095320230761413, 0.001113181070121395, 0.0011051513720303774, 0.0011141455358674014, 0.0011027808366126793, 0.0010914738360402543, 0.0011754997675631975, 0.012442513813026423, 0.006413004255507055, 0.008665646745844982, 0.004406430279337909, 0.0013810386285618987, 0.0012062110472470522, 0.0011104767449026884, 0.0010938865814877803, 0.0011058528826377073, 0.0011063448375437496, 0.0010914130943155912, 0.0011780200699387595, 0.0010906560238190862, 0.0010864109526453323, 0.0010873513718590486, 0.0010923819296971657, 0.0010934153024843613, 0.0010956567441394857, 0.001097303254768079, 0.0011564372080313258, 0.001083174254745245, 0.0011533186039995663, 0.0010861159318538254, 0.0010852865593204664, 0.0011406544904592772, 0.0011012287902494157, 0.0011963826024818213, 0.0010890935569308525, 0.0011040551403840614, 0.001082858070731163, 0.0010830871625478532, 0.0011629900694716461, 0.0010868162783079368, 0.0010779321404867047, 0.001083460582290278, 0.0011410082333049801, 0.0010976516046063152, 0.0011158363032696206, 0.0011003127695212876, 0.001174710395875885, 0.01195664364887878, 0.011797885047705021, 0.015562516699415134, 0.002467779439960628, 0.001170311629403989, 0.001112876698201479, 0.0011712048137777072, 0.0011012779066858942, 0.001089691812562388, 0.0010910444421866952, 0.0011211603035240672, 0.001105166302972235, 0.001100882186641007, 0.001109246324292969, 0.001108611652324366, 0.0012704822787105343, 0.0011852523474412601, 0.0013097842077746294, 0.0011977820229435035, 0.0011877928844241555, 0.001182902766781491, 0.0011888532547366826, 0.0012644010698855963, 0.0011855460930788933, 0.0011897103963852968, 0.0011977043955825096, 0.0011871602564920173, 0.001218097861098169, 0.00119408190813522, 0.0011964382083956586, 0.029923200160589847, 0.007868043999837409, 0.0013530856529034155, 0.0011804434876932308, 0.0012395430009725483, 0.0011760088598875458, 0.001178239745651047, 0.0012846272086811273, 0.0010767482345663878, 0.0010967409064949944, 0.0010831394871740148, 0.0012552762772281502, 0.001177110860860625, 0.0011831248357762085, 0.001164210372777699, 0.0010973093246088126, 0.0011637908390311653, 0.0010984331872963974, 0.0011699816271619394, 0.0011713223959600857, 0.0010978233009620115, 0.0010813433720275414, 0.001286684952373075, 0.001092514675652045, 0.001176145046862752, 0.0011020949770978025, 0.0011041228128813726, 0.0010972533721563427, 0.001107500511394857, 0.0011007888838215623, 0.0011024901177734137, 0.001092875046049093, 0.0010881504412134026, 0.001215879420944771, 0.001275945790545192, 0.0011961918356720098, 0.0011050949051846252, 0.0010969632570523508, 0.0010989249310304607, 0.0010959032089124586, 0.001205538185638224, 0.0010783434899653806, 0.0011841534409498752, 0.0011739419302145063, 0.0010990538136210553, 0.0010903113965638155, 0.0011708120014085326, 0.0010911474183071838, 0.0010748356499514261, 0.0010936738825728033, 0.0010895349767578895, 0.0010904448816206219, 0.0010911687223110781, 0.001091143349185586, 0.0010876111621253711, 0.001106647000279884, 0.001092391906219513, 0.0010984305124513286, 0.0011750155123123943, 0.0011774373489843552, 0.0012672110460698605, 0.0010948645351671201, 0.0010971708364005005, 0.0010965498608298773, 0.0010891606500588876, 0.0011021899774150794, 0.0011923195101147474, 0.0010916530696112056, 0.0010940202100332393, 0.0010947963022518643, 0.001169754208477084, 0.0010913441168837422, 0.001160533836133085, 0.0011091613952546966]
[895.6779103679977, 881.0222163228996, 879.817155600522, 879.8987578116985, 882.6012905552438, 811.2376467823099, 887.0867943561426, 885.5627321849246, 887.0064639199054, 684.4621641724411, 703.1353232705635, 881.2130602580089, 878.3088868978864, 866.6625641085471, 886.1241103963373, 54.9280487167404, 45.71528342713515, 849.5979523966976, 871.6887781494069, 872.7514495783199, 848.6281277599306, 859.7973277113081, 902.6028546620392, 896.2619333834446, 894.0456348310051, 905.4381446656776, 904.7049452496257, 904.8408374905265, 897.7185793832285, 885.3950776707043, 839.6688802287912, 841.9725436628273, 920.541591452207, 912.9871836480079, 914.9699300424028, 915.5606908801152, 865.7831014245425, 864.2589124059353, 859.2867577505158, 862.5489521764888, 863.6998662271016, 845.7113848909283, 913.5451446621388, 913.792281778052, 917.7292244362258, 916.5507010554583, 919.246837852784, 914.7735076434642, 921.729990682114, 811.2543855165322, 814.6583947512371, 895.3284052995449, 897.7096404899125, 896.8139992874187, 888.6414113337285, 893.7219520673647, 900.635089133147, 896.6758923958208, 899.0478778903324, 902.9701464653812, 901.4616465051812, 896.6720395283098, 899.7488339105911, 898.6605183938234, 902.9815811378974, 900.3131226841979, 904.315796345631, 914.3079346678502, 904.9170792175446, 893.3988406914694, 894.7973063627061, 889.0995720270964, 902.4606263792425, 899.4881040532216, 899.7014120599296, 898.3866613055086, 907.2539400566221, 622.8188842147018, 938.7468743064404, 913.5203230771815, 898.3648525044638, 895.9905618730294, 888.1827849989423, 22.66027944460689, 180.55305380129403, 862.5540401530195, 875.2501664941709, 873.1166922690579, 883.233346508086, 879.7386436712376, 881.5643143523281, 879.5985272288469, 881.9893880135228, 875.2024425008859, 880.7744753166445, 868.0972454841625, 878.603983894391, 884.3900459546569, 885.4050506801589, 809.2018060982033, 877.5898326629177, 872.2827538982701, 885.2447732802856, 882.0766502433635, 888.524148169969, 835.852991590013, 821.9533893981305, 895.4454004985383, 878.275866969171, 861.6436024011489, 886.7136497389165, 874.0940146777161, 879.9058647112217, 824.8734107828599, 889.5563993985429, 828.4694465130607, 882.7986271715976, 886.650185836928, 884.5897549745075, 845.965633690491, 890.5089241585912, 887.2940517063718, 883.4462572088706, 886.8491624621043, 699.0621610572146, 865.5600303704051, 829.9492314367129, 89.21960082903165, 51.500875149893425, 798.5594719215309, 885.2296633317043, 893.2945899641529, 814.1078769939467, 807.5370464807997, 819.7129918248269, 833.690090546719, 837.1234589319009, 838.2195844810603, 829.1808405753733, 824.871905779183, 835.8511076813229, 909.2738210919865, 907.8268616056653, 917.3380310511093, 914.5132185403727, 909.0233185021971, 912.0873356905502, 917.5402311515992, 910.2463444879533, 909.2840908302874, 917.2867005001524, 911.8802802897956, 908.043913462916, 909.112550821974, 910.5292383365295, 908.1867549499333, 921.2266973624523, 906.9906804150185, 911.2053296323762, 917.9462950184457, 913.9177276025364, 919.128373542468, 911.7958202049958, 912.8409873780664, 915.3124988958713, 915.1844703690239, 917.0632909059206, 912.975011248399, 898.3264509617898, 904.8534212673565, 897.5488101035786, 906.7984923202124, 916.1923694184818, 850.7019972219924, 80.36961140063767, 155.9331570911196, 115.39819581031058, 226.94106943869667, 724.0927077045764, 829.0423158387665, 900.5141301609432, 914.1715575667026, 904.2794170005468, 903.8773138944175, 916.2433593735514, 848.881971978615, 916.8793626595109, 920.4620015704666, 919.6659202170282, 915.4307415879938, 914.5655797279302, 912.6946056316085, 911.3251014746651, 864.7248575669418, 923.2124892362707, 867.0630964697211, 920.712025918965, 921.4156311178615, 876.6896622634225, 908.0765131226832, 835.8530105048019, 918.1947626410307, 905.7518627666873, 923.4820582948456, 923.2867257401529, 859.8525698970984, 920.1187173575454, 927.7021831341655, 922.9685106643618, 876.41786519231, 911.0358840669313, 896.1888021296696, 908.8324953595416, 851.273644560183, 83.63551088133114, 84.76095469285188, 64.25695916121212, 405.22259964040967, 854.4732658166231, 898.572143361525, 853.8216272989109, 908.0360133704373, 917.6906612233054, 916.5529481051918, 891.9331132727119, 904.841196578831, 908.3624134669515, 901.5130166308171, 902.0291261627588, 787.1026749109337, 843.7021889547946, 763.4845450603165, 834.8764473376701, 841.8976179376609, 845.3780209854922, 841.1467067241099, 790.8882899715361, 843.4931428123343, 840.5406921199522, 834.9305585654505, 842.3462582507067, 820.9521024020647, 837.4634882138739, 835.8141632244689, 33.418885501325605, 127.09639143104243, 739.0515137413706, 847.1392408239336, 806.7489382904811, 850.3337297098454, 848.7237030418127, 778.4359487657574, 928.7222099813382, 911.792378744983, 923.2421233289801, 796.6373762819442, 849.5376546512082, 845.219346058215, 858.9512886868478, 911.320060418239, 859.2609311416145, 910.3876426579266, 854.7142765187953, 853.7359171557036, 910.8934007173195, 924.7756317449648, 777.1910273417494, 915.3195122099119, 850.2352687429147, 907.3628142588457, 905.6963485704559, 911.3665315375439, 902.9341203107311, 908.4394062268709, 907.0376086631937, 915.0176899135447, 918.990575315022, 822.4499755271563, 783.7323555671715, 835.9863110403265, 904.8996564081822, 911.6075616671968, 909.9802650416723, 912.4893438284299, 829.5050392539744, 927.3482979269473, 844.4851532060275, 851.830890661923, 909.8735545125835, 917.1691712583786, 854.1080880593647, 916.4664491910811, 930.3747973424513, 914.3493466695564, 917.8227604731725, 917.0568974690381, 916.4485560784915, 916.4698669028097, 919.4462458861083, 903.6305160969019, 915.4223812045097, 910.3898595900577, 851.052594218123, 849.3020888649314, 789.1345353257525, 913.3550022673444, 911.4350899817116, 911.9512351615205, 918.1382011422585, 907.2846065478283, 838.7013644553725, 916.0419439448395, 914.0598965439746, 913.4119268973783, 854.8804464673917, 916.3012697181443, 861.6724207990471, 901.5820459297271]
Elapsed: 0.07807552561737925~0.16091185521341625
Time per graph: 0.0018112296209612689~0.0037277710962115543
Speed: 841.5831562872381~174.12495938740486
Total Time: 0.0487
best val loss: 0.34691405296325684 test_score: 0.8372

Testing...
Test loss: 0.5468 score: 0.8140 time: 0.04s
test Score 0.8140
Epoch Time List: [0.5091898591490462, 0.21777532389387488, 0.22003529395442456, 0.22011822497006506, 0.2203031408134848, 0.22476634592749178, 0.2191435620188713, 0.21867023990489542, 0.21942473202943802, 0.24040018604137003, 0.23323823499958962, 0.22454964509233832, 0.22136349906213582, 0.22192379494663328, 0.22051425103563815, 5.942992649972439, 5.878618599846959, 1.239305712049827, 0.2249449238879606, 0.2225545340916142, 0.2277157239150256, 0.22536815202329308, 0.2140564020955935, 0.21386375499423593, 0.21445986395701766, 0.21330553689040244, 0.21266255201771855, 0.212438968825154, 0.21278830990195274, 0.21516101201996207, 0.21804861701093614, 0.23021274409256876, 0.225886041065678, 0.20996377407573164, 0.21006781898904592, 0.21247607003897429, 0.21933059312868863, 0.2244830859126523, 0.22554177697747946, 0.2244537939550355, 0.22516849800013006, 0.22609717410523444, 0.22071170096751302, 0.21091403788886964, 0.2113282687496394, 0.21063946618232876, 0.21052049985155463, 0.20993879297748208, 0.2100525270216167, 0.22181457886472344, 0.23868121206760406, 0.21834522194694728, 0.21608584106434137, 0.21641623903997242, 0.21619958989322186, 0.21882279694546014, 0.21405489998869598, 0.2178189919795841, 0.21497116517275572, 0.21405988396145403, 0.21427671785932034, 0.21424900798592716, 0.21360759704839438, 0.21444302413146943, 0.21374160086270422, 0.2132547440705821, 0.21571378293447196, 0.21138987189624459, 0.21205091499723494, 0.21451270615216345, 0.21527854702435434, 0.21616175305098295, 0.214629793073982, 0.21412122005131096, 0.21401175600476563, 0.2141044739400968, 0.2130449659889564, 0.2602556520141661, 0.21885462501086295, 0.20828296500258148, 0.21231834590435028, 0.21403194195590913, 0.2158234331291169, 2.642231769161299, 6.233384242863394, 0.27055510913487524, 0.22544380987528712, 0.22108573094010353, 0.22011361713521183, 0.21947362297214568, 0.21869001106824726, 0.22065978101454675, 0.21968839003238827, 0.2194409550866112, 0.2193058280972764, 0.22013180085923523, 0.22089704894460738, 0.2210324329789728, 0.22018862911500037, 0.2292408790672198, 0.23703821003437042, 0.2266188251087442, 0.22419499792158604, 0.22265638515818864, 0.22420148004312068, 0.22272208495996892, 0.22420876694377512, 0.217558188829571, 0.22708560107275844, 0.22407294914592057, 0.22960919397883117, 0.2387108012335375, 0.2248314939206466, 0.21995732805225998, 0.22169142006896436, 0.2250436309259385, 0.2216406058287248, 0.23250281088985503, 0.22271263191942126, 0.22196744591929018, 0.22072558104991913, 0.21727032191120088, 0.22281901899259537, 0.21918572497088462, 0.23566306498833, 0.2683373789768666, 0.22845105815213174, 1.488354678847827, 4.759840441867709, 2.794625661801547, 0.22685276507399976, 0.21853269403800368, 0.22639454808086157, 0.24179879704024643, 0.23948401189409196, 0.23632643814198673, 0.23504728288389742, 0.23531487886793911, 0.23524819395970553, 0.23732292896602303, 0.23653145704884082, 0.2246555269230157, 0.2130938160698861, 0.21290802804287523, 0.2141923678573221, 0.21396676695439965, 0.21336566901300102, 0.2133945298846811, 0.21282320306636393, 0.21868288097903132, 0.21632667700760067, 0.2175051097292453, 0.21446666691917926, 0.2145544158993289, 0.2155083038378507, 0.21541049901861697, 0.21260370698291808, 0.21323900390416384, 0.2131356201134622, 0.21305244299583137, 0.21252805285621434, 0.2127103810198605, 0.21266429394017905, 0.2124105991097167, 0.2126256680348888, 0.2127401059260592, 0.21200777892954648, 0.212758325971663, 0.21881869400385767, 0.21506983297877014, 0.21575891703832895, 0.21543536300305277, 0.2134706120705232, 0.2162340400973335, 3.9610243539791554, 2.0111426011426374, 1.223530926159583, 1.2471187439514324, 0.26980870694387704, 0.25431255297735333, 0.22775063884910196, 0.21829471900127828, 0.21746438404079527, 0.21814564103260636, 0.22136199520900846, 0.21567427786067128, 0.21661152702290565, 0.21513244800735265, 0.22650687908753753, 0.21552444202825427, 0.21707168605644256, 0.21593833400402218, 0.21535623096860945, 0.21871457493398339, 0.21176352503243834, 0.21911886101588607, 0.21381583192851394, 0.21518102404661477, 0.21495254896581173, 0.22194099507760257, 0.23363048408646137, 0.21715587691869587, 0.22130607115104795, 0.2147327169077471, 0.2149813739815727, 0.22721181390807033, 0.21071095194201916, 0.210708640050143, 0.21065969206392765, 0.22742192703299224, 0.21520912693813443, 0.22355602600146085, 0.2178319098893553, 0.221198350074701, 1.134587470907718, 2.1051780249690637, 2.531879846006632, 2.864472347078845, 1.6054952108534053, 0.22055872704368085, 0.22254139196593314, 0.21552252711262554, 0.2141416349913925, 0.21958454814739525, 0.2186494901543483, 0.21822030807379633, 0.223786617978476, 0.21880887693259865, 0.22291716607287526, 0.22969375702086836, 0.2352121148724109, 0.2447654769057408, 0.23416475916747004, 0.23987566318828613, 0.23795786197297275, 0.23521362512838095, 0.24830289802048355, 0.23521796893328428, 0.24426648195367306, 0.24339056585449725, 0.2438112599775195, 0.24249923508614302, 0.2393947591772303, 0.24637141509447247, 4.248307910049334, 3.982974610873498, 1.233023485983722, 0.23963302513584495, 0.24169579800218344, 0.23146689601708204, 0.2312086089514196, 0.2352563429158181, 0.23167480691336095, 0.24928567302413285, 0.21248814393766224, 0.22090183990076184, 0.22916425508446991, 0.2297881320118904, 0.23446002206765115, 0.2287115678191185, 0.2675995910540223, 0.21792722109239548, 0.22367123397998512, 0.21755377610679716, 0.22006934613455087, 0.2171546530444175, 0.22645034489687532, 0.271648914902471, 0.2144668409600854, 0.2171838020440191, 0.2180242749163881, 0.2185939010232687, 0.21747881406918168, 0.2106779778841883, 0.21701069094706327, 0.22158156614750624, 0.21375864907167852, 0.22192037000786513, 0.24529246799647808, 0.2400863110087812, 0.2304876489797607, 0.22128683584742248, 0.21799338201526552, 0.2127987949643284, 0.22259326791390777, 0.21116050926502794, 0.22157787694595754, 0.21948153211269528, 0.21359684597700834, 0.2178874087985605, 0.225381366093643, 0.2156422979896888, 0.21069219300989062, 0.21964530705008656, 0.21705879480578005, 0.21665763203054667, 0.2197657919023186, 0.21196336799766868, 0.21481570589821786, 0.21776814211625606, 0.21678424009587616, 0.21658347896300256, 0.22387254098430276, 0.2374770159367472, 0.24275302898604423, 0.23436523915734142, 0.21594148001167923, 0.21867397089954466, 0.21557390002999455, 0.21598413295578212, 0.21578940888866782, 0.21495771303307265, 0.21053912898059934, 0.2158125260611996, 0.21863491111434996, 0.21337695489637554, 0.2188805480254814, 0.21273435105104]
Total Epoch List: [20, 298]
Total Time List: [0.050746947061270475, 0.0486709859687835]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2bf9a4a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.1930;  Loss pred: 2.1930; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.2133;  Loss pred: 2.2133; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.2001;  Loss pred: 2.2001; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.2236;  Loss pred: 2.2236; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.1644;  Loss pred: 2.1644; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.1811;  Loss pred: 2.1811; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.1753;  Loss pred: 2.1753; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.1835;  Loss pred: 2.1835; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 2.1335;  Loss pred: 2.1335; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.1345;  Loss pred: 2.1345; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 2.0628;  Loss pred: 2.0628; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 2.1123;  Loss pred: 2.1123; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 2.0337;  Loss pred: 2.0337; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 1.9964;  Loss pred: 1.9964; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 1.9571;  Loss pred: 1.9571; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 1.9661;  Loss pred: 1.9661; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 1.9112;  Loss pred: 1.9112; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 1.9255;  Loss pred: 1.9255; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 1.9108;  Loss pred: 1.9108; Loss self: 0.0000; time: 0.15s
Val loss: 0.6931 score: 0.5227 time: 0.05s
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 1.8734;  Loss pred: 1.8734; Loss self: 0.0000; time: 0.15s
Val loss: 0.6931 score: 0.4545 time: 0.05s
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 1.8524;  Loss pred: 1.8524; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
Test loss: 0.6929 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 22/1000, LR 0.000270
Train loss: 1.8119;  Loss pred: 1.8119; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
Test loss: 0.6929 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 019,   Train_Loss: 1.8734,   Val_Loss: 0.6931,   Val_Precision: 0.4750,   Val_Recall: 0.8636,   Val_accuracy: 0.6129,   Val_Score: 0.4545,   Val_Loss: 0.6931,   Test_Precision: 0.4872,   Test_Recall: 0.9048,   Test_accuracy: 0.6333,   Test_Score: 0.4884,   Test_loss: 0.6928


[0.049124801997095346, 0.049941986915655434, 0.05001039104536176, 0.05000575305894017, 0.04985263501293957, 0.054238114040344954, 0.049600558006204665, 0.04968592105433345, 0.04960505000781268, 0.0642840500222519, 0.06257685902528465, 0.04993117100093514, 0.05009627097751945, 0.05076947109773755, 0.04965444398112595, 0.8010479350341484, 0.9624789939261973, 0.051789202028885484, 0.05047673103399575, 0.050415270030498505, 0.05067001504357904, 0.05001178605016321, 0.04764000000432134, 0.047977045993320644, 0.04809597891289741, 0.0474908200558275, 0.04752930800896138, 0.04752216988708824, 0.047899198019877076, 0.04856589005794376, 0.051210662932135165, 0.05107054894324392, 0.046711631934158504, 0.04709814197849482, 0.04699607996735722, 0.046965755987912416, 0.04966601904015988, 0.04975360899697989, 0.05004150199238211, 0.04985224304255098, 0.04978581296745688, 0.0508447689935565, 0.04706937610171735, 0.04705664608627558, 0.04685477900784463, 0.04691502603236586, 0.04677742498461157, 0.04700617108028382, 0.04665140598081052, 0.053004335961304605, 0.05278286000248045, 0.04802707000635564, 0.0478996749734506, 0.04794751200824976, 0.048388471943326294, 0.04811339802108705, 0.0477440869435668, 0.04795489693060517, 0.04782837606035173, 0.04762062197551131, 0.04770031000953168, 0.04795510298572481, 0.04779111500829458, 0.04784899204969406, 0.04762001894414425, 0.04776116099674255, 0.047549761016853154, 0.04703010700177401, 0.047518166014924645, 0.04813079896848649, 0.048055576044134796, 0.04836353694554418, 0.04764750809408724, 0.0478049679659307, 0.047793634003028274, 0.0478635779581964, 0.04739577102009207, 0.06904093804769218, 0.045805745059624314, 0.047070655040442944, 0.047864739899523556, 0.04799157695379108, 0.04841345804743469, 1.8975935449125245, 0.238157146028243, 0.049851948977448046, 0.0491288109915331, 0.04924885800573975, 0.048684756038710475, 0.04887815297115594, 0.04877692903392017, 0.04888593906071037, 0.04875342105515301, 0.04913148994091898, 0.04882066999562085, 0.049533621058799326, 0.048941275919787586, 0.048621080932207406, 0.0485653430223465, 0.05313878401648253, 0.04899783292785287, 0.04929594194982201, 0.04857413598801941, 0.04874859796836972, 0.04839485802222043, 0.05144445307087153, 0.05231440195348114, 0.04802079498767853, 0.0489595599938184, 0.04990462400019169, 0.04849367099814117, 0.049193792976439, 0.04886886395979673, 0.05212921090424061, 0.04833870008587837, 0.05190294003114104, 0.048708730028010905, 0.04849714203737676, 0.04861010401509702, 0.05082948796916753, 0.04828699503559619, 0.048461950034834445, 0.048673023004084826, 0.04848626104649156, 0.06151098199188709, 0.04967881890479475, 0.05181039799936116, 0.48195687495172024, 0.8349372680531815, 0.053846960072405636, 0.04857496509794146, 0.04813641600776464, 0.052818552940152586, 0.05324833106715232, 0.052457384997978806, 0.05157791904639453, 0.05136637797113508, 0.05129920702893287, 0.05185841000638902, 0.05212930601555854, 0.05144456902053207, 0.04729048500303179, 0.04736585996579379, 0.04687475995160639, 0.04701954999472946, 0.047303516999818385, 0.04714460810646415, 0.04686443007085472, 0.04723995900712907, 0.04728995088953525, 0.0468773830216378, 0.04715531296096742, 0.04735453799366951, 0.04729887400753796, 0.04722528194542974, 0.047347089974209666, 0.04667689302004874, 0.04740952793508768, 0.04719024198129773, 0.04684369906317443, 0.047050187014974654, 0.046783454017713666, 0.04715968098025769, 0.04710568499285728, 0.04697849100921303, 0.046985063003376126, 0.046888803015463054, 0.04709876992274076, 0.047866786015219986, 0.04752150899730623, 0.04790825804229826, 0.04741957597434521, 0.04693337494973093, 0.05054649000521749, 0.5350280939601362, 0.27575918298680335, 0.37262281007133424, 0.1894765020115301, 0.059384661028161645, 0.051867075031623244, 0.0477505000308156, 0.04703712300397456, 0.047551673953421414, 0.04757282801438123, 0.046930763055570424, 0.05065486300736666, 0.046898209024220705, 0.04671567096374929, 0.046756108989939094, 0.04697242297697812, 0.04701685800682753, 0.04711323999799788, 0.0471840399550274, 0.04972679994534701, 0.046576492954045534, 0.04959269997198135, 0.04670298506971449, 0.04666732205078006, 0.04904814308974892, 0.04735283798072487, 0.051444451906718314, 0.04683102294802666, 0.04747437103651464, 0.04656289704144001, 0.04657274798955768, 0.050008572987280786, 0.04673309996724129, 0.046351082040928304, 0.04658880503848195, 0.04906335403211415, 0.04719901899807155, 0.047980961040593684, 0.04731344908941537, 0.05051254702266306, 0.5141356769017875, 0.5073090570513159, 0.6691882180748507, 0.106114515918307, 0.050323400064371526, 0.04785369802266359, 0.050361806992441416, 0.047354949987493455, 0.046856747940182686, 0.046914911014027894, 0.04820989305153489, 0.0475221510278061, 0.0473379340255633, 0.04769759194459766, 0.04767030104994774, 0.05463073798455298, 0.05096585093997419, 0.056320720934309065, 0.051504626986570656, 0.05107509403023869, 0.05086481897160411, 0.051120689953677356, 0.05436924600508064, 0.05097848200239241, 0.051157547044567764, 0.05150128901004791, 0.051047891029156744, 0.05237820802722126, 0.05134552204981446, 0.05144684296101332, 1.2866976069053635, 0.33832589199300855, 0.058182683074846864, 0.05075906997080892, 0.05330034904181957, 0.05056838097516447, 0.05066430906299502, 0.055238969973288476, 0.04630017408635467, 0.04715985897928476, 0.04657499794848263, 0.05397687992081046, 0.050615767017006874, 0.05087436793837696, 0.05006104602944106, 0.04718430095817894, 0.05004300607834011, 0.04723262705374509, 0.0503092099679634, 0.05036686302628368, 0.047206401941366494, 0.04649776499718428, 0.05532745295204222, 0.04697813105303794, 0.05057423701509833, 0.0473900840152055, 0.047477280953899026, 0.04718189500272274, 0.04762252198997885, 0.04733392200432718, 0.04740707506425679, 0.046993626980111, 0.04679046897217631, 0.05228281510062516, 0.05486566899344325, 0.05143624893389642, 0.04751908092293888, 0.04716942005325109, 0.047253772034309804, 0.04712383798323572, 0.05183814198244363, 0.04636877006851137, 0.050918597960844636, 0.05047950299922377, 0.047259313985705376, 0.04688339005224407, 0.0503449160605669, 0.0469193389872089, 0.04621793294791132, 0.047027976950630546, 0.04685000400058925, 0.046889129909686744, 0.04692025505937636, 0.0469191640149802, 0.04676727997139096, 0.04758582101203501, 0.046972851967439055, 0.047232512035407126, 0.05052566702943295, 0.05062980600632727, 0.054490074981004, 0.04707917501218617, 0.047178345965221524, 0.047151644015684724, 0.04683390795253217, 0.04739416902884841, 0.05126973893493414, 0.04694108199328184, 0.04704286903142929, 0.047076240996830165, 0.05029943096451461, 0.04692779702600092, 0.049902954953722656, 0.04769393999595195, 0.04728252300992608, 0.047412080923095345, 0.04685024404898286, 0.04701363202184439, 0.046846509096212685, 0.047080556047149, 0.046837082016281784, 0.04687859502155334, 0.04819124401547015, 0.04725940595380962, 0.04696432501077652, 0.04687174002174288, 0.0469009829685092, 0.04679184197448194, 0.04713254503440112, 0.0473725909832865, 0.04688101296778768, 0.04789142496883869, 0.046625213930383325, 0.04658536706119776, 0.04673787800129503, 0.04681029904168099]
[0.001116472772661258, 0.001135045157173987, 0.0011365997964854944, 0.0011364943877031858, 0.001133014432112263, 0.00123268441000784, 0.0011272854092319242, 0.0011292254785075784, 0.001127387500177561, 0.0014610011368693615, 0.001422201341483742, 0.001134799340930344, 0.0011385516131254421, 0.0011538516158576715, 0.0011285100904801352, 0.018205634887139735, 0.021874522589231758, 0.0011770273188383064, 0.0011471984325908124, 0.0011458015916022387, 0.0011783724428739311, 0.0011630647918642607, 0.0011079069768446823, 0.0011157452556586197, 0.001118511137509242, 0.0011044376757169186, 0.0011053327443944507, 0.0011051667415601916, 0.00111393483767156, 0.0011294393036731106, 0.0011909456495845386, 0.0011876871847266027, 0.0010863170217246163, 0.0010953056274068563, 0.0010929320922641213, 0.0010922268834398236, 0.0011550236986083693, 0.0011570606743483696, 0.001163755860287956, 0.0011593544893616506, 0.001157809603894346, 0.0011824364882222441, 0.0010946366535283106, 0.0010943406066575715, 0.001089646023438247, 0.0010910471170317642, 0.0010878470926653854, 0.001093166769308926, 0.0010849164181583842, 0.0012326589758442931, 0.001227508372150708, 0.0011169086047989685, 0.0011139459296151302, 0.001115058418796506, 0.0011253133010075882, 0.0011189162330485362, 0.0011103276033387628, 0.0011152301611768644, 0.001112287815357017, 0.001107456325011891, 0.001109309535105388, 0.001115234953156391, 0.0011114212792626647, 0.0011127672569696294, 0.0011074423010266105, 0.00111072467434285, 0.001105808395740771, 0.0010937234186459072, 0.0011050736282540614, 0.0011193209062438719, 0.0011175715359101116, 0.0011247334173382368, 0.0011080815835834243, 0.0011117434410681558, 0.0011114798605355413, 0.0011131064641441023, 0.001102227233025397, 0.001605603210411446, 0.0010652498851075423, 0.0010946663962893707, 0.0011131334860354316, 0.0011160831849718856, 0.0011258943731961555, 0.04413008243982615, 0.005538538279726581, 0.0011593476506383267, 0.0011425304881751883, 0.00114532227920325, 0.0011322036288072203, 0.0011367012318873474, 0.0011343471868353527, 0.0011368823037374504, 0.0011338004896547212, 0.0011425927893236973, 0.0011353644185028103, 0.0011519446757860309, 0.0011381692074369206, 0.0011307228123769164, 0.0011294265819150348, 0.0012357856748019193, 0.0011394844866942527, 0.0011464172546470234, 0.0011296310694888234, 0.0011336883248458075, 0.0011254618144702427, 0.0011963826295551519, 0.001216613998918166, 0.0011167626741320589, 0.001138594418460893, 0.0011605726511672485, 0.0011277597906544459, 0.0011440416971264884, 0.0011364852083673657, 0.001212307230331177, 0.0011241558159506598, 0.00120704511700328, 0.0011327611634421142, 0.0011278405124971339, 0.0011304675352348144, 0.0011820811155620356, 0.0011229533729208417, 0.0011270220938333591, 0.0011319307675368565, 0.0011275874661974782, 0.0014304879532996998, 0.0011553213698789477, 0.0012048929767293293, 0.011208299417481866, 0.01941714576867864, 0.0012522548854047822, 0.0011296503511149176, 0.001119451535064294, 0.0012283384404686648, 0.0012383332806314494, 0.0012199391859995072, 0.0011994864894510355, 0.001194566929561281, 0.0011930048146263459, 0.0012060095350323028, 0.0012123094422222917, 0.0011963853260588853, 0.0010997787210007394, 0.0011015316271114834, 0.0010901106965489858, 0.0010934779068541734, 0.0011000817906934508, 0.00109638623503405, 0.0010898704667640633, 0.001098603697840211, 0.0010997662997566336, 0.0010901716981776232, 0.0010966351851387772, 0.0011012683254341747, 0.0010999738141287897, 0.0010982623708239475, 0.0011010951156792946, 0.0010855091400011334, 0.0011025471612811089, 0.0010974474879371565, 0.001089388350306382, 0.001094190395697085, 0.0010879873027375272, 0.001096736766982737, 0.001095481046345518, 0.0010925230467258844, 0.0010926758837994449, 0.0010904372794293734, 0.001095320230761413, 0.001113181070121395, 0.0011051513720303774, 0.0011141455358674014, 0.0011027808366126793, 0.0010914738360402543, 0.0011754997675631975, 0.012442513813026423, 0.006413004255507055, 0.008665646745844982, 0.004406430279337909, 0.0013810386285618987, 0.0012062110472470522, 0.0011104767449026884, 0.0010938865814877803, 0.0011058528826377073, 0.0011063448375437496, 0.0010914130943155912, 0.0011780200699387595, 0.0010906560238190862, 0.0010864109526453323, 0.0010873513718590486, 0.0010923819296971657, 0.0010934153024843613, 0.0010956567441394857, 0.001097303254768079, 0.0011564372080313258, 0.001083174254745245, 0.0011533186039995663, 0.0010861159318538254, 0.0010852865593204664, 0.0011406544904592772, 0.0011012287902494157, 0.0011963826024818213, 0.0010890935569308525, 0.0011040551403840614, 0.001082858070731163, 0.0010830871625478532, 0.0011629900694716461, 0.0010868162783079368, 0.0010779321404867047, 0.001083460582290278, 0.0011410082333049801, 0.0010976516046063152, 0.0011158363032696206, 0.0011003127695212876, 0.001174710395875885, 0.01195664364887878, 0.011797885047705021, 0.015562516699415134, 0.002467779439960628, 0.001170311629403989, 0.001112876698201479, 0.0011712048137777072, 0.0011012779066858942, 0.001089691812562388, 0.0010910444421866952, 0.0011211603035240672, 0.001105166302972235, 0.001100882186641007, 0.001109246324292969, 0.001108611652324366, 0.0012704822787105343, 0.0011852523474412601, 0.0013097842077746294, 0.0011977820229435035, 0.0011877928844241555, 0.001182902766781491, 0.0011888532547366826, 0.0012644010698855963, 0.0011855460930788933, 0.0011897103963852968, 0.0011977043955825096, 0.0011871602564920173, 0.001218097861098169, 0.00119408190813522, 0.0011964382083956586, 0.029923200160589847, 0.007868043999837409, 0.0013530856529034155, 0.0011804434876932308, 0.0012395430009725483, 0.0011760088598875458, 0.001178239745651047, 0.0012846272086811273, 0.0010767482345663878, 0.0010967409064949944, 0.0010831394871740148, 0.0012552762772281502, 0.001177110860860625, 0.0011831248357762085, 0.001164210372777699, 0.0010973093246088126, 0.0011637908390311653, 0.0010984331872963974, 0.0011699816271619394, 0.0011713223959600857, 0.0010978233009620115, 0.0010813433720275414, 0.001286684952373075, 0.001092514675652045, 0.001176145046862752, 0.0011020949770978025, 0.0011041228128813726, 0.0010972533721563427, 0.001107500511394857, 0.0011007888838215623, 0.0011024901177734137, 0.001092875046049093, 0.0010881504412134026, 0.001215879420944771, 0.001275945790545192, 0.0011961918356720098, 0.0011050949051846252, 0.0010969632570523508, 0.0010989249310304607, 0.0010959032089124586, 0.001205538185638224, 0.0010783434899653806, 0.0011841534409498752, 0.0011739419302145063, 0.0010990538136210553, 0.0010903113965638155, 0.0011708120014085326, 0.0010911474183071838, 0.0010748356499514261, 0.0010936738825728033, 0.0010895349767578895, 0.0010904448816206219, 0.0010911687223110781, 0.001091143349185586, 0.0010876111621253711, 0.001106647000279884, 0.001092391906219513, 0.0010984305124513286, 0.0011750155123123943, 0.0011774373489843552, 0.0012672110460698605, 0.0010948645351671201, 0.0010971708364005005, 0.0010965498608298773, 0.0010891606500588876, 0.0011021899774150794, 0.0011923195101147474, 0.0010916530696112056, 0.0010940202100332393, 0.0010947963022518643, 0.001169754208477084, 0.0010913441168837422, 0.001160533836133085, 0.0011091613952546966, 0.0010995935583703739, 0.0011026065330952406, 0.001089540559278671, 0.0010933402795777764, 0.001089453699911923, 0.001094896652259279, 0.0010892344654949253, 0.0010901998842221706, 0.0011207266050109336, 0.001099055952414177, 0.0010921936049017795, 0.0010900404656219275, 0.0010907205341513767, 0.00108818237149958, 0.0010961056984744447, 0.0011016881624020117, 0.001090256115529946, 0.0011137540690427602, 0.001084307300706589, 0.0010833806293301803, 0.001086927395378954, 0.0010886116056204882]
[895.6779103679977, 881.0222163228996, 879.817155600522, 879.8987578116985, 882.6012905552438, 811.2376467823099, 887.0867943561426, 885.5627321849246, 887.0064639199054, 684.4621641724411, 703.1353232705635, 881.2130602580089, 878.3088868978864, 866.6625641085471, 886.1241103963373, 54.9280487167404, 45.71528342713515, 849.5979523966976, 871.6887781494069, 872.7514495783199, 848.6281277599306, 859.7973277113081, 902.6028546620392, 896.2619333834446, 894.0456348310051, 905.4381446656776, 904.7049452496257, 904.8408374905265, 897.7185793832285, 885.3950776707043, 839.6688802287912, 841.9725436628273, 920.541591452207, 912.9871836480079, 914.9699300424028, 915.5606908801152, 865.7831014245425, 864.2589124059353, 859.2867577505158, 862.5489521764888, 863.6998662271016, 845.7113848909283, 913.5451446621388, 913.792281778052, 917.7292244362258, 916.5507010554583, 919.246837852784, 914.7735076434642, 921.729990682114, 811.2543855165322, 814.6583947512371, 895.3284052995449, 897.7096404899125, 896.8139992874187, 888.6414113337285, 893.7219520673647, 900.635089133147, 896.6758923958208, 899.0478778903324, 902.9701464653812, 901.4616465051812, 896.6720395283098, 899.7488339105911, 898.6605183938234, 902.9815811378974, 900.3131226841979, 904.315796345631, 914.3079346678502, 904.9170792175446, 893.3988406914694, 894.7973063627061, 889.0995720270964, 902.4606263792425, 899.4881040532216, 899.7014120599296, 898.3866613055086, 907.2539400566221, 622.8188842147018, 938.7468743064404, 913.5203230771815, 898.3648525044638, 895.9905618730294, 888.1827849989423, 22.66027944460689, 180.55305380129403, 862.5540401530195, 875.2501664941709, 873.1166922690579, 883.233346508086, 879.7386436712376, 881.5643143523281, 879.5985272288469, 881.9893880135228, 875.2024425008859, 880.7744753166445, 868.0972454841625, 878.603983894391, 884.3900459546569, 885.4050506801589, 809.2018060982033, 877.5898326629177, 872.2827538982701, 885.2447732802856, 882.0766502433635, 888.524148169969, 835.852991590013, 821.9533893981305, 895.4454004985383, 878.275866969171, 861.6436024011489, 886.7136497389165, 874.0940146777161, 879.9058647112217, 824.8734107828599, 889.5563993985429, 828.4694465130607, 882.7986271715976, 886.650185836928, 884.5897549745075, 845.965633690491, 890.5089241585912, 887.2940517063718, 883.4462572088706, 886.8491624621043, 699.0621610572146, 865.5600303704051, 829.9492314367129, 89.21960082903165, 51.500875149893425, 798.5594719215309, 885.2296633317043, 893.2945899641529, 814.1078769939467, 807.5370464807997, 819.7129918248269, 833.690090546719, 837.1234589319009, 838.2195844810603, 829.1808405753733, 824.871905779183, 835.8511076813229, 909.2738210919865, 907.8268616056653, 917.3380310511093, 914.5132185403727, 909.0233185021971, 912.0873356905502, 917.5402311515992, 910.2463444879533, 909.2840908302874, 917.2867005001524, 911.8802802897956, 908.043913462916, 909.112550821974, 910.5292383365295, 908.1867549499333, 921.2266973624523, 906.9906804150185, 911.2053296323762, 917.9462950184457, 913.9177276025364, 919.128373542468, 911.7958202049958, 912.8409873780664, 915.3124988958713, 915.1844703690239, 917.0632909059206, 912.975011248399, 898.3264509617898, 904.8534212673565, 897.5488101035786, 906.7984923202124, 916.1923694184818, 850.7019972219924, 80.36961140063767, 155.9331570911196, 115.39819581031058, 226.94106943869667, 724.0927077045764, 829.0423158387665, 900.5141301609432, 914.1715575667026, 904.2794170005468, 903.8773138944175, 916.2433593735514, 848.881971978615, 916.8793626595109, 920.4620015704666, 919.6659202170282, 915.4307415879938, 914.5655797279302, 912.6946056316085, 911.3251014746651, 864.7248575669418, 923.2124892362707, 867.0630964697211, 920.712025918965, 921.4156311178615, 876.6896622634225, 908.0765131226832, 835.8530105048019, 918.1947626410307, 905.7518627666873, 923.4820582948456, 923.2867257401529, 859.8525698970984, 920.1187173575454, 927.7021831341655, 922.9685106643618, 876.41786519231, 911.0358840669313, 896.1888021296696, 908.8324953595416, 851.273644560183, 83.63551088133114, 84.76095469285188, 64.25695916121212, 405.22259964040967, 854.4732658166231, 898.572143361525, 853.8216272989109, 908.0360133704373, 917.6906612233054, 916.5529481051918, 891.9331132727119, 904.841196578831, 908.3624134669515, 901.5130166308171, 902.0291261627588, 787.1026749109337, 843.7021889547946, 763.4845450603165, 834.8764473376701, 841.8976179376609, 845.3780209854922, 841.1467067241099, 790.8882899715361, 843.4931428123343, 840.5406921199522, 834.9305585654505, 842.3462582507067, 820.9521024020647, 837.4634882138739, 835.8141632244689, 33.418885501325605, 127.09639143104243, 739.0515137413706, 847.1392408239336, 806.7489382904811, 850.3337297098454, 848.7237030418127, 778.4359487657574, 928.7222099813382, 911.792378744983, 923.2421233289801, 796.6373762819442, 849.5376546512082, 845.219346058215, 858.9512886868478, 911.320060418239, 859.2609311416145, 910.3876426579266, 854.7142765187953, 853.7359171557036, 910.8934007173195, 924.7756317449648, 777.1910273417494, 915.3195122099119, 850.2352687429147, 907.3628142588457, 905.6963485704559, 911.3665315375439, 902.9341203107311, 908.4394062268709, 907.0376086631937, 915.0176899135447, 918.990575315022, 822.4499755271563, 783.7323555671715, 835.9863110403265, 904.8996564081822, 911.6075616671968, 909.9802650416723, 912.4893438284299, 829.5050392539744, 927.3482979269473, 844.4851532060275, 851.830890661923, 909.8735545125835, 917.1691712583786, 854.1080880593647, 916.4664491910811, 930.3747973424513, 914.3493466695564, 917.8227604731725, 917.0568974690381, 916.4485560784915, 916.4698669028097, 919.4462458861083, 903.6305160969019, 915.4223812045097, 910.3898595900577, 851.052594218123, 849.3020888649314, 789.1345353257525, 913.3550022673444, 911.4350899817116, 911.9512351615205, 918.1382011422585, 907.2846065478283, 838.7013644553725, 916.0419439448395, 914.0598965439746, 913.4119268973783, 854.8804464673917, 916.3012697181443, 861.6724207990471, 901.5820459297271, 909.4269354233267, 906.9418418851526, 917.8180577894674, 914.6283354585432, 917.8912330839256, 913.3282104174276, 918.0759805884595, 917.2629849557121, 892.2782733352209, 909.8717838736129, 915.5885875104804, 917.3971348205367, 916.8251341101221, 918.9636095849821, 912.3207747134203, 907.6978714373214, 917.2156759826338, 897.8642842216248, 922.247779156656, 923.0366252886282, 920.0246532118668, 918.6012668218972]
Elapsed: 0.07606833600130973~0.15580585377027137
Time per graph: 0.001764840692964644~0.003609462118105498
Speed: 846.2610315676847~169.34413512862028
Total Time: 0.0479
best val loss: 0.6930575966835022 test_score: 0.4884

Testing...
Test loss: 0.6928 score: 0.5116 time: 0.04s
test Score 0.5116
Epoch Time List: [0.5091898591490462, 0.21777532389387488, 0.22003529395442456, 0.22011822497006506, 0.2203031408134848, 0.22476634592749178, 0.2191435620188713, 0.21867023990489542, 0.21942473202943802, 0.24040018604137003, 0.23323823499958962, 0.22454964509233832, 0.22136349906213582, 0.22192379494663328, 0.22051425103563815, 5.942992649972439, 5.878618599846959, 1.239305712049827, 0.2249449238879606, 0.2225545340916142, 0.2277157239150256, 0.22536815202329308, 0.2140564020955935, 0.21386375499423593, 0.21445986395701766, 0.21330553689040244, 0.21266255201771855, 0.212438968825154, 0.21278830990195274, 0.21516101201996207, 0.21804861701093614, 0.23021274409256876, 0.225886041065678, 0.20996377407573164, 0.21006781898904592, 0.21247607003897429, 0.21933059312868863, 0.2244830859126523, 0.22554177697747946, 0.2244537939550355, 0.22516849800013006, 0.22609717410523444, 0.22071170096751302, 0.21091403788886964, 0.2113282687496394, 0.21063946618232876, 0.21052049985155463, 0.20993879297748208, 0.2100525270216167, 0.22181457886472344, 0.23868121206760406, 0.21834522194694728, 0.21608584106434137, 0.21641623903997242, 0.21619958989322186, 0.21882279694546014, 0.21405489998869598, 0.2178189919795841, 0.21497116517275572, 0.21405988396145403, 0.21427671785932034, 0.21424900798592716, 0.21360759704839438, 0.21444302413146943, 0.21374160086270422, 0.2132547440705821, 0.21571378293447196, 0.21138987189624459, 0.21205091499723494, 0.21451270615216345, 0.21527854702435434, 0.21616175305098295, 0.214629793073982, 0.21412122005131096, 0.21401175600476563, 0.2141044739400968, 0.2130449659889564, 0.2602556520141661, 0.21885462501086295, 0.20828296500258148, 0.21231834590435028, 0.21403194195590913, 0.2158234331291169, 2.642231769161299, 6.233384242863394, 0.27055510913487524, 0.22544380987528712, 0.22108573094010353, 0.22011361713521183, 0.21947362297214568, 0.21869001106824726, 0.22065978101454675, 0.21968839003238827, 0.2194409550866112, 0.2193058280972764, 0.22013180085923523, 0.22089704894460738, 0.2210324329789728, 0.22018862911500037, 0.2292408790672198, 0.23703821003437042, 0.2266188251087442, 0.22419499792158604, 0.22265638515818864, 0.22420148004312068, 0.22272208495996892, 0.22420876694377512, 0.217558188829571, 0.22708560107275844, 0.22407294914592057, 0.22960919397883117, 0.2387108012335375, 0.2248314939206466, 0.21995732805225998, 0.22169142006896436, 0.2250436309259385, 0.2216406058287248, 0.23250281088985503, 0.22271263191942126, 0.22196744591929018, 0.22072558104991913, 0.21727032191120088, 0.22281901899259537, 0.21918572497088462, 0.23566306498833, 0.2683373789768666, 0.22845105815213174, 1.488354678847827, 4.759840441867709, 2.794625661801547, 0.22685276507399976, 0.21853269403800368, 0.22639454808086157, 0.24179879704024643, 0.23948401189409196, 0.23632643814198673, 0.23504728288389742, 0.23531487886793911, 0.23524819395970553, 0.23732292896602303, 0.23653145704884082, 0.2246555269230157, 0.2130938160698861, 0.21290802804287523, 0.2141923678573221, 0.21396676695439965, 0.21336566901300102, 0.2133945298846811, 0.21282320306636393, 0.21868288097903132, 0.21632667700760067, 0.2175051097292453, 0.21446666691917926, 0.2145544158993289, 0.2155083038378507, 0.21541049901861697, 0.21260370698291808, 0.21323900390416384, 0.2131356201134622, 0.21305244299583137, 0.21252805285621434, 0.2127103810198605, 0.21266429394017905, 0.2124105991097167, 0.2126256680348888, 0.2127401059260592, 0.21200777892954648, 0.212758325971663, 0.21881869400385767, 0.21506983297877014, 0.21575891703832895, 0.21543536300305277, 0.2134706120705232, 0.2162340400973335, 3.9610243539791554, 2.0111426011426374, 1.223530926159583, 1.2471187439514324, 0.26980870694387704, 0.25431255297735333, 0.22775063884910196, 0.21829471900127828, 0.21746438404079527, 0.21814564103260636, 0.22136199520900846, 0.21567427786067128, 0.21661152702290565, 0.21513244800735265, 0.22650687908753753, 0.21552444202825427, 0.21707168605644256, 0.21593833400402218, 0.21535623096860945, 0.21871457493398339, 0.21176352503243834, 0.21911886101588607, 0.21381583192851394, 0.21518102404661477, 0.21495254896581173, 0.22194099507760257, 0.23363048408646137, 0.21715587691869587, 0.22130607115104795, 0.2147327169077471, 0.2149813739815727, 0.22721181390807033, 0.21071095194201916, 0.210708640050143, 0.21065969206392765, 0.22742192703299224, 0.21520912693813443, 0.22355602600146085, 0.2178319098893553, 0.221198350074701, 1.134587470907718, 2.1051780249690637, 2.531879846006632, 2.864472347078845, 1.6054952108534053, 0.22055872704368085, 0.22254139196593314, 0.21552252711262554, 0.2141416349913925, 0.21958454814739525, 0.2186494901543483, 0.21822030807379633, 0.223786617978476, 0.21880887693259865, 0.22291716607287526, 0.22969375702086836, 0.2352121148724109, 0.2447654769057408, 0.23416475916747004, 0.23987566318828613, 0.23795786197297275, 0.23521362512838095, 0.24830289802048355, 0.23521796893328428, 0.24426648195367306, 0.24339056585449725, 0.2438112599775195, 0.24249923508614302, 0.2393947591772303, 0.24637141509447247, 4.248307910049334, 3.982974610873498, 1.233023485983722, 0.23963302513584495, 0.24169579800218344, 0.23146689601708204, 0.2312086089514196, 0.2352563429158181, 0.23167480691336095, 0.24928567302413285, 0.21248814393766224, 0.22090183990076184, 0.22916425508446991, 0.2297881320118904, 0.23446002206765115, 0.2287115678191185, 0.2675995910540223, 0.21792722109239548, 0.22367123397998512, 0.21755377610679716, 0.22006934613455087, 0.2171546530444175, 0.22645034489687532, 0.271648914902471, 0.2144668409600854, 0.2171838020440191, 0.2180242749163881, 0.2185939010232687, 0.21747881406918168, 0.2106779778841883, 0.21701069094706327, 0.22158156614750624, 0.21375864907167852, 0.22192037000786513, 0.24529246799647808, 0.2400863110087812, 0.2304876489797607, 0.22128683584742248, 0.21799338201526552, 0.2127987949643284, 0.22259326791390777, 0.21116050926502794, 0.22157787694595754, 0.21948153211269528, 0.21359684597700834, 0.2178874087985605, 0.225381366093643, 0.2156422979896888, 0.21069219300989062, 0.21964530705008656, 0.21705879480578005, 0.21665763203054667, 0.2197657919023186, 0.21196336799766868, 0.21481570589821786, 0.21776814211625606, 0.21678424009587616, 0.21658347896300256, 0.22387254098430276, 0.2374770159367472, 0.24275302898604423, 0.23436523915734142, 0.21594148001167923, 0.21867397089954466, 0.21557390002999455, 0.21598413295578212, 0.21578940888866782, 0.21495771303307265, 0.21053912898059934, 0.2158125260611996, 0.21863491111434996, 0.21337695489637554, 0.2188805480254814, 0.21273435105104, 0.23688166891224682, 0.24126330891158432, 0.2489162189885974, 0.23716056207194924, 0.23676245904061943, 0.2364233258413151, 0.2353986200178042, 0.2365002949954942, 0.23749824112746865, 0.24201670184265822, 0.23479515803046525, 0.2345935080666095, 0.2364724698709324, 0.24362274596933275, 0.23250171111430973, 0.23032481491100043, 0.24079033604357392, 0.2362823380390182, 0.23490169900469482, 0.23544057190883905, 0.24036855204030871, 0.24037853186018765]
Total Epoch List: [20, 298, 22]
Total Time List: [0.050746947061270475, 0.0486709859687835, 0.04788909002672881]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2adba200>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.4397;  Loss pred: 2.4397; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.4474;  Loss pred: 2.4474; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.4457;  Loss pred: 2.4457; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.4451;  Loss pred: 2.4451; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 2.4316;  Loss pred: 2.4316; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.4055;  Loss pred: 2.4055; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.3868;  Loss pred: 2.3868; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 2.3429;  Loss pred: 2.3429; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 9/1000, LR 0.000210
Train loss: 2.3524;  Loss pred: 2.3524; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 006,   Train_Loss: 2.3868,   Val_Loss: 0.6931,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.6931,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6936


[0.04922064999118447, 0.04941669595427811, 0.04924111801665276, 0.049747382989153266, 0.049422334996052086, 0.049253578996285796, 0.049656921066343784, 0.049624510924331844, 0.04941847105510533]
[0.0011186511361632836, 0.0011231067262335935, 0.00111911631856029, 0.0011306223406625743, 0.0011232348862739111, 0.001119399522642859, 0.0011285663878714497, 0.0011278297937348145, 0.001123147069434212]
[893.9337454479064, 890.3873306444889, 893.5621645536098, 884.4686364626175, 890.285738290487, 893.3360965163167, 886.0799069925038, 886.6586124564899, 890.3553481235119]
Elapsed: 0.04944462933215416~0.00018230936541727632
Time per graph: 0.0011237415757307764~4.143394668574468e-06
Speed: 889.8963977208814~3.2775708282147016
Total Time: 0.0497
best val loss: 0.6930846571922302 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.04s
test Score 0.5000
Epoch Time List: [0.23813166713807732, 0.21787546400446445, 0.21795870806090534, 0.21790761197917163, 0.21779315907042474, 0.2183288859669119, 0.21822236198931932, 0.21912875003181398, 0.2178315658820793]
Total Epoch List: [9]
Total Time List: [0.04973241395782679]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2acb55a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.1494;  Loss pred: 3.1494; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 3.0879;  Loss pred: 3.0879; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 3.1184;  Loss pred: 3.1184; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 3.1347;  Loss pred: 3.1347; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 3.1035;  Loss pred: 3.1035; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 3.0945;  Loss pred: 3.0945; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 3.0577;  Loss pred: 3.0577; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 3.0394;  Loss pred: 3.0394; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 2.9749;  Loss pred: 2.9749; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.9178;  Loss pred: 2.9178; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 2.8457;  Loss pred: 2.8457; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 2.8551;  Loss pred: 2.8551; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 2.7695;  Loss pred: 2.7695; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 2.7883;  Loss pred: 2.7883; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 2.7193;  Loss pred: 2.7193; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 2.7068;  Loss pred: 2.7068; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 2.6383;  Loss pred: 2.6383; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 2.5780;  Loss pred: 2.5780; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 2.5280;  Loss pred: 2.5280; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 2.5455;  Loss pred: 2.5455; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 2.4675;  Loss pred: 2.4675; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 22/1000, LR 0.000270
Train loss: 2.4510;  Loss pred: 2.4510; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 2.3975;  Loss pred: 2.3975; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 2.3460;  Loss pred: 2.3460; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 2.3271;  Loss pred: 2.3271; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 2.3157;  Loss pred: 2.3157; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 2.2390;  Loss pred: 2.2390; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 2.1817;  Loss pred: 2.1817; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 2.1863;  Loss pred: 2.1863; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 2.1467;  Loss pred: 2.1467; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 2.1363;  Loss pred: 2.1363; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 2.1220;  Loss pred: 2.1220; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 2.0545;  Loss pred: 2.0545; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 2.0299;  Loss pred: 2.0299; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 2.0201;  Loss pred: 2.0201; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 1.9975;  Loss pred: 1.9975; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 1.9533;  Loss pred: 1.9533; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 1.9224;  Loss pred: 1.9224; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 1.8947;  Loss pred: 1.8947; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 1.8668;  Loss pred: 1.8668; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 1.8292;  Loss pred: 1.8292; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 1.8351;  Loss pred: 1.8351; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 1.7935;  Loss pred: 1.7935; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 1.7576;  Loss pred: 1.7576; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 1.7583;  Loss pred: 1.7583; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 1.7194;  Loss pred: 1.7194; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 1.7181;  Loss pred: 1.7181; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 1.6927;  Loss pred: 1.6927; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 1.6891;  Loss pred: 1.6891; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 1.6702;  Loss pred: 1.6702; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 1.6218;  Loss pred: 1.6218; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 1.6025;  Loss pred: 1.6025; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 1.5744;  Loss pred: 1.5744; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 1.5592;  Loss pred: 1.5592; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 1.5446;  Loss pred: 1.5446; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 1.5319;  Loss pred: 1.5319; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 1.5392;  Loss pred: 1.5392; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 1.4916;  Loss pred: 1.4916; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 1.4814;  Loss pred: 1.4814; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 1.4599;  Loss pred: 1.4599; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 1.4490;  Loss pred: 1.4490; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4884 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 1.4431;  Loss pred: 1.4431; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 1.4171;  Loss pred: 1.4171; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 1.4288;  Loss pred: 1.4288; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4884 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 1.4060;  Loss pred: 1.4060; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 1.3889;  Loss pred: 1.3889; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 1.3780;  Loss pred: 1.3780; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 1.3824;  Loss pred: 1.3824; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.3570;  Loss pred: 1.3570; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4884 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 1.3458;  Loss pred: 1.3458; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 1.3171;  Loss pred: 1.3171; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4884 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 1.3202;  Loss pred: 1.3202; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6862 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4884 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 1.3093;  Loss pred: 1.3093; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4884 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 1.3005;  Loss pred: 1.3005; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.4884 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 1.2851;  Loss pred: 1.2851; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6851 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.4884 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 1.2718;  Loss pred: 1.2718; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.4884 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 1.2790;  Loss pred: 1.2790; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.4884 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 1.2649;  Loss pred: 1.2649; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4884 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 1.2492;  Loss pred: 1.2492; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6836 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.4884 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 1.2365;  Loss pred: 1.2365; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.4884 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 1.2342;  Loss pred: 1.2342; Loss self: 0.0000; time: 0.13s
Val loss: 0.6828 score: 0.5227 time: 0.04s
Test loss: 0.6854 score: 0.5116 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 1.2147;  Loss pred: 1.2147; Loss self: 0.0000; time: 0.14s
Val loss: 0.6824 score: 0.5455 time: 0.04s
Test loss: 0.6851 score: 0.5116 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 1.2201;  Loss pred: 1.2201; Loss self: 0.0000; time: 0.14s
Val loss: 0.6819 score: 0.5682 time: 0.04s
Test loss: 0.6847 score: 0.5116 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.2105;  Loss pred: 1.2105; Loss self: 0.0000; time: 0.13s
Val loss: 0.6814 score: 0.5909 time: 0.04s
Test loss: 0.6844 score: 0.5116 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 1.2032;  Loss pred: 1.2032; Loss self: 0.0000; time: 0.13s
Val loss: 0.6810 score: 0.6136 time: 0.04s
Test loss: 0.6840 score: 0.5349 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 1.1911;  Loss pred: 1.1911; Loss self: 0.0000; time: 0.13s
Val loss: 0.6805 score: 0.6136 time: 0.05s
Test loss: 0.6836 score: 0.5581 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 1.1886;  Loss pred: 1.1886; Loss self: 0.0000; time: 0.14s
Val loss: 0.6800 score: 0.6136 time: 0.04s
Test loss: 0.6832 score: 0.5581 time: 0.04s
Epoch 88/1000, LR 0.000266
Train loss: 1.1723;  Loss pred: 1.1723; Loss self: 0.0000; time: 0.14s
Val loss: 0.6794 score: 0.6136 time: 0.04s
Test loss: 0.6828 score: 0.5581 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 1.1754;  Loss pred: 1.1754; Loss self: 0.0000; time: 0.13s
Val loss: 0.6789 score: 0.6136 time: 0.04s
Test loss: 0.6824 score: 0.5581 time: 0.04s
Epoch 90/1000, LR 0.000266
Train loss: 1.1604;  Loss pred: 1.1604; Loss self: 0.0000; time: 0.13s
Val loss: 0.6783 score: 0.6136 time: 0.05s
Test loss: 0.6819 score: 0.5581 time: 0.05s
Epoch 91/1000, LR 0.000266
Train loss: 1.1615;  Loss pred: 1.1615; Loss self: 0.0000; time: 0.14s
Val loss: 0.6777 score: 0.6591 time: 0.04s
Test loss: 0.6815 score: 0.5814 time: 0.04s
Epoch 92/1000, LR 0.000266
Train loss: 1.1512;  Loss pred: 1.1512; Loss self: 0.0000; time: 0.13s
Val loss: 0.6771 score: 0.6591 time: 0.04s
Test loss: 0.6810 score: 0.6047 time: 0.04s
Epoch 93/1000, LR 0.000265
Train loss: 1.1463;  Loss pred: 1.1463; Loss self: 0.0000; time: 0.13s
Val loss: 0.6765 score: 0.7045 time: 0.04s
Test loss: 0.6805 score: 0.6047 time: 0.04s
Epoch 94/1000, LR 0.000265
Train loss: 1.1463;  Loss pred: 1.1463; Loss self: 0.0000; time: 0.13s
Val loss: 0.6759 score: 0.7045 time: 0.04s
Test loss: 0.6800 score: 0.6279 time: 0.04s
Epoch 95/1000, LR 0.000265
Train loss: 1.1432;  Loss pred: 1.1432; Loss self: 0.0000; time: 0.13s
Val loss: 0.6752 score: 0.7045 time: 0.04s
Test loss: 0.6795 score: 0.6279 time: 0.04s
Epoch 96/1000, LR 0.000265
Train loss: 1.1246;  Loss pred: 1.1246; Loss self: 0.0000; time: 0.13s
Val loss: 0.6746 score: 0.7045 time: 0.05s
Test loss: 0.6789 score: 0.6279 time: 0.04s
Epoch 97/1000, LR 0.000265
Train loss: 1.1278;  Loss pred: 1.1278; Loss self: 0.0000; time: 0.14s
Val loss: 0.6739 score: 0.7045 time: 0.04s
Test loss: 0.6784 score: 0.6279 time: 0.05s
Epoch 98/1000, LR 0.000265
Train loss: 1.1257;  Loss pred: 1.1257; Loss self: 0.0000; time: 0.14s
Val loss: 0.6732 score: 0.7045 time: 0.04s
Test loss: 0.6779 score: 0.6279 time: 0.05s
Epoch 99/1000, LR 0.000265
Train loss: 1.1142;  Loss pred: 1.1142; Loss self: 0.0000; time: 0.14s
Val loss: 0.6724 score: 0.7045 time: 0.04s
Test loss: 0.6773 score: 0.6279 time: 0.05s
Epoch 100/1000, LR 0.000265
Train loss: 1.1137;  Loss pred: 1.1137; Loss self: 0.0000; time: 0.14s
Val loss: 0.6716 score: 0.7045 time: 0.04s
Test loss: 0.6766 score: 0.6512 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 1.1037;  Loss pred: 1.1037; Loss self: 0.0000; time: 0.14s
Val loss: 0.6708 score: 0.7045 time: 0.04s
Test loss: 0.6760 score: 0.6512 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 1.0965;  Loss pred: 1.0965; Loss self: 0.0000; time: 0.14s
Val loss: 0.6700 score: 0.7273 time: 0.05s
Test loss: 0.6753 score: 0.6744 time: 0.05s
Epoch 103/1000, LR 0.000264
Train loss: 1.0873;  Loss pred: 1.0873; Loss self: 0.0000; time: 0.14s
Val loss: 0.6691 score: 0.7500 time: 0.04s
Test loss: 0.6745 score: 0.6744 time: 0.05s
Epoch 104/1000, LR 0.000264
Train loss: 1.1006;  Loss pred: 1.1006; Loss self: 0.0000; time: 0.14s
Val loss: 0.6682 score: 0.7500 time: 0.05s
Test loss: 0.6738 score: 0.6744 time: 0.05s
Epoch 105/1000, LR 0.000264
Train loss: 1.0917;  Loss pred: 1.0917; Loss self: 0.0000; time: 0.14s
Val loss: 0.6673 score: 0.7500 time: 0.05s
Test loss: 0.6731 score: 0.6744 time: 0.05s
Epoch 106/1000, LR 0.000264
Train loss: 1.0873;  Loss pred: 1.0873; Loss self: 0.0000; time: 0.14s
Val loss: 0.6664 score: 0.7500 time: 0.05s
Test loss: 0.6724 score: 0.6744 time: 0.05s
Epoch 107/1000, LR 0.000264
Train loss: 1.0794;  Loss pred: 1.0794; Loss self: 0.0000; time: 0.14s
Val loss: 0.6655 score: 0.7500 time: 0.05s
Test loss: 0.6717 score: 0.6744 time: 0.05s
Epoch 108/1000, LR 0.000264
Train loss: 1.0720;  Loss pred: 1.0720; Loss self: 0.0000; time: 0.14s
Val loss: 0.6645 score: 0.7727 time: 0.05s
Test loss: 0.6709 score: 0.6744 time: 0.05s
Epoch 109/1000, LR 0.000264
Train loss: 1.0734;  Loss pred: 1.0734; Loss self: 0.0000; time: 0.14s
Val loss: 0.6635 score: 0.7955 time: 0.05s
Test loss: 0.6701 score: 0.6744 time: 0.05s
Epoch 110/1000, LR 0.000263
Train loss: 1.0664;  Loss pred: 1.0664; Loss self: 0.0000; time: 0.14s
Val loss: 0.6624 score: 0.7955 time: 0.05s
Test loss: 0.6692 score: 0.6744 time: 0.05s
Epoch 111/1000, LR 0.000263
Train loss: 1.0590;  Loss pred: 1.0590; Loss self: 0.0000; time: 0.14s
Val loss: 0.6613 score: 0.7955 time: 1.64s
Test loss: 0.6684 score: 0.6744 time: 1.62s
Epoch 112/1000, LR 0.000263
Train loss: 1.0622;  Loss pred: 1.0622; Loss self: 0.0000; time: 5.42s
Val loss: 0.6602 score: 0.7955 time: 1.71s
Test loss: 0.6675 score: 0.6744 time: 1.18s
Epoch 113/1000, LR 0.000263
Train loss: 1.0536;  Loss pred: 1.0536; Loss self: 0.0000; time: 2.88s
Val loss: 0.6591 score: 0.7955 time: 1.65s
Test loss: 0.6667 score: 0.6744 time: 0.21s
Epoch 114/1000, LR 0.000263
Train loss: 1.0545;  Loss pred: 1.0545; Loss self: 0.0000; time: 0.31s
Val loss: 0.6580 score: 0.7955 time: 0.04s
Test loss: 0.6658 score: 0.6744 time: 0.05s
Epoch 115/1000, LR 0.000263
Train loss: 1.0445;  Loss pred: 1.0445; Loss self: 0.0000; time: 0.13s
Val loss: 0.6569 score: 0.7955 time: 0.04s
Test loss: 0.6650 score: 0.6744 time: 0.04s
Epoch 116/1000, LR 0.000263
Train loss: 1.0423;  Loss pred: 1.0423; Loss self: 0.0000; time: 0.13s
Val loss: 0.6558 score: 0.7955 time: 0.04s
Test loss: 0.6642 score: 0.6744 time: 0.04s
Epoch 117/1000, LR 0.000262
Train loss: 1.0406;  Loss pred: 1.0406; Loss self: 0.0000; time: 0.13s
Val loss: 0.6546 score: 0.7955 time: 0.04s
Test loss: 0.6632 score: 0.6744 time: 0.04s
Epoch 118/1000, LR 0.000262
Train loss: 1.0392;  Loss pred: 1.0392; Loss self: 0.0000; time: 0.13s
Val loss: 0.6534 score: 0.7955 time: 0.04s
Test loss: 0.6623 score: 0.6744 time: 0.04s
Epoch 119/1000, LR 0.000262
Train loss: 1.0330;  Loss pred: 1.0330; Loss self: 0.0000; time: 0.13s
Val loss: 0.6520 score: 0.8182 time: 0.05s
Test loss: 0.6612 score: 0.6744 time: 0.04s
Epoch 120/1000, LR 0.000262
Train loss: 1.0293;  Loss pred: 1.0293; Loss self: 0.0000; time: 0.13s
Val loss: 0.6507 score: 0.8182 time: 0.04s
Test loss: 0.6601 score: 0.6744 time: 0.04s
Epoch 121/1000, LR 0.000262
Train loss: 1.0241;  Loss pred: 1.0241; Loss self: 0.0000; time: 0.13s
Val loss: 0.6493 score: 0.8182 time: 0.04s
Test loss: 0.6590 score: 0.7209 time: 0.04s
Epoch 122/1000, LR 0.000262
Train loss: 1.0252;  Loss pred: 1.0252; Loss self: 0.0000; time: 0.13s
Val loss: 0.6479 score: 0.8182 time: 0.04s
Test loss: 0.6578 score: 0.7209 time: 0.04s
Epoch 123/1000, LR 0.000262
Train loss: 1.0179;  Loss pred: 1.0179; Loss self: 0.0000; time: 0.13s
Val loss: 0.6465 score: 0.8182 time: 0.04s
Test loss: 0.6567 score: 0.7209 time: 0.04s
Epoch 124/1000, LR 0.000261
Train loss: 1.0180;  Loss pred: 1.0180; Loss self: 0.0000; time: 0.13s
Val loss: 0.6451 score: 0.8182 time: 0.04s
Test loss: 0.6555 score: 0.7442 time: 0.04s
Epoch 125/1000, LR 0.000261
Train loss: 1.0112;  Loss pred: 1.0112; Loss self: 0.0000; time: 0.13s
Val loss: 0.6436 score: 0.8182 time: 0.04s
Test loss: 0.6544 score: 0.7442 time: 0.04s
Epoch 126/1000, LR 0.000261
Train loss: 1.0131;  Loss pred: 1.0131; Loss self: 0.0000; time: 0.13s
Val loss: 0.6421 score: 0.8182 time: 0.04s
Test loss: 0.6532 score: 0.7209 time: 0.04s
Epoch 127/1000, LR 0.000261
Train loss: 1.0078;  Loss pred: 1.0078; Loss self: 0.0000; time: 0.13s
Val loss: 0.6406 score: 0.8182 time: 0.04s
Test loss: 0.6520 score: 0.7209 time: 0.05s
Epoch 128/1000, LR 0.000261
Train loss: 1.0056;  Loss pred: 1.0056; Loss self: 0.0000; time: 0.13s
Val loss: 0.6391 score: 0.8182 time: 0.04s
Test loss: 0.6508 score: 0.7209 time: 0.04s
Epoch 129/1000, LR 0.000261
Train loss: 0.9959;  Loss pred: 0.9959; Loss self: 0.0000; time: 0.13s
Val loss: 0.6376 score: 0.8182 time: 0.04s
Test loss: 0.6496 score: 0.7209 time: 0.04s
Epoch 130/1000, LR 0.000260
Train loss: 0.9971;  Loss pred: 0.9971; Loss self: 0.0000; time: 0.13s
Val loss: 0.6360 score: 0.8182 time: 0.04s
Test loss: 0.6484 score: 0.7209 time: 0.04s
Epoch 131/1000, LR 0.000260
Train loss: 0.9922;  Loss pred: 0.9922; Loss self: 0.0000; time: 0.13s
Val loss: 0.6344 score: 0.8182 time: 0.04s
Test loss: 0.6472 score: 0.7209 time: 0.04s
Epoch 132/1000, LR 0.000260
Train loss: 0.9905;  Loss pred: 0.9905; Loss self: 0.0000; time: 0.13s
Val loss: 0.6328 score: 0.8182 time: 0.04s
Test loss: 0.6459 score: 0.7442 time: 0.04s
Epoch 133/1000, LR 0.000260
Train loss: 0.9876;  Loss pred: 0.9876; Loss self: 0.0000; time: 0.14s
Val loss: 0.6310 score: 0.8182 time: 0.04s
Test loss: 0.6445 score: 0.7674 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 0.9830;  Loss pred: 0.9830; Loss self: 0.0000; time: 0.13s
Val loss: 0.6293 score: 0.8182 time: 0.04s
Test loss: 0.6431 score: 0.7907 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9809;  Loss pred: 0.9809; Loss self: 0.0000; time: 0.13s
Val loss: 0.6275 score: 0.8182 time: 0.04s
Test loss: 0.6417 score: 0.7907 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9791;  Loss pred: 0.9791; Loss self: 0.0000; time: 0.13s
Val loss: 0.6258 score: 0.8182 time: 0.05s
Test loss: 0.6403 score: 0.7907 time: 0.05s
Epoch 137/1000, LR 0.000259
Train loss: 0.9733;  Loss pred: 0.9733; Loss self: 0.0000; time: 0.13s
Val loss: 0.6240 score: 0.8182 time: 0.04s
Test loss: 0.6389 score: 0.7907 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9735;  Loss pred: 0.9735; Loss self: 0.0000; time: 0.13s
Val loss: 0.6221 score: 0.8182 time: 0.04s
Test loss: 0.6374 score: 0.7907 time: 0.04s
Epoch 139/1000, LR 0.000259
Train loss: 0.9691;  Loss pred: 0.9691; Loss self: 0.0000; time: 0.13s
Val loss: 0.6202 score: 0.8182 time: 0.04s
Test loss: 0.6359 score: 0.7907 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9665;  Loss pred: 0.9665; Loss self: 0.0000; time: 0.13s
Val loss: 0.6184 score: 0.8182 time: 0.04s
Test loss: 0.6345 score: 0.7907 time: 0.04s
Epoch 141/1000, LR 0.000259
Train loss: 0.9624;  Loss pred: 0.9624; Loss self: 0.0000; time: 0.14s
Val loss: 0.6165 score: 0.8182 time: 0.05s
Test loss: 0.6330 score: 0.7907 time: 0.04s
Epoch 142/1000, LR 0.000259
Train loss: 0.9550;  Loss pred: 0.9550; Loss self: 0.0000; time: 0.13s
Val loss: 0.6145 score: 0.8182 time: 0.04s
Test loss: 0.6314 score: 0.7907 time: 0.04s
Epoch 143/1000, LR 0.000258
Train loss: 0.9518;  Loss pred: 0.9518; Loss self: 0.0000; time: 0.13s
Val loss: 0.6125 score: 0.8182 time: 0.04s
Test loss: 0.6297 score: 0.7907 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9511;  Loss pred: 0.9511; Loss self: 0.0000; time: 0.13s
Val loss: 0.6104 score: 0.8182 time: 0.04s
Test loss: 0.6280 score: 0.7907 time: 0.04s
Epoch 145/1000, LR 0.000258
Train loss: 0.9481;  Loss pred: 0.9481; Loss self: 0.0000; time: 0.14s
Val loss: 0.6083 score: 0.8182 time: 0.04s
Test loss: 0.6264 score: 0.7907 time: 0.04s
Epoch 146/1000, LR 0.000258
Train loss: 0.9459;  Loss pred: 0.9459; Loss self: 0.0000; time: 0.13s
Val loss: 0.6063 score: 0.8182 time: 0.04s
Test loss: 0.6248 score: 0.7907 time: 0.04s
Epoch 147/1000, LR 0.000258
Train loss: 0.9421;  Loss pred: 0.9421; Loss self: 0.0000; time: 0.13s
Val loss: 0.6042 score: 0.8182 time: 0.04s
Test loss: 0.6231 score: 0.7907 time: 0.04s
Epoch 148/1000, LR 0.000257
Train loss: 0.9422;  Loss pred: 0.9422; Loss self: 0.0000; time: 0.13s
Val loss: 0.6021 score: 0.8182 time: 0.04s
Test loss: 0.6215 score: 0.7907 time: 0.04s
Epoch 149/1000, LR 0.000257
Train loss: 0.9352;  Loss pred: 0.9352; Loss self: 0.0000; time: 0.13s
Val loss: 0.6000 score: 0.8182 time: 0.04s
Test loss: 0.6198 score: 0.7907 time: 0.04s
Epoch 150/1000, LR 0.000257
Train loss: 0.9317;  Loss pred: 0.9317; Loss self: 0.0000; time: 0.13s
Val loss: 0.5979 score: 0.8182 time: 0.04s
Test loss: 0.6182 score: 0.7907 time: 0.04s
Epoch 151/1000, LR 0.000257
Train loss: 0.9282;  Loss pred: 0.9282; Loss self: 0.0000; time: 0.13s
Val loss: 0.5957 score: 0.8182 time: 0.04s
Test loss: 0.6164 score: 0.7907 time: 0.04s
Epoch 152/1000, LR 0.000257
Train loss: 0.9272;  Loss pred: 0.9272; Loss self: 0.0000; time: 0.13s
Val loss: 0.5934 score: 0.8182 time: 0.04s
Test loss: 0.6146 score: 0.7907 time: 0.04s
Epoch 153/1000, LR 0.000257
Train loss: 0.9272;  Loss pred: 0.9272; Loss self: 0.0000; time: 0.14s
Val loss: 0.5910 score: 0.8182 time: 0.05s
Test loss: 0.6126 score: 0.7907 time: 0.05s
Epoch 154/1000, LR 0.000256
Train loss: 0.9218;  Loss pred: 0.9218; Loss self: 0.0000; time: 0.14s
Val loss: 0.5887 score: 0.8182 time: 0.04s
Test loss: 0.6107 score: 0.7907 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9216;  Loss pred: 0.9216; Loss self: 0.0000; time: 0.13s
Val loss: 0.5864 score: 0.8182 time: 0.04s
Test loss: 0.6088 score: 0.7907 time: 0.04s
Epoch 156/1000, LR 0.000256
Train loss: 0.9168;  Loss pred: 0.9168; Loss self: 0.0000; time: 0.13s
Val loss: 0.5840 score: 0.8182 time: 0.04s
Test loss: 0.6069 score: 0.7907 time: 0.04s
Epoch 157/1000, LR 0.000256
Train loss: 0.9094;  Loss pred: 0.9094; Loss self: 0.0000; time: 0.13s
Val loss: 0.5817 score: 0.8182 time: 0.04s
Test loss: 0.6050 score: 0.7907 time: 0.04s
Epoch 158/1000, LR 0.000256
Train loss: 0.9117;  Loss pred: 0.9117; Loss self: 0.0000; time: 0.13s
Val loss: 0.5793 score: 0.8182 time: 0.04s
Test loss: 0.6032 score: 0.7907 time: 0.04s
Epoch 159/1000, LR 0.000255
Train loss: 0.9077;  Loss pred: 0.9077; Loss self: 0.0000; time: 0.13s
Val loss: 0.5770 score: 0.8182 time: 0.04s
Test loss: 0.6013 score: 0.7907 time: 0.04s
Epoch 160/1000, LR 0.000255
Train loss: 0.9058;  Loss pred: 0.9058; Loss self: 0.0000; time: 0.13s
Val loss: 0.5746 score: 0.8182 time: 0.04s
Test loss: 0.5994 score: 0.7907 time: 0.04s
Epoch 161/1000, LR 0.000255
Train loss: 0.9012;  Loss pred: 0.9012; Loss self: 0.0000; time: 0.13s
Val loss: 0.5722 score: 0.8182 time: 0.04s
Test loss: 0.5975 score: 0.7907 time: 0.04s
Epoch 162/1000, LR 0.000255
Train loss: 0.8924;  Loss pred: 0.8924; Loss self: 0.0000; time: 0.13s
Val loss: 0.5697 score: 0.8182 time: 0.04s
Test loss: 0.5955 score: 0.7907 time: 0.05s
Epoch 163/1000, LR 0.000255
Train loss: 0.8909;  Loss pred: 0.8909; Loss self: 0.0000; time: 0.13s
Val loss: 0.5673 score: 0.8182 time: 0.04s
Test loss: 0.5935 score: 0.7907 time: 0.04s
Epoch 164/1000, LR 0.000254
Train loss: 0.8917;  Loss pred: 0.8917; Loss self: 0.0000; time: 0.13s
Val loss: 0.5648 score: 0.8182 time: 0.04s
Test loss: 0.5916 score: 0.7907 time: 0.04s
Epoch 165/1000, LR 0.000254
Train loss: 0.8860;  Loss pred: 0.8860; Loss self: 0.0000; time: 0.14s
Val loss: 0.5623 score: 0.8182 time: 0.04s
Test loss: 0.5895 score: 0.7907 time: 0.05s
Epoch 166/1000, LR 0.000254
Train loss: 0.8868;  Loss pred: 0.8868; Loss self: 0.0000; time: 0.13s
Val loss: 0.5598 score: 0.8182 time: 0.04s
Test loss: 0.5875 score: 0.7907 time: 0.04s
Epoch 167/1000, LR 0.000254
Train loss: 0.8808;  Loss pred: 0.8808; Loss self: 0.0000; time: 0.13s
Val loss: 0.5573 score: 0.8409 time: 0.04s
Test loss: 0.5855 score: 0.7907 time: 0.04s
Epoch 168/1000, LR 0.000254
Train loss: 0.8770;  Loss pred: 0.8770; Loss self: 0.0000; time: 0.14s
Val loss: 0.5547 score: 0.8409 time: 0.04s
Test loss: 0.5834 score: 0.7907 time: 0.04s
Epoch 169/1000, LR 0.000253
Train loss: 0.8750;  Loss pred: 0.8750; Loss self: 0.0000; time: 0.14s
Val loss: 0.5521 score: 0.8409 time: 0.05s
Test loss: 0.5812 score: 0.7907 time: 0.05s
Epoch 170/1000, LR 0.000253
Train loss: 0.8725;  Loss pred: 0.8725; Loss self: 0.0000; time: 0.14s
Val loss: 0.5497 score: 0.8409 time: 0.05s
Test loss: 0.5793 score: 0.7907 time: 0.04s
Epoch 171/1000, LR 0.000253
Train loss: 0.8671;  Loss pred: 0.8671; Loss self: 0.0000; time: 0.13s
Val loss: 0.5471 score: 0.8409 time: 0.05s
Test loss: 0.5772 score: 0.7907 time: 0.05s
Epoch 172/1000, LR 0.000253
Train loss: 0.8636;  Loss pred: 0.8636; Loss self: 0.0000; time: 0.14s
Val loss: 0.5446 score: 0.8409 time: 0.04s
Test loss: 0.5751 score: 0.7907 time: 0.05s
Epoch 173/1000, LR 0.000253
Train loss: 0.8611;  Loss pred: 0.8611; Loss self: 0.0000; time: 0.14s
Val loss: 0.5420 score: 0.8409 time: 0.05s
Test loss: 0.5731 score: 0.8140 time: 0.04s
Epoch 174/1000, LR 0.000252
Train loss: 0.8566;  Loss pred: 0.8566; Loss self: 0.0000; time: 0.13s
Val loss: 0.5395 score: 0.8409 time: 0.04s
Test loss: 0.5710 score: 0.8140 time: 0.04s
Epoch 175/1000, LR 0.000252
Train loss: 0.8547;  Loss pred: 0.8547; Loss self: 0.0000; time: 0.14s
Val loss: 0.5369 score: 0.8409 time: 0.04s
Test loss: 0.5689 score: 0.8140 time: 0.04s
Epoch 176/1000, LR 0.000252
Train loss: 0.8501;  Loss pred: 0.8501; Loss self: 0.0000; time: 0.13s
Val loss: 0.5344 score: 0.8409 time: 0.04s
Test loss: 0.5668 score: 0.8140 time: 0.04s
Epoch 177/1000, LR 0.000252
Train loss: 0.8472;  Loss pred: 0.8472; Loss self: 0.0000; time: 0.13s
Val loss: 0.5317 score: 0.8409 time: 0.05s
Test loss: 0.5646 score: 0.8140 time: 0.05s
Epoch 178/1000, LR 0.000251
Train loss: 0.8479;  Loss pred: 0.8479; Loss self: 0.0000; time: 0.15s
Val loss: 0.5291 score: 0.8409 time: 0.87s
Test loss: 0.5625 score: 0.8140 time: 1.20s
Epoch 179/1000, LR 0.000251
Train loss: 0.8417;  Loss pred: 0.8417; Loss self: 0.0000; time: 3.04s
Val loss: 0.5265 score: 0.8636 time: 0.05s
Test loss: 0.5603 score: 0.8140 time: 0.05s
Epoch 180/1000, LR 0.000251
Train loss: 0.8376;  Loss pred: 0.8376; Loss self: 0.0000; time: 0.14s
Val loss: 0.5238 score: 0.8636 time: 0.04s
Test loss: 0.5581 score: 0.8140 time: 0.05s
Epoch 181/1000, LR 0.000251
Train loss: 0.8365;  Loss pred: 0.8365; Loss self: 0.0000; time: 0.14s
Val loss: 0.5211 score: 0.8636 time: 0.04s
Test loss: 0.5557 score: 0.8140 time: 0.04s
Epoch 182/1000, LR 0.000251
Train loss: 0.8333;  Loss pred: 0.8333; Loss self: 0.0000; time: 0.13s
Val loss: 0.5183 score: 0.8636 time: 0.05s
Test loss: 0.5533 score: 0.8140 time: 0.04s
Epoch 183/1000, LR 0.000250
Train loss: 0.8288;  Loss pred: 0.8288; Loss self: 0.0000; time: 0.13s
Val loss: 0.5155 score: 0.8636 time: 0.04s
Test loss: 0.5509 score: 0.8140 time: 0.04s
Epoch 184/1000, LR 0.000250
Train loss: 0.8266;  Loss pred: 0.8266; Loss self: 0.0000; time: 0.13s
Val loss: 0.5128 score: 0.8636 time: 0.04s
Test loss: 0.5486 score: 0.8140 time: 0.04s
Epoch 185/1000, LR 0.000250
Train loss: 0.8187;  Loss pred: 0.8187; Loss self: 0.0000; time: 0.13s
Val loss: 0.5102 score: 0.8636 time: 0.04s
Test loss: 0.5464 score: 0.8140 time: 0.04s
Epoch 186/1000, LR 0.000250
Train loss: 0.8178;  Loss pred: 0.8178; Loss self: 0.0000; time: 0.14s
Val loss: 0.5077 score: 0.8636 time: 0.04s
Test loss: 0.5444 score: 0.8140 time: 0.04s
Epoch 187/1000, LR 0.000249
Train loss: 0.8143;  Loss pred: 0.8143; Loss self: 0.0000; time: 0.14s
Val loss: 0.5052 score: 0.8636 time: 0.04s
Test loss: 0.5423 score: 0.8140 time: 0.04s
Epoch 188/1000, LR 0.000249
Train loss: 0.8128;  Loss pred: 0.8128; Loss self: 0.0000; time: 0.14s
Val loss: 0.5028 score: 0.8636 time: 0.04s
Test loss: 0.5404 score: 0.8140 time: 0.04s
Epoch 189/1000, LR 0.000249
Train loss: 0.8100;  Loss pred: 0.8100; Loss self: 0.0000; time: 0.13s
Val loss: 0.5003 score: 0.8636 time: 0.04s
Test loss: 0.5384 score: 0.8140 time: 0.04s
Epoch 190/1000, LR 0.000249
Train loss: 0.8087;  Loss pred: 0.8087; Loss self: 0.0000; time: 0.13s
Val loss: 0.4978 score: 0.8636 time: 0.04s
Test loss: 0.5363 score: 0.8140 time: 0.04s
Epoch 191/1000, LR 0.000249
Train loss: 0.8037;  Loss pred: 0.8037; Loss self: 0.0000; time: 0.13s
Val loss: 0.4953 score: 0.8636 time: 0.04s
Test loss: 0.5342 score: 0.8140 time: 0.04s
Epoch 192/1000, LR 0.000248
Train loss: 0.8002;  Loss pred: 0.8002; Loss self: 0.0000; time: 0.13s
Val loss: 0.4928 score: 0.8636 time: 0.04s
Test loss: 0.5320 score: 0.8140 time: 0.04s
Epoch 193/1000, LR 0.000248
Train loss: 0.7966;  Loss pred: 0.7966; Loss self: 0.0000; time: 0.14s
Val loss: 0.4902 score: 0.8636 time: 0.04s
Test loss: 0.5298 score: 0.8140 time: 0.04s
Epoch 194/1000, LR 0.000248
Train loss: 0.7951;  Loss pred: 0.7951; Loss self: 0.0000; time: 0.13s
Val loss: 0.4876 score: 0.8636 time: 0.04s
Test loss: 0.5276 score: 0.8140 time: 0.04s
Epoch 195/1000, LR 0.000248
Train loss: 0.7915;  Loss pred: 0.7915; Loss self: 0.0000; time: 0.13s
Val loss: 0.4850 score: 0.8636 time: 0.04s
Test loss: 0.5253 score: 0.8140 time: 0.04s
Epoch 196/1000, LR 0.000247
Train loss: 0.7891;  Loss pred: 0.7891; Loss self: 0.0000; time: 0.13s
Val loss: 0.4824 score: 0.8636 time: 0.04s
Test loss: 0.5231 score: 0.8140 time: 0.04s
Epoch 197/1000, LR 0.000247
Train loss: 0.7844;  Loss pred: 0.7844; Loss self: 0.0000; time: 0.13s
Val loss: 0.4798 score: 0.8636 time: 0.05s
Test loss: 0.5207 score: 0.8140 time: 0.04s
Epoch 198/1000, LR 0.000247
Train loss: 0.7823;  Loss pred: 0.7823; Loss self: 0.0000; time: 0.13s
Val loss: 0.4771 score: 0.8636 time: 0.04s
Test loss: 0.5183 score: 0.8140 time: 0.04s
Epoch 199/1000, LR 0.000247
Train loss: 0.7793;  Loss pred: 0.7793; Loss self: 0.0000; time: 0.14s
Val loss: 0.4745 score: 0.8636 time: 0.04s
Test loss: 0.5160 score: 0.8140 time: 0.04s
Epoch 200/1000, LR 0.000246
Train loss: 0.7801;  Loss pred: 0.7801; Loss self: 0.0000; time: 0.13s
Val loss: 0.4719 score: 0.8636 time: 0.04s
Test loss: 0.5136 score: 0.8140 time: 0.04s
Epoch 201/1000, LR 0.000246
Train loss: 0.7747;  Loss pred: 0.7747; Loss self: 0.0000; time: 0.13s
Val loss: 0.4695 score: 0.8636 time: 0.04s
Test loss: 0.5115 score: 0.8140 time: 0.04s
Epoch 202/1000, LR 0.000246
Train loss: 0.7688;  Loss pred: 0.7688; Loss self: 0.0000; time: 0.13s
Val loss: 0.4673 score: 0.8636 time: 0.05s
Test loss: 0.5097 score: 0.8140 time: 0.05s
Epoch 203/1000, LR 0.000246
Train loss: 0.7672;  Loss pred: 0.7672; Loss self: 0.0000; time: 0.15s
Val loss: 0.4651 score: 0.8636 time: 0.05s
Test loss: 0.5078 score: 0.8140 time: 0.05s
Epoch 204/1000, LR 0.000245
Train loss: 0.7635;  Loss pred: 0.7635; Loss self: 0.0000; time: 0.15s
Val loss: 0.4630 score: 0.8636 time: 0.05s
Test loss: 0.5061 score: 0.8140 time: 0.05s
Epoch 205/1000, LR 0.000245
Train loss: 0.7615;  Loss pred: 0.7615; Loss self: 0.0000; time: 0.14s
Val loss: 0.4608 score: 0.8636 time: 0.04s
Test loss: 0.5042 score: 0.8140 time: 0.05s
Epoch 206/1000, LR 0.000245
Train loss: 0.7589;  Loss pred: 0.7589; Loss self: 0.0000; time: 0.13s
Val loss: 0.4586 score: 0.8636 time: 0.04s
Test loss: 0.5022 score: 0.8140 time: 0.04s
Epoch 207/1000, LR 0.000245
Train loss: 0.7562;  Loss pred: 0.7562; Loss self: 0.0000; time: 0.13s
Val loss: 0.4562 score: 0.8636 time: 0.04s
Test loss: 0.5001 score: 0.8140 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.7540;  Loss pred: 0.7540; Loss self: 0.0000; time: 0.13s
Val loss: 0.4537 score: 0.8636 time: 0.04s
Test loss: 0.4977 score: 0.8140 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.7504;  Loss pred: 0.7504; Loss self: 0.0000; time: 0.13s
Val loss: 0.4512 score: 0.8636 time: 0.04s
Test loss: 0.4953 score: 0.8140 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.7493;  Loss pred: 0.7493; Loss self: 0.0000; time: 0.13s
Val loss: 0.4486 score: 0.8636 time: 0.04s
Test loss: 0.4929 score: 0.8140 time: 0.04s
Epoch 211/1000, LR 0.000244
Train loss: 0.7446;  Loss pred: 0.7446; Loss self: 0.0000; time: 0.13s
Val loss: 0.4461 score: 0.8636 time: 0.04s
Test loss: 0.4903 score: 0.8140 time: 0.04s
Epoch 212/1000, LR 0.000243
Train loss: 0.7428;  Loss pred: 0.7428; Loss self: 0.0000; time: 0.13s
Val loss: 0.4438 score: 0.8636 time: 0.04s
Test loss: 0.4881 score: 0.8140 time: 0.04s
Epoch 213/1000, LR 0.000243
Train loss: 0.7420;  Loss pred: 0.7420; Loss self: 0.0000; time: 0.13s
Val loss: 0.4417 score: 0.8636 time: 0.04s
Test loss: 0.4862 score: 0.8140 time: 0.04s
Epoch 214/1000, LR 0.000243
Train loss: 0.7387;  Loss pred: 0.7387; Loss self: 0.0000; time: 0.13s
Val loss: 0.4398 score: 0.8636 time: 0.05s
Test loss: 0.4845 score: 0.8140 time: 0.05s
Epoch 215/1000, LR 0.000243
Train loss: 0.7372;  Loss pred: 0.7372; Loss self: 0.0000; time: 0.15s
Val loss: 0.4381 score: 0.8636 time: 0.05s
Test loss: 0.4831 score: 0.8140 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.7331;  Loss pred: 0.7331; Loss self: 0.0000; time: 0.14s
Val loss: 0.4365 score: 0.8636 time: 0.05s
Test loss: 0.4819 score: 0.8140 time: 0.05s
Epoch 217/1000, LR 0.000242
Train loss: 0.7281;  Loss pred: 0.7281; Loss self: 0.0000; time: 0.13s
Val loss: 0.4348 score: 0.8636 time: 0.04s
Test loss: 0.4803 score: 0.8140 time: 0.04s
Epoch 218/1000, LR 0.000242
Train loss: 0.7265;  Loss pred: 0.7265; Loss self: 0.0000; time: 0.13s
Val loss: 0.4329 score: 0.8636 time: 0.04s
Test loss: 0.4786 score: 0.8140 time: 0.04s
Epoch 219/1000, LR 0.000242
Train loss: 0.7246;  Loss pred: 0.7246; Loss self: 0.0000; time: 0.13s
Val loss: 0.4308 score: 0.8636 time: 0.04s
Test loss: 0.4765 score: 0.8140 time: 0.04s
Epoch 220/1000, LR 0.000241
Train loss: 0.7218;  Loss pred: 0.7218; Loss self: 0.0000; time: 0.13s
Val loss: 0.4288 score: 0.8636 time: 0.04s
Test loss: 0.4746 score: 0.8140 time: 0.04s
Epoch 221/1000, LR 0.000241
Train loss: 0.7203;  Loss pred: 0.7203; Loss self: 0.0000; time: 0.13s
Val loss: 0.4268 score: 0.8636 time: 0.04s
Test loss: 0.4726 score: 0.8140 time: 0.05s
Epoch 222/1000, LR 0.000241
Train loss: 0.7177;  Loss pred: 0.7177; Loss self: 0.0000; time: 0.14s
Val loss: 0.4248 score: 0.8636 time: 0.04s
Test loss: 0.4707 score: 0.8140 time: 0.04s
Epoch 223/1000, LR 0.000241
Train loss: 0.7157;  Loss pred: 0.7157; Loss self: 0.0000; time: 0.13s
Val loss: 0.4230 score: 0.8636 time: 0.04s
Test loss: 0.4690 score: 0.8140 time: 0.04s
Epoch 224/1000, LR 0.000240
Train loss: 0.7120;  Loss pred: 0.7120; Loss self: 0.0000; time: 0.13s
Val loss: 0.4211 score: 0.8636 time: 0.04s
Test loss: 0.4672 score: 0.8140 time: 0.04s
Epoch 225/1000, LR 0.000240
Train loss: 0.7136;  Loss pred: 0.7136; Loss self: 0.0000; time: 0.13s
Val loss: 0.4193 score: 0.8636 time: 0.04s
Test loss: 0.4653 score: 0.8140 time: 0.04s
Epoch 226/1000, LR 0.000240
Train loss: 0.7087;  Loss pred: 0.7087; Loss self: 0.0000; time: 0.13s
Val loss: 0.4175 score: 0.8636 time: 0.04s
Test loss: 0.4636 score: 0.8140 time: 0.05s
Epoch 227/1000, LR 0.000240
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.14s
Val loss: 0.4158 score: 0.8636 time: 0.04s
Test loss: 0.4619 score: 0.8140 time: 0.04s
Epoch 228/1000, LR 0.000239
Train loss: 0.7027;  Loss pred: 0.7027; Loss self: 0.0000; time: 0.13s
Val loss: 0.4142 score: 0.8636 time: 0.04s
Test loss: 0.4604 score: 0.8140 time: 0.04s
Epoch 229/1000, LR 0.000239
Train loss: 0.7021;  Loss pred: 0.7021; Loss self: 0.0000; time: 0.13s
Val loss: 0.4125 score: 0.8636 time: 0.04s
Test loss: 0.4588 score: 0.8140 time: 0.04s
Epoch 230/1000, LR 0.000239
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.13s
Val loss: 0.4109 score: 0.8636 time: 0.04s
Test loss: 0.4573 score: 0.8140 time: 0.04s
Epoch 231/1000, LR 0.000238
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.13s
Val loss: 0.4095 score: 0.8636 time: 0.04s
Test loss: 0.4559 score: 0.8140 time: 0.04s
Epoch 232/1000, LR 0.000238
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.13s
Val loss: 0.4081 score: 0.8636 time: 0.04s
Test loss: 0.4546 score: 0.8140 time: 0.04s
Epoch 233/1000, LR 0.000238
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.14s
Val loss: 0.4067 score: 0.8636 time: 0.04s
Test loss: 0.4533 score: 0.8140 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.13s
Val loss: 0.4054 score: 0.8636 time: 0.04s
Test loss: 0.4520 score: 0.8140 time: 0.04s
Epoch 235/1000, LR 0.000237
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.13s
Val loss: 0.4040 score: 0.8636 time: 0.04s
Test loss: 0.4507 score: 0.8140 time: 0.05s
Epoch 236/1000, LR 0.000237
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.13s
Val loss: 0.4025 score: 0.8636 time: 0.04s
Test loss: 0.4492 score: 0.8140 time: 0.04s
Epoch 237/1000, LR 0.000237
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.13s
Val loss: 0.4010 score: 0.8636 time: 0.04s
Test loss: 0.4477 score: 0.8140 time: 0.04s
Epoch 238/1000, LR 0.000236
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.13s
Val loss: 0.3996 score: 0.8636 time: 0.04s
Test loss: 0.4462 score: 0.8140 time: 0.04s
Epoch 239/1000, LR 0.000236
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.14s
Val loss: 0.3986 score: 0.8636 time: 0.04s
Test loss: 0.4452 score: 0.8140 time: 0.04s
Epoch 240/1000, LR 0.000236
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.13s
Val loss: 0.3973 score: 0.8636 time: 0.04s
Test loss: 0.4440 score: 0.8140 time: 0.04s
Epoch 241/1000, LR 0.000236
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.14s
Val loss: 0.3960 score: 0.8636 time: 0.04s
Test loss: 0.4427 score: 0.8140 time: 0.04s
Epoch 242/1000, LR 0.000235
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.13s
Val loss: 0.3945 score: 0.8636 time: 0.04s
Test loss: 0.4412 score: 0.8140 time: 0.04s
Epoch 243/1000, LR 0.000235
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.13s
Val loss: 0.3931 score: 0.8636 time: 0.05s
Test loss: 0.4396 score: 0.8140 time: 0.05s
Epoch 244/1000, LR 0.000235
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.15s
Val loss: 0.3915 score: 0.8636 time: 0.05s
Test loss: 0.4379 score: 0.8140 time: 0.05s
Epoch 245/1000, LR 0.000234
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.15s
Val loss: 0.3899 score: 0.8636 time: 0.05s
Test loss: 0.4362 score: 0.8140 time: 0.05s
Epoch 246/1000, LR 0.000234
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.15s
Val loss: 0.3884 score: 0.8636 time: 0.05s
Test loss: 0.4347 score: 0.8140 time: 0.05s
Epoch 247/1000, LR 0.000234
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.15s
Val loss: 0.3870 score: 0.8636 time: 0.05s
Test loss: 0.4331 score: 0.8140 time: 0.05s
Epoch 248/1000, LR 0.000234
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.13s
Val loss: 0.3857 score: 0.8636 time: 0.04s
Test loss: 0.4317 score: 0.8140 time: 0.04s
Epoch 249/1000, LR 0.000233
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.13s
Val loss: 0.3847 score: 0.8636 time: 0.04s
Test loss: 0.4308 score: 0.8140 time: 0.04s
Epoch 250/1000, LR 0.000233
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.13s
Val loss: 0.3841 score: 0.8636 time: 0.04s
Test loss: 0.4303 score: 0.8140 time: 0.04s
Epoch 251/1000, LR 0.000233
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.13s
Val loss: 0.3834 score: 0.8636 time: 0.04s
Test loss: 0.4296 score: 0.8140 time: 0.04s
Epoch 252/1000, LR 0.000232
Train loss: 0.6605;  Loss pred: 0.6605; Loss self: 0.0000; time: 0.13s
Val loss: 0.3828 score: 0.8636 time: 0.04s
Test loss: 0.4290 score: 0.8140 time: 0.04s
Epoch 253/1000, LR 0.000232
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.13s
Val loss: 0.3819 score: 0.8636 time: 0.04s
Test loss: 0.4282 score: 0.8140 time: 0.04s
Epoch 254/1000, LR 0.000232
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 0.13s
Val loss: 0.3810 score: 0.8636 time: 0.04s
Test loss: 0.4272 score: 0.8140 time: 0.05s
Epoch 255/1000, LR 0.000232
Train loss: 0.6577;  Loss pred: 0.6577; Loss self: 0.0000; time: 0.14s
Val loss: 0.3799 score: 0.8636 time: 0.04s
Test loss: 0.4260 score: 0.8140 time: 0.05s
Epoch 256/1000, LR 0.000231
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.14s
Val loss: 0.3790 score: 0.8636 time: 0.05s
Test loss: 0.4251 score: 0.8140 time: 0.05s
Epoch 257/1000, LR 0.000231
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.14s
Val loss: 0.3778 score: 0.8636 time: 0.05s
Test loss: 0.4238 score: 0.8140 time: 0.05s
Epoch 258/1000, LR 0.000231
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.14s
Val loss: 0.3767 score: 0.8636 time: 0.04s
Test loss: 0.4226 score: 0.8140 time: 0.04s
Epoch 259/1000, LR 0.000230
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.13s
Val loss: 0.3758 score: 0.8636 time: 0.04s
Test loss: 0.4216 score: 0.8140 time: 0.04s
Epoch 260/1000, LR 0.000230
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 0.13s
Val loss: 0.3748 score: 0.8636 time: 0.04s
Test loss: 0.4206 score: 0.8140 time: 0.04s
Epoch 261/1000, LR 0.000230
Train loss: 0.6516;  Loss pred: 0.6516; Loss self: 0.0000; time: 0.13s
Val loss: 0.3737 score: 0.8636 time: 0.04s
Test loss: 0.4194 score: 0.8140 time: 0.04s
Epoch 262/1000, LR 0.000229
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 0.13s
Val loss: 0.3725 score: 0.8636 time: 0.04s
Test loss: 0.4180 score: 0.8140 time: 0.04s
Epoch 263/1000, LR 0.000229
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.13s
Val loss: 0.3717 score: 0.8636 time: 0.04s
Test loss: 0.4172 score: 0.8140 time: 0.04s
Epoch 264/1000, LR 0.000229
Train loss: 0.6442;  Loss pred: 0.6442; Loss self: 0.0000; time: 0.13s
Val loss: 0.3712 score: 0.8636 time: 0.04s
Test loss: 0.4167 score: 0.8140 time: 0.04s
Epoch 265/1000, LR 0.000228
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.13s
Val loss: 0.3705 score: 0.8636 time: 0.04s
Test loss: 0.4159 score: 0.8140 time: 0.04s
Epoch 266/1000, LR 0.000228
Train loss: 0.6415;  Loss pred: 0.6415; Loss self: 0.0000; time: 0.13s
Val loss: 0.3697 score: 0.8636 time: 0.04s
Test loss: 0.4151 score: 0.8140 time: 0.05s
Epoch 267/1000, LR 0.000228
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.14s
Val loss: 0.3691 score: 0.8636 time: 0.05s
Test loss: 0.4145 score: 0.8140 time: 0.05s
Epoch 268/1000, LR 0.000228
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 0.14s
Val loss: 0.3683 score: 0.8636 time: 0.04s
Test loss: 0.4135 score: 0.8140 time: 0.05s
Epoch 269/1000, LR 0.000227
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 0.14s
Val loss: 0.3675 score: 0.8636 time: 0.04s
Test loss: 0.4127 score: 0.8140 time: 0.05s
Epoch 270/1000, LR 0.000227
Train loss: 0.6397;  Loss pred: 0.6397; Loss self: 0.0000; time: 0.15s
Val loss: 0.3665 score: 0.8636 time: 0.04s
Test loss: 0.4116 score: 0.8140 time: 0.05s
Epoch 271/1000, LR 0.000227
Train loss: 0.6329;  Loss pred: 0.6329; Loss self: 0.0000; time: 0.14s
Val loss: 0.3657 score: 0.8636 time: 0.05s
Test loss: 0.4107 score: 0.8140 time: 0.05s
Epoch 272/1000, LR 0.000226
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.14s
Val loss: 0.3648 score: 0.8636 time: 0.04s
Test loss: 0.4097 score: 0.8140 time: 0.04s
Epoch 273/1000, LR 0.000226
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 0.13s
Val loss: 0.3639 score: 0.8636 time: 0.04s
Test loss: 0.4088 score: 0.8140 time: 0.04s
Epoch 274/1000, LR 0.000226
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 0.13s
Val loss: 0.3631 score: 0.8636 time: 0.04s
Test loss: 0.4079 score: 0.8140 time: 0.04s
Epoch 275/1000, LR 0.000225
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.13s
Val loss: 0.3625 score: 0.8636 time: 0.04s
Test loss: 0.4072 score: 0.8140 time: 0.04s
Epoch 276/1000, LR 0.000225
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 0.13s
Val loss: 0.3619 score: 0.8636 time: 0.04s
Test loss: 0.4066 score: 0.8140 time: 0.05s
Epoch 277/1000, LR 0.000225
Train loss: 0.6292;  Loss pred: 0.6292; Loss self: 0.0000; time: 0.13s
Val loss: 0.3614 score: 0.8636 time: 0.04s
Test loss: 0.4060 score: 0.8140 time: 0.04s
Epoch 278/1000, LR 0.000224
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.13s
Val loss: 0.3608 score: 0.8636 time: 0.04s
Test loss: 0.4053 score: 0.8140 time: 0.04s
Epoch 279/1000, LR 0.000224
Train loss: 0.6288;  Loss pred: 0.6288; Loss self: 0.0000; time: 0.13s
Val loss: 0.3604 score: 0.8636 time: 0.04s
Test loss: 0.4049 score: 0.8140 time: 0.04s
Epoch 280/1000, LR 0.000224
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.13s
Val loss: 0.3597 score: 0.8636 time: 0.04s
Test loss: 0.4041 score: 0.8140 time: 0.05s
Epoch 281/1000, LR 0.000223
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.13s
Val loss: 0.3592 score: 0.8636 time: 0.04s
Test loss: 0.4035 score: 0.8140 time: 0.04s
Epoch 282/1000, LR 0.000223
Train loss: 0.6250;  Loss pred: 0.6250; Loss self: 0.0000; time: 0.13s
Val loss: 0.3587 score: 0.8636 time: 0.04s
Test loss: 0.4030 score: 0.8140 time: 0.04s
Epoch 283/1000, LR 0.000223
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 0.13s
Val loss: 0.3580 score: 0.8636 time: 0.04s
Test loss: 0.4023 score: 0.8140 time: 0.04s
Epoch 284/1000, LR 0.000222
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.13s
Val loss: 0.3575 score: 0.8636 time: 0.04s
Test loss: 0.4017 score: 0.8140 time: 0.04s
Epoch 285/1000, LR 0.000222
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.13s
Val loss: 0.3569 score: 0.8636 time: 0.05s
Test loss: 0.4009 score: 0.8140 time: 0.04s
Epoch 286/1000, LR 0.000222
Train loss: 0.6202;  Loss pred: 0.6202; Loss self: 0.0000; time: 0.13s
Val loss: 0.3566 score: 0.8636 time: 0.04s
Test loss: 0.4006 score: 0.8140 time: 0.04s
Epoch 287/1000, LR 0.000221
Train loss: 0.6212;  Loss pred: 0.6212; Loss self: 0.0000; time: 0.13s
Val loss: 0.3558 score: 0.8636 time: 0.04s
Test loss: 0.3998 score: 0.8140 time: 0.04s
Epoch 288/1000, LR 0.000221
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.13s
Val loss: 0.3550 score: 0.8636 time: 0.04s
Test loss: 0.3988 score: 0.8140 time: 0.04s
Epoch 289/1000, LR 0.000221
Train loss: 0.6194;  Loss pred: 0.6194; Loss self: 0.0000; time: 0.13s
Val loss: 0.3541 score: 0.8409 time: 0.05s
Test loss: 0.3978 score: 0.8140 time: 0.04s
Epoch 290/1000, LR 0.000220
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.13s
Val loss: 0.3535 score: 0.8409 time: 0.04s
Test loss: 0.3972 score: 0.8140 time: 0.04s
Epoch 291/1000, LR 0.000220
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.13s
Val loss: 0.3531 score: 0.8409 time: 0.04s
Test loss: 0.3968 score: 0.8140 time: 0.04s
Epoch 292/1000, LR 0.000220
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.13s
Val loss: 0.3528 score: 0.8409 time: 0.04s
Test loss: 0.3964 score: 0.8140 time: 0.04s
Epoch 293/1000, LR 0.000219
Train loss: 0.6129;  Loss pred: 0.6129; Loss self: 0.0000; time: 0.13s
Val loss: 0.3525 score: 0.8636 time: 0.04s
Test loss: 0.3961 score: 0.8140 time: 0.04s
Epoch 294/1000, LR 0.000219
Train loss: 0.6155;  Loss pred: 0.6155; Loss self: 0.0000; time: 0.13s
Val loss: 0.3523 score: 0.8636 time: 0.04s
Test loss: 0.3959 score: 0.8140 time: 0.04s
Epoch 295/1000, LR 0.000219
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.13s
Val loss: 0.3523 score: 0.8636 time: 0.04s
Test loss: 0.3959 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 296/1000, LR 0.000218
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 0.13s
Val loss: 0.3522 score: 0.8636 time: 0.04s
Test loss: 0.3958 score: 0.8140 time: 0.04s
Epoch 297/1000, LR 0.000218
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.13s
Val loss: 0.3519 score: 0.8636 time: 0.04s
Test loss: 0.3954 score: 0.8140 time: 0.04s
Epoch 298/1000, LR 0.000218
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 0.13s
Val loss: 0.3512 score: 0.8636 time: 0.04s
Test loss: 0.3946 score: 0.8140 time: 0.04s
Epoch 299/1000, LR 0.000217
Train loss: 0.6092;  Loss pred: 0.6092; Loss self: 0.0000; time: 0.13s
Val loss: 0.3505 score: 0.8636 time: 0.04s
Test loss: 0.3939 score: 0.8140 time: 0.04s
Epoch 300/1000, LR 0.000217
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.13s
Val loss: 0.3499 score: 0.8636 time: 0.04s
Test loss: 0.3932 score: 0.8140 time: 0.04s
Epoch 301/1000, LR 0.000217
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 0.13s
Val loss: 0.3489 score: 0.8409 time: 0.04s
Test loss: 0.3921 score: 0.8140 time: 0.04s
Epoch 302/1000, LR 0.000216
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.14s
Val loss: 0.3476 score: 0.8409 time: 0.04s
Test loss: 0.3907 score: 0.8140 time: 0.04s
Epoch 303/1000, LR 0.000216
Train loss: 0.6085;  Loss pred: 0.6085; Loss self: 0.0000; time: 0.13s
Val loss: 0.3466 score: 0.8409 time: 0.04s
Test loss: 0.3897 score: 0.8140 time: 0.04s
Epoch 304/1000, LR 0.000216
Train loss: 0.6076;  Loss pred: 0.6076; Loss self: 0.0000; time: 0.13s
Val loss: 0.3460 score: 0.8409 time: 0.04s
Test loss: 0.3890 score: 0.8140 time: 0.04s
Epoch 305/1000, LR 0.000215
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 0.13s
Val loss: 0.3457 score: 0.8409 time: 0.04s
Test loss: 0.3886 score: 0.8140 time: 0.04s
Epoch 306/1000, LR 0.000215
Train loss: 0.6061;  Loss pred: 0.6061; Loss self: 0.0000; time: 0.13s
Val loss: 0.3453 score: 0.8409 time: 0.04s
Test loss: 0.3882 score: 0.8140 time: 0.04s
Epoch 307/1000, LR 0.000215
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.13s
Val loss: 0.3453 score: 0.8409 time: 0.04s
Test loss: 0.3882 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 308/1000, LR 0.000214
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.13s
Val loss: 0.3451 score: 0.8409 time: 0.04s
Test loss: 0.3880 score: 0.8140 time: 0.04s
Epoch 309/1000, LR 0.000214
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.13s
Val loss: 0.3448 score: 0.8409 time: 0.04s
Test loss: 0.3877 score: 0.8140 time: 0.05s
Epoch 310/1000, LR 0.000214
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.18s
Val loss: 0.3448 score: 0.8409 time: 0.04s
Test loss: 0.3876 score: 0.8140 time: 0.04s
Epoch 311/1000, LR 0.000213
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.13s
Val loss: 0.3445 score: 0.8409 time: 0.04s
Test loss: 0.3873 score: 0.8140 time: 0.04s
Epoch 312/1000, LR 0.000213
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.13s
Val loss: 0.3441 score: 0.8409 time: 0.04s
Test loss: 0.3868 score: 0.8140 time: 0.04s
Epoch 313/1000, LR 0.000213
Train loss: 0.6011;  Loss pred: 0.6011; Loss self: 0.0000; time: 0.13s
Val loss: 0.3436 score: 0.8409 time: 0.04s
Test loss: 0.3863 score: 0.8140 time: 0.04s
Epoch 314/1000, LR 0.000212
Train loss: 0.6023;  Loss pred: 0.6023; Loss self: 0.0000; time: 0.13s
Val loss: 0.3429 score: 0.8409 time: 0.04s
Test loss: 0.3856 score: 0.8140 time: 0.04s
Epoch 315/1000, LR 0.000212
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.13s
Val loss: 0.3420 score: 0.8409 time: 0.04s
Test loss: 0.3846 score: 0.8140 time: 0.04s
Epoch 316/1000, LR 0.000212
Train loss: 0.6009;  Loss pred: 0.6009; Loss self: 0.0000; time: 0.13s
Val loss: 0.3412 score: 0.8409 time: 0.04s
Test loss: 0.3837 score: 0.8140 time: 0.06s
Epoch 317/1000, LR 0.000211
Train loss: 0.6025;  Loss pred: 0.6025; Loss self: 0.0000; time: 0.18s
Val loss: 0.3405 score: 0.8409 time: 0.04s
Test loss: 0.3830 score: 0.8140 time: 0.05s
Epoch 318/1000, LR 0.000211
Train loss: 0.5998;  Loss pred: 0.5998; Loss self: 0.0000; time: 0.14s
Val loss: 0.3403 score: 0.8409 time: 0.04s
Test loss: 0.3828 score: 0.8140 time: 0.05s
Epoch 319/1000, LR 0.000210
Train loss: 0.5973;  Loss pred: 0.5973; Loss self: 0.0000; time: 0.15s
Val loss: 0.3409 score: 0.8409 time: 0.04s
Test loss: 0.3834 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 320/1000, LR 0.000210
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.13s
Val loss: 0.3417 score: 0.8409 time: 0.04s
Test loss: 0.3842 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 317,   Train_Loss: 0.5998,   Val_Loss: 0.3403,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3403,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.3828


[0.04922064999118447, 0.04941669595427811, 0.04924111801665276, 0.049747382989153266, 0.049422334996052086, 0.049253578996285796, 0.049656921066343784, 0.049624510924331844, 0.04941847105510533, 0.04981841496191919, 0.04920222098007798, 0.048916789004579186, 0.049421956995502114, 0.04967916198074818, 0.049651845009066164, 0.0494889150140807, 0.04979349207133055, 0.04955250897910446, 0.04921476298477501, 0.04957458202261478, 0.051353024085983634, 0.04950558696873486, 0.04897504497785121, 0.04956593899987638, 0.04918252502102405, 0.049170505022630095, 0.04965343698859215, 0.049291520030237734, 0.04899183998350054, 0.049021818093024194, 0.054138557985424995, 0.05422956799156964, 0.04967658198438585, 0.0487282678950578, 0.049015368917025626, 0.04886267299298197, 0.04886375507339835, 0.048661505919881165, 0.04884719604160637, 0.04872864997014403, 0.049043782986700535, 0.051025092019699514, 0.0531561030074954, 0.04926647793035954, 0.048869961057789624, 0.048978606006130576, 0.04875357390847057, 0.04872388602234423, 0.04884985601529479, 0.04902935295831412, 0.0490850149653852, 0.04902375303208828, 0.048582189017906785, 0.04863453097641468, 0.04892378195654601, 0.049488153075799346, 0.0483997690025717, 0.0484091219259426, 0.048583543044514954, 0.04856289294548333, 0.048826897982507944, 0.04832498903851956, 0.04875470302067697, 0.05018236709292978, 0.04849700501654297, 0.04877716698683798, 0.0486322189681232, 0.04840682400390506, 0.04883570899255574, 0.04902175010647625, 0.04866177495568991, 0.048763645929284394, 0.04898058401886374, 0.04855458007659763, 0.04852651199325919, 0.048703090054914355, 0.04872171301394701, 0.04852223990019411, 0.057644085958600044, 0.04941553995013237, 0.04876890406012535, 0.048995697987265885, 0.04859578295145184, 0.0488334329565987, 0.04879172402434051, 0.04880735604092479, 0.048909430974163115, 0.048781379009597, 0.04895205399952829, 0.04862294008489698, 0.060744834947399795, 0.04898693994618952, 0.04906703601591289, 0.04886687803082168, 0.05206469795666635, 0.048269725986756384, 0.04908407502807677, 0.04853798798285425, 0.05188106093555689, 0.049352950998581946, 0.048838256974704564, 0.0491840090835467, 0.04917822091374546, 0.04886202106717974, 0.052633660030551255, 0.050360189052298665, 0.05081832897849381, 0.05090985691640526, 0.05051422503311187, 0.05445338599383831, 0.051394575042650104, 0.05101532791741192, 0.050891519989818335, 0.051028690999373794, 0.051450675004161894, 0.05085875291842967, 0.05130895401816815, 0.051750886952504516, 0.0553202040027827, 1.6320026649627835, 1.1890135350404307, 0.21809672401286662, 0.05045364401303232, 0.04817296506371349, 0.04741288803052157, 0.04697693698108196, 0.04705099493730813, 0.05109367205295712, 0.04697785899043083, 0.04745295504108071, 0.04739968595094979, 0.04714998498093337, 0.04735149699263275, 0.050252343993633986, 0.047257171012461185, 0.050606470089405775, 0.046967274975031614, 0.046911550918594, 0.049634119030088186, 0.04671421798411757, 0.049022478982806206, 0.0491070969728753, 0.048724394058808684, 0.04896201903466135, 0.049843472079373896, 0.048960992018692195, 0.048587990924715996, 0.048574209096841514, 0.048867576057091355, 0.04900996305514127, 0.04848380200564861, 0.04859815095551312, 0.04893369204364717, 0.048831686028279364, 0.04920670704450458, 0.04843733995221555, 0.04853672906756401, 0.04871616803575307, 0.049378890078514814, 0.048486235085874796, 0.05235349596478045, 0.05274222698062658, 0.04963420901913196, 0.04763965599704534, 0.04782193596474826, 0.04776431410573423, 0.04774418903980404, 0.04799273191019893, 0.047783607034944, 0.04747210699133575, 0.054642876028083265, 0.04817644995637238, 0.0481116030132398, 0.05200566095300019, 0.048233981942757964, 0.048657121951691806, 0.04847503104247153, 0.05680633708834648, 0.04860574600752443, 0.051963796955533326, 0.05112795205786824, 0.04877728992141783, 0.0490876529365778, 0.04892161290626973, 0.04917198501061648, 0.052201297017745674, 1.2123389809858054, 0.04946600797120482, 0.04962431103922427, 0.04879154602531344, 0.049221316003240645, 0.04876709496602416, 0.04870954796206206, 0.04835848498623818, 0.04920786607544869, 0.048726508975960314, 0.04812199797015637, 0.047821801039390266, 0.04856651392765343, 0.05131046799942851, 0.048049286007881165, 0.05164725298527628, 0.05145609600003809, 0.04833760601468384, 0.048224042053334415, 0.04886943404562771, 0.047672220040112734, 0.04791681398637593, 0.0515491790138185, 0.0463056480512023, 0.052274529938586056, 0.051531131961382926, 0.05140866804867983, 0.05155245109926909, 0.0467433090088889, 0.049791830009780824, 0.050736612058244646, 0.04659940500278026, 0.04689398698974401, 0.047288903035223484, 0.046894517028704286, 0.04982420802116394, 0.05158586299512535, 0.052360652945935726, 0.05559466499835253, 0.04835569206625223, 0.04867534909863025, 0.04864098795223981, 0.048137011006474495, 0.05337206204421818, 0.048029719037003815, 0.04748962598387152, 0.04780523106455803, 0.04785729700233787, 0.05063087004236877, 0.04818917403463274, 0.04815867205616087, 0.04847988695837557, 0.048324229079298675, 0.04832670500036329, 0.048194619943387806, 0.05225338996388018, 0.048430958064273, 0.051732346997596323, 0.04912716511171311, 0.05177642998751253, 0.04930500499904156, 0.04871988599188626, 0.049124004086479545, 0.047767264069989324, 0.0479331340175122, 0.05372519791126251, 0.05322086100932211, 0.05320592294447124, 0.05312361603137106, 0.05193437694106251, 0.047587974928319454, 0.04753601795528084, 0.04571136797312647, 0.04685731395147741, 0.04701671004295349, 0.0504841529764235, 0.05070089700166136, 0.05424257495906204, 0.05044817691668868, 0.05047270900104195, 0.047409993945620954, 0.04717209795489907, 0.04725920397322625, 0.05080418905708939, 0.047456349013373256, 0.047319949022494256, 0.05053976993076503, 0.04730846895836294, 0.05008306202944368, 0.050264724995940924, 0.05049548007082194, 0.05078498797956854, 0.05355830304324627, 0.05065505800303072, 0.04778049804735929, 0.047645945101976395, 0.0473485360853374, 0.04742443689610809, 0.05185207596514374, 0.04764196393080056, 0.04799645801540464, 0.04766341403592378, 0.051274716039188206, 0.047679720097221434, 0.04745074000675231, 0.04733020404819399, 0.0479344540508464, 0.0479714839020744, 0.048080409062094986, 0.047148940968327224, 0.047501410939730704, 0.0476705749752, 0.047117594978772104, 0.050434182048775256, 0.050861166906543076, 0.04763863096013665, 0.04747937305364758, 0.04781302798073739, 0.047992025036364794, 0.04742154397536069, 0.05185983399860561, 0.048035344923846424, 0.04808577103540301, 0.04808476194739342, 0.048557470086961985, 0.049155042972415686, 0.04686610703356564, 0.04647403210401535, 0.04696266900282353, 0.0473943951074034, 0.0472661629319191, 0.05993368092458695, 0.04999486403539777, 0.047554571996442974, 0.04769750696141273, 0.04753842996433377, 0.04761332902126014, 0.04706378700211644, 0.06663567596115172, 0.050083913025446236, 0.05082681297790259, 0.04772041703108698, 0.04722999699879438]
[0.0011186511361632836, 0.0011231067262335935, 0.00111911631856029, 0.0011306223406625743, 0.0011232348862739111, 0.001119399522642859, 0.0011285663878714497, 0.0011278297937348145, 0.001123147069434212, 0.0011585677898120741, 0.0011442376972111159, 0.0011375997442925393, 0.0011493478371047002, 0.0011553293483894925, 0.0011546940699782829, 0.0011509050003274581, 0.0011579881877053617, 0.0011523839297466153, 0.0011445293717389537, 0.0011528972563398785, 0.0011942563740926426, 0.0011512927202031362, 0.0011389545343686329, 0.0011526962558110786, 0.001143779651651722, 0.001143500116805351, 0.001154731092757957, 0.0011463144193078543, 0.0011393451158953614, 0.0011400422812331207, 0.001259036232219186, 0.0012611527439899916, 0.0011552693484740894, 0.0011332155324432047, 0.0011398923003959449, 0.0011363412323949296, 0.0011363663970557756, 0.0011316629283693294, 0.0011359813032931713, 0.0011332244179103263, 0.001140553092713966, 0.0011866300469697561, 0.0012361884420347769, 0.0011457320448920825, 0.0011365107222741774, 0.001139037348979781, 0.0011338040443830365, 0.00113311362842661, 0.0011360431631463905, 0.001140217510658468, 0.0011415119759391906, 0.0011400872798160064, 0.001129818349253646, 0.001131035604102667, 0.0011377623710824654, 0.001150887280832543, 0.001125576023315621, 0.0011257935331614559, 0.0011298498382445337, 0.0011293696033833332, 0.0011355092554071614, 0.0011238369543841759, 0.0011338303028064411, 0.0011670317928588322, 0.0011278373259661157, 0.001134352720624139, 0.0011309818364679813, 0.0011257400931140712, 0.0011357141626175753, 0.0011400407001506104, 0.0011316691850160444, 0.0011340382774252184, 0.0011390833492759009, 0.0011291762808511077, 0.001128523534726958, 0.001132630001277078, 0.001133063093347605, 0.0011284241837254444, 0.001340560138572094, 0.0011491986034914505, 0.0011341605595377988, 0.00113943483691316, 0.001130134487243066, 0.0011356612315488069, 0.0011346912563800118, 0.0011350547916494137, 0.001137428627306119, 0.0011344506746417907, 0.0011384198604541462, 0.0011307660484859764, 0.0014126705801720883, 0.0011392311615392912, 0.0011410938608351834, 0.0011364390239725973, 0.001210806929224799, 0.0011225517671338695, 0.0011414901169320179, 0.0011287904182059128, 0.0012065363008269044, 0.0011477430464786498, 0.0011357734180163853, 0.0011438141647336442, 0.0011436795561336154, 0.0011363260713297613, 0.001224038605361657, 0.0011711671872627596, 0.001181821604151019, 0.001183950160846634, 0.0011747494193746946, 0.0012663578138101934, 0.0011952226754104675, 0.0011864029748235331, 0.0011835237206934497, 0.0011867137441714836, 0.0011965273256781836, 0.0011827616957774342, 0.0011932314887946082, 0.0012035089988954538, 0.0012865163721577373, 0.03795355034797171, 0.027651477559079783, 0.005072016837508526, 0.001173340558442612, 0.001120301513109616, 0.0011026253030353853, 0.0010924869065367898, 0.0010942091845885613, 0.001188224931464119, 0.0010925083486146706, 0.0011035570939786213, 0.0011023182779290649, 0.0010965112786263573, 0.0011011976044798314, 0.001168659162642651, 0.0010990039770339812, 0.0011768946532419947, 0.0010922622087216655, 0.0010909663004324186, 0.0011542818379090276, 0.0010863771624213389, 0.001140057650762935, 0.0011420255109971, 0.001133125443228109, 0.0011386516054572408, 0.0011591505134738115, 0.0011386277213649347, 0.0011299532773189765, 0.0011296327696939887, 0.0011364552571416595, 0.001139766582677704, 0.0011275302792011306, 0.0011301895571049563, 0.0011379928382243528, 0.0011356206053088223, 0.0011443420242908042, 0.0011264497663305943, 0.0011287611411061398, 0.0011329341403663504, 0.0011483462808956934, 0.0011275868624622046, 0.0012175231619716384, 0.0012265634181541066, 0.0011542839306774874, 0.0011078989766754729, 0.00111213804569182, 0.0011107980024589355, 0.0011103299776698615, 0.001116110044423231, 0.0011112466752312559, 0.0011040024881705988, 0.001270764558792634, 0.0011203825571249391, 0.0011188744886799953, 0.001209433975651167, 0.001121720510296697, 0.0011315609756207398, 0.0011273263033132913, 0.001321077606705732, 0.0011303661862214983, 0.0012084603943147284, 0.0011890221408806568, 0.0011343555795678565, 0.0011415733241064603, 0.0011377119280527844, 0.001143534535130616, 0.0012139836515754807, 0.028193929790367566, 0.001150372278400112, 0.0011540537450982387, 0.0011346871168677543, 0.0011446817675172243, 0.0011341184875819572, 0.0011327801851642339, 0.001124615929912516, 0.0011443689784988067, 0.0011331746273479143, 0.0011191162318641016, 0.0011121349078927968, 0.00112945381227101, 0.0011932666976611282, 0.0011174252559972364, 0.0012010989066343321, 0.001196653395349723, 0.001124130372434508, 0.0011214893500775445, 0.0011364984661773887, 0.0011086562800026217, 0.001114344511311068, 0.0011988181166004302, 0.001076875536074472, 0.0012156867427578152, 0.0011983984177065797, 0.0011955504197367402, 0.0011988942116109092, 0.0010870536978811371, 0.001157949535111182, 0.0011799212106568522, 0.001083707093087913, 0.0010905578369707909, 0.001099741931051709, 0.0010905701634582393, 0.0011587025121200917, 0.0011996712324447757, 0.001217689603393854, 0.0012928991860081983, 0.0011245509782849357, 0.001131984862758843, 0.0011311857663311583, 0.001119465372243593, 0.0012412107452143763, 0.0011169702101628794, 0.0011044099066016633, 0.0011117495596408844, 0.0011129603954032063, 0.001177462094008576, 0.0011206784659216916, 0.0011199691175851365, 0.0011274392315901297, 0.0011238192809139226, 0.0011238768604735648, 0.001120805114962507, 0.001215195115439074, 0.0011263013503319303, 0.001203077837153403, 0.001142492211900305, 0.0012041030229654076, 0.0011466280232335245, 0.0011330206044624712, 0.0011424186996855708, 0.0011108666062788215, 0.0011147240469188884, 0.001249423207238663, 0.0012376944420772584, 0.0012373470452202613, 0.0012354329309621176, 0.0012077762079316862, 0.0011066970913562663, 0.001105488789657694, 0.001063055069142476, 0.0010897049756157537, 0.0010934118614640347, 0.0011740500692191513, 0.001179090627945613, 0.001261455231606094, 0.0011732134166671786, 0.0011737839302567894, 0.001102557998735371, 0.0010970255338348621, 0.0010990512551913082, 0.0011814927687695206, 0.00110363602356682, 0.0011004639307556803, 0.0011753434867619774, 0.0011001969525200684, 0.00116472237277776, 0.0011689470929288587, 0.0011743134900191149, 0.0011810462320829894, 0.0012455419312382852, 0.0011780246047216447, 0.001111174373194402, 0.0011080452349296836, 0.0011011287461706373, 0.0011028938813048394, 0.0012058622317475288, 0.0011079526495535014, 0.001116196698032666, 0.0011084514892075298, 0.0011924352567253072, 0.001108830699935382, 0.0011035055815523795, 0.0011007024197254417, 0.001114754745368521, 0.0011156159046994045, 0.0011181490479556973, 0.0010964869992634238, 0.0011046839753425745, 0.0011086180226790698, 0.0010957580227621418, 0.0011728879546226804, 0.0011828178350358856, 0.0011078751386078291, 0.001104171466363897, 0.0011119308832729625, 0.0011160936054968556, 0.0011028266040781557, 0.001206042651130363, 0.0011171010447406145, 0.0011182737450093725, 0.0011182502778463584, 0.0011292434903944647, 0.0011431405342422253, 0.0010899094658968753, 0.0010807914442794268, 0.0010921550930889194, 0.001102195235055893, 0.001099213091439979, 0.0013938065331299291, 0.0011626712566371574, 0.001105920278987046, 0.001109244347939831, 0.001105544882891483, 0.0011072867214246545, 0.0010945066744678241, 0.0015496668828174818, 0.0011647421633824706, 0.001182018906462851, 0.0011097771402578368, 0.0010983720232277763]
[893.9337454479064, 890.3873306444889, 893.5621645536098, 884.4686364626175, 890.285738290487, 893.3360965163167, 886.0799069925038, 886.6586124564899, 890.3553481235119, 863.1346467539938, 873.944288356632, 879.043798152301, 870.058626045776, 865.5540529581294, 866.0302551123413, 868.8814452239565, 863.5666672745369, 867.7663530242727, 873.7215703609587, 867.3799807406222, 837.3411452459424, 868.5888327545041, 877.9981727315737, 867.5312294619748, 874.2942738628973, 874.5079998712603, 866.002488606765, 872.3610059828096, 877.6971841531473, 877.1604496267938, 794.2583179178197, 792.9253651197193, 865.5990062584337, 882.4446642061172, 877.2758616341624, 880.0173499754299, 879.9978621252011, 883.6553490719633, 880.2961783799028, 882.4377450708369, 876.7676019539631, 842.7226350400068, 808.9381569965105, 872.8044261816829, 879.8861114121149, 877.9343371801508, 881.986622780265, 882.5240248752056, 880.2482444684542, 877.0256469947622, 876.0311070562705, 877.1258286132141, 885.0980342641774, 884.1454648930994, 878.9181514665506, 868.894822850599, 888.4339922720543, 888.2623416673907, 885.0733665225074, 885.4497208037374, 880.6621304389354, 889.8087895213997, 881.9661968151793, 856.8746850934876, 886.6526909307519, 881.5600137581404, 884.1874977612079, 888.3045084001197, 880.5032400892285, 877.1616661298938, 883.650463616558, 881.8044504374701, 877.898883023517, 885.601315718621, 886.1135538852066, 882.9008580670358, 882.5633858089284, 886.191570884756, 745.9568364200029, 870.1716108615508, 881.7093766754921, 877.6280727988775, 884.850441507607, 880.5442787161159, 881.2969998467113, 881.0147381051466, 879.1760432198683, 881.4838955565482, 878.4105361628821, 884.3562303085915, 707.8791149442515, 877.7849779397127, 876.3520989133052, 879.9416237083678, 825.8955047773266, 890.827513953524, 876.0478826463249, 885.9040472627232, 828.8188256869239, 871.2751543719344, 880.4573026074938, 874.2678931877596, 874.3707926201407, 880.0290913239114, 816.9676966230471, 853.8490583374266, 846.1513958516324, 844.6301483543086, 851.2453664648656, 789.6662294768167, 836.664180301446, 842.8839283286028, 844.9344804125096, 842.6631990330241, 835.7519118363692, 845.4788513781687, 838.0603507289194, 830.9036333901711, 777.2928674998564, 26.347996190913438, 36.16443272745238, 197.1602287683295, 852.267479211075, 892.6168431427932, 906.9264030556245, 915.3427780384338, 913.9020345328346, 841.5914979732078, 915.3248130945877, 906.1606376836653, 907.1790063017996, 911.9833233751496, 908.1022297286656, 855.6814783693927, 909.9148145931415, 849.6937234316575, 915.531080371585, 916.618597296394, 866.33954304565, 920.490631238216, 877.1486243092993, 875.6371818059495, 882.5148230289013, 878.2317569371332, 862.7007350435797, 878.2501789094383, 884.9923444380684, 885.2434409023869, 879.9290545895637, 877.3726262886703, 886.8941423981205, 884.8073260927627, 878.740152319704, 880.5757797324032, 873.8646128282679, 887.744868781407, 885.9270252872462, 882.6638410566704, 870.8174673757942, 886.8496373010189, 821.3396108051163, 815.2860139143323, 866.3379723332603, 902.6093723822629, 899.1689510792132, 900.2536894974012, 900.6331632138764, 895.9689996489255, 899.8902064583413, 905.7950599885536, 786.927832603476, 892.5522747927656, 893.7552961635233, 826.8330641708601, 891.4876663309815, 883.7349657197489, 887.0546150310958, 756.9578009074136, 884.669067590144, 827.4991921163139, 841.0272320575491, 881.5577919411825, 875.9840291316603, 878.9571202892449, 874.4816787590755, 823.7343218767587, 35.468627730698586, 869.2838125330669, 866.5107706182913, 881.3002149530425, 873.6052485303106, 881.7420851079592, 882.7838031568472, 889.1924553103122, 873.8440300188919, 882.476518504834, 893.5622337764765, 899.1714880119508, 885.3837041722713, 838.0356226818849, 894.9144424944635, 832.5709019269339, 835.6638638105808, 889.5765335779682, 891.6714188421457, 879.8956001792936, 901.9928160219642, 897.3885453282868, 834.1548948524132, 928.6124222352511, 822.5803283265847, 834.4470296562455, 836.4348198883989, 834.1019502098835, 919.9177574660566, 863.5954933079046, 847.514216176611, 922.7585630639381, 916.9619126094856, 909.3042392624574, 916.951548379943, 863.0342901132476, 833.5617067037014, 821.2273449759891, 773.4555105471766, 889.2438131396323, 883.4040391342575, 884.0280966789028, 893.2835483743775, 805.6649556536706, 895.2790243655446, 905.4609108651163, 899.4831536726837, 898.5045686533322, 849.2842403066918, 892.316601423727, 892.8817628080564, 886.9657645225006, 889.8227828826454, 889.7771946106558, 892.2157711900269, 822.9131168278933, 887.8618494999512, 831.2014145037328, 875.2794895089062, 830.4937209917865, 872.1224143641376, 882.5964824129753, 875.3358118833586, 900.1980925052719, 897.0830070132719, 800.3693177831148, 807.9538584027824, 808.180699071366, 809.4328513820903, 827.9679575014128, 903.5896161744599, 904.5772416286937, 940.6850397756533, 917.6795760108697, 914.5684579103066, 851.7524305118349, 848.1112276690288, 792.7352274934028, 852.3598399008794, 851.9455533704822, 906.9817652649524, 911.5558108335994, 909.875672564455, 846.3868983654141, 906.0958310948562, 908.7076568818638, 850.8151117210497, 908.9281675516723, 858.5737025168395, 855.4707103932712, 851.5613662785416, 846.7069051448719, 802.8633761095671, 848.8787042239156, 899.9487606298927, 902.4902309727995, 908.159017260852, 906.705547062148, 829.2821299750018, 902.5656470093685, 895.8994429588739, 902.1594627609139, 838.6199538801148, 901.8509318494479, 906.202937907418, 908.5107673783793, 897.058302873082, 896.3658511747764, 894.335153107085, 912.0035172982078, 905.2362687617417, 902.0239429117479, 912.6102471777856, 852.5963593186541, 845.4387230047652, 902.6287937615547, 905.6564405644895, 899.3364740949594, 895.9821963632039, 906.7608600500641, 829.1580725298151, 895.1741695239352, 894.2354271150476, 894.2541931900101, 885.5486071039314, 874.7830822593379, 917.5073997335276, 925.247886900797, 915.6208731964258, 907.2802786606943, 909.7417123098406, 717.4596877189275, 860.0883476661681, 904.2243089311444, 901.5146228667044, 904.5313451088145, 903.1084547942402, 913.6536334839843, 645.2999745221883, 858.5591141441545, 846.0101564639639, 901.0818151900912, 910.4383386070858]
Elapsed: 0.06169266823614101~0.12494523714782461
Time per graph: 0.0014339983159333298~0.002905776391244697
Speed: 863.3895534202993~94.78194093151511
Total Time: 0.0483
best val loss: 0.340329110622406 test_score: 0.8140

Testing...
Test loss: 0.5603 score: 0.8140 time: 0.04s
test Score 0.8140
Epoch Time List: [0.23813166713807732, 0.21787546400446445, 0.21795870806090534, 0.21790761197917163, 0.21779315907042474, 0.2183288859669119, 0.21822236198931932, 0.21912875003181398, 0.2178315658820793, 0.2293252928648144, 0.22305703908205032, 0.22217801690567285, 0.22349613602273166, 0.2270448449999094, 0.22481186187360436, 0.2235944310668856, 0.22430307499598712, 0.22434541897382587, 0.22276867413893342, 0.22341025713831186, 0.22989780188072473, 0.22800555697176605, 0.22295724204741418, 0.22522015182767063, 0.23285500216297805, 0.22203104989603162, 0.22194739500992, 0.22235608496703207, 0.22171535005327314, 0.22188551304861903, 0.22601664392277598, 0.22882700199261308, 0.22964408807456493, 0.22081053198780864, 0.22028645710088313, 0.22075844008941203, 0.22037347592413425, 0.2217377859633416, 0.22000217298045754, 0.21992720500566065, 0.2207843429641798, 0.22330255305860192, 0.2273284998955205, 0.22129533987026662, 0.21990900009404868, 0.2212669770233333, 0.22002193506341428, 0.2223352069268003, 0.2196739410283044, 0.22038937313482165, 0.22090945404488593, 0.22018742794170976, 0.23567817907314748, 0.21976327488664538, 0.21919648395851254, 0.2207454979652539, 0.2194248678861186, 0.2193827579030767, 0.21937898290343583, 0.21941289398819208, 0.219468007911928, 0.21977219393011183, 0.21952972398139536, 0.22578598489053547, 0.21937466086819768, 0.21966476913075894, 0.21939015306998044, 0.21927297196816653, 0.21942798304371536, 0.22052942495793104, 0.21866370178759098, 0.21870325005147606, 0.21974813705310225, 0.2193316709017381, 0.21977342886384577, 0.2198807019740343, 0.21964150213170797, 0.21947308001108468, 0.23327766510192305, 0.22237078903708607, 0.2198719420703128, 0.22098350303713232, 0.22132594289723784, 0.22071762499399483, 0.22128706087823957, 0.22076440684031695, 0.22102576296310872, 0.22103593405336142, 0.2207026258111, 0.22062094695866108, 0.23867329803761095, 0.2267324860440567, 0.2212409790372476, 0.2212268158327788, 0.2269628179492429, 0.22847850993275642, 0.2283724050503224, 0.2202083331067115, 0.22706031589768827, 0.2287506009452045, 0.21961057803127915, 0.22018752014264464, 0.22073196107521653, 0.22045714512933046, 0.22599502187222242, 0.22598708292935044, 0.23347120499238372, 0.22804869711399078, 0.23380189400631934, 0.23066436813678592, 0.23250174487475306, 0.23128900292795151, 0.23132441088091582, 0.2307833330705762, 0.23629935504868627, 0.23236647713929415, 0.23396026494447142, 0.23540832905564457, 0.23542386991903186, 3.4044338408857584, 8.31708962121047, 4.751387019990943, 0.40355133498087525, 0.21851674292702228, 0.2129443408921361, 0.2111917679430917, 0.21072265703696758, 0.21535606298130006, 0.210354253067635, 0.20867011707741767, 0.2122363978996873, 0.21043146192096174, 0.21448558394331485, 0.2113388340221718, 0.21237197588197887, 0.21459599409718066, 0.21143290190957487, 0.20999059290625155, 0.21388627297710627, 0.21065894491039217, 0.22202645288780332, 0.22588130400981754, 0.22123831510543823, 0.22057268302887678, 0.22591909405309707, 0.22108357807155699, 0.22048462100792676, 0.21991980099119246, 0.22037834592629224, 0.2277351119555533, 0.22056486003566533, 0.22051419597119093, 0.22080940392334014, 0.2243153640301898, 0.22081458498723805, 0.21976078604348004, 0.2196500200079754, 0.21980625588912517, 0.22127176995854825, 0.22012361395172775, 0.21999057196080685, 0.235896008904092, 0.230898872949183, 0.21619865915272385, 0.21629684208892286, 0.2163176869507879, 0.21661262400448322, 0.2160312820924446, 0.21648012183140963, 0.215624077944085, 0.22443556098733097, 0.21887521480675787, 0.21709406294394284, 0.2288813820341602, 0.2171285559888929, 0.21736095112282783, 0.22113178484141827, 0.23633675405289978, 0.23611182300373912, 0.2268109960714355, 0.22675222402904183, 0.2323338621063158, 0.2202021380653605, 0.22613018390256912, 0.22236141096800566, 0.23313715495169163, 2.217121317051351, 3.132935312925838, 0.228103480883874, 0.22377683594822884, 0.22629045299254358, 0.22052037599496543, 0.21857453405391425, 0.21798051695805043, 0.22331432602368295, 0.22268705302849412, 0.22306406497955322, 0.2185059170005843, 0.2177193600218743, 0.2172505451599136, 0.2180559280095622, 0.22187124204356223, 0.21801849792245775, 0.21921814803499728, 0.2173702041618526, 0.22194710292387754, 0.2168629610678181, 0.22111894586123526, 0.21732949011493474, 0.21318403189070523, 0.22224833478685468, 0.24037967494223267, 0.23923362896312028, 0.22842865821439773, 0.20995938195846975, 0.2134257589932531, 0.2103546588914469, 0.21010717493481934, 0.20982031500898302, 0.21024163509719074, 0.2089225830277428, 0.21087995497509837, 0.22045292903203517, 0.24328081298153847, 0.24090550490655005, 0.22037320991512388, 0.21730068395845592, 0.21750130399595946, 0.22043247905094177, 0.22018267202656716, 0.2241115040378645, 0.21539489598944783, 0.21600535907782614, 0.21635254099965096, 0.22306914709042758, 0.23036545608192682, 0.2187027158215642, 0.21892246592324227, 0.22034479898866266, 0.21853994892444462, 0.21854416304267943, 0.2224766359431669, 0.22072938701603562, 0.2219292160589248, 0.22218219202477485, 0.22019299410749227, 0.2217483768472448, 0.22407105891034007, 0.22275189694482833, 0.22042198304552585, 0.21847894205711782, 0.2270788410678506, 0.24142572993878275, 0.24728125403635204, 0.24214859085623175, 0.24118583789095283, 0.21248258301056921, 0.21378145506605506, 0.2101878869580105, 0.2118182589765638, 0.2141191988484934, 0.21138517395593226, 0.2206074489513412, 0.23330433294177055, 0.2309353039599955, 0.23037926596589386, 0.22287207806948572, 0.2136638859519735, 0.2149029620923102, 0.21318442199844867, 0.21497709990944713, 0.21841056598350406, 0.21390215889550745, 0.21478096686769277, 0.21945742692332715, 0.23069599992595613, 0.23251527396496385, 0.23222634301055223, 0.24088248808402568, 0.23418286792002618, 0.22355355904437602, 0.21951329102739692, 0.2138344410341233, 0.21333783504087478, 0.21996184904128313, 0.21520148299168795, 0.21476116590201855, 0.2189927069703117, 0.21935578098054975, 0.21648752293549478, 0.21447979903314263, 0.21468854695558548, 0.2165042501874268, 0.22051772009581327, 0.21892137394752353, 0.21791634208057076, 0.21418235590681434, 0.2185620239470154, 0.21395884605590254, 0.21448842098470777, 0.21502134599722922, 0.21514609304722399, 0.2183258100412786, 0.2178922750754282, 0.21400811290368438, 0.21746469591744244, 0.21605725912377238, 0.21692015614826232, 0.2189915479393676, 0.21924916794523597, 0.22342689288780093, 0.21341609803494066, 0.2128630301449448, 0.2090352000668645, 0.20909889496397227, 0.218366821995005, 0.2144517379347235, 0.22375106089748442, 0.2602780129527673, 0.21465725800953805, 0.21800793090369552, 0.21742138906847686, 0.21361728094052523, 0.21367876313161105, 0.23123857111204416, 0.27355366095434874, 0.22875113296322525, 0.23033525294158608, 0.21467701299116015]
Total Epoch List: [9, 320]
Total Time List: [0.04973241395782679, 0.04826219903770834]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2acb7c40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.0928;  Loss pred: 2.0928; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4884 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.0576;  Loss pred: 2.0576; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.1023;  Loss pred: 2.1023; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4884 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.0539;  Loss pred: 2.0539; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4884 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.0751;  Loss pred: 2.0751; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4884 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.0462;  Loss pred: 2.0462; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.4884 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 2.0060;  Loss pred: 2.0060; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4884 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.0476;  Loss pred: 2.0476; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4884 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 2.0023;  Loss pred: 2.0023; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 1.9771;  Loss pred: 1.9771; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 1.9568;  Loss pred: 1.9568; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 1.9099;  Loss pred: 1.9099; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 1.8915;  Loss pred: 1.8915; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 1.8511;  Loss pred: 1.8511; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 1.8242;  Loss pred: 1.8242; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 1.8017;  Loss pred: 1.8017; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 1.8064;  Loss pred: 1.8064; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 1.7769;  Loss pred: 1.7769; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 1.7626;  Loss pred: 1.7626; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 1.7224;  Loss pred: 1.7224; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.50s
Epoch 21/1000, LR 0.000270
Train loss: 1.7056;  Loss pred: 1.7056; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.31s
Epoch 22/1000, LR 0.000270
Train loss: 1.6701;  Loss pred: 1.6701; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.53s
Epoch 23/1000, LR 0.000270
Train loss: 1.6465;  Loss pred: 1.6465; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.38s
Epoch 24/1000, LR 0.000270
Train loss: 1.6389;  Loss pred: 1.6389; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.18s
Epoch 25/1000, LR 0.000270
Train loss: 1.6083;  Loss pred: 1.6083; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.54s
Epoch 26/1000, LR 0.000270
Train loss: 1.5889;  Loss pred: 1.5889; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 1.5813;  Loss pred: 1.5813; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 1.5455;  Loss pred: 1.5455; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 1.5361;  Loss pred: 1.5361; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 1.5076;  Loss pred: 1.5076; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 1.4928;  Loss pred: 1.4928; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 1.4887;  Loss pred: 1.4887; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 1.4592;  Loss pred: 1.4592; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 1.4599;  Loss pred: 1.4599; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 1.4389;  Loss pred: 1.4389; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 1.4201;  Loss pred: 1.4201; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 1.4174;  Loss pred: 1.4174; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 1.3955;  Loss pred: 1.3955; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 1.3925;  Loss pred: 1.3925; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 1.3775;  Loss pred: 1.3775; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 1.3618;  Loss pred: 1.3618; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 1.3447;  Loss pred: 1.3447; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 1.3420;  Loss pred: 1.3420; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 1.3232;  Loss pred: 1.3232; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 1.3269;  Loss pred: 1.3269; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 1.2997;  Loss pred: 1.2997; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 1.3008;  Loss pred: 1.3008; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 1.2736;  Loss pred: 1.2736; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 1.2621;  Loss pred: 1.2621; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 1.2649;  Loss pred: 1.2649; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 1.2602;  Loss pred: 1.2602; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 1.2416;  Loss pred: 1.2416; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 1.2257;  Loss pred: 1.2257; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 1.2294;  Loss pred: 1.2294; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 1.2259;  Loss pred: 1.2259; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.04s
Test loss: 0.6919 score: 0.5116 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 1.2098;  Loss pred: 1.2098; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.04s
Test loss: 0.6918 score: 0.5116 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 1.2023;  Loss pred: 1.2023; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.04s
Test loss: 0.6916 score: 0.5116 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 1.2021;  Loss pred: 1.2021; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.04s
Test loss: 0.6915 score: 0.5116 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 1.1912;  Loss pred: 1.1912; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.04s
Test loss: 0.6914 score: 0.5116 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 1.1772;  Loss pred: 1.1772; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.04s
Test loss: 0.6912 score: 0.5116 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 1.1721;  Loss pred: 1.1721; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.04s
Test loss: 0.6911 score: 0.5116 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 1.1720;  Loss pred: 1.1720; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.04s
Test loss: 0.6910 score: 0.5116 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 1.1664;  Loss pred: 1.1664; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.04s
Test loss: 0.6908 score: 0.5116 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 1.1571;  Loss pred: 1.1571; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.04s
Test loss: 0.6906 score: 0.5116 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 1.1529;  Loss pred: 1.1529; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.04s
Test loss: 0.6905 score: 0.5116 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 1.1469;  Loss pred: 1.1469; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.04s
Test loss: 0.6903 score: 0.5116 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 1.1415;  Loss pred: 1.1415; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.04s
Test loss: 0.6901 score: 0.5116 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 1.1397;  Loss pred: 1.1397; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.04s
Test loss: 0.6899 score: 0.5116 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.1385;  Loss pred: 1.1385; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.04s
Test loss: 0.6897 score: 0.5116 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 1.1291;  Loss pred: 1.1291; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.04s
Test loss: 0.6895 score: 0.5116 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 1.1210;  Loss pred: 1.1210; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.04s
Test loss: 0.6893 score: 0.5116 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 1.1188;  Loss pred: 1.1188; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.04s
Test loss: 0.6891 score: 0.5116 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 1.1119;  Loss pred: 1.1119; Loss self: 0.0000; time: 0.13s
Val loss: 0.6890 score: 0.5227 time: 0.04s
Test loss: 0.6889 score: 0.5116 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 1.1057;  Loss pred: 1.1057; Loss self: 0.0000; time: 0.13s
Val loss: 0.6888 score: 0.5227 time: 0.04s
Test loss: 0.6886 score: 0.5349 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 1.1086;  Loss pred: 1.1086; Loss self: 0.0000; time: 0.23s
Val loss: 0.6886 score: 0.5227 time: 0.04s
Test loss: 0.6884 score: 0.5349 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 1.0970;  Loss pred: 1.0970; Loss self: 0.0000; time: 0.14s
Val loss: 0.6884 score: 0.5227 time: 0.04s
Test loss: 0.6882 score: 0.5349 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 1.0956;  Loss pred: 1.0956; Loss self: 0.0000; time: 0.13s
Val loss: 0.6882 score: 0.5227 time: 0.04s
Test loss: 0.6879 score: 0.5349 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 1.0926;  Loss pred: 1.0926; Loss self: 0.0000; time: 0.13s
Val loss: 0.6880 score: 0.5227 time: 0.04s
Test loss: 0.6876 score: 0.5349 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 1.0926;  Loss pred: 1.0926; Loss self: 0.0000; time: 0.13s
Val loss: 0.6878 score: 0.5227 time: 0.04s
Test loss: 0.6874 score: 0.5349 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 1.0870;  Loss pred: 1.0870; Loss self: 0.0000; time: 0.13s
Val loss: 0.6875 score: 0.5227 time: 0.04s
Test loss: 0.6871 score: 0.5349 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 1.0787;  Loss pred: 1.0787; Loss self: 0.0000; time: 0.15s
Val loss: 0.6873 score: 0.5227 time: 0.04s
Test loss: 0.6868 score: 0.5349 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 1.0839;  Loss pred: 1.0839; Loss self: 0.0000; time: 0.16s
Val loss: 0.6870 score: 0.5455 time: 0.04s
Test loss: 0.6865 score: 0.5349 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 1.0772;  Loss pred: 1.0772; Loss self: 0.0000; time: 0.15s
Val loss: 0.6868 score: 0.5455 time: 0.04s
Test loss: 0.6862 score: 0.5349 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.0742;  Loss pred: 1.0742; Loss self: 0.0000; time: 0.14s
Val loss: 0.6865 score: 0.5455 time: 0.04s
Test loss: 0.6858 score: 0.5349 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 1.0726;  Loss pred: 1.0726; Loss self: 0.0000; time: 0.14s
Val loss: 0.6862 score: 0.5455 time: 0.04s
Test loss: 0.6855 score: 0.5349 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 1.0680;  Loss pred: 1.0680; Loss self: 0.0000; time: 0.14s
Val loss: 0.6859 score: 0.5455 time: 0.04s
Test loss: 0.6852 score: 0.5349 time: 0.04s
Epoch 87/1000, LR 0.000266
Train loss: 1.0660;  Loss pred: 1.0660; Loss self: 0.0000; time: 0.14s
Val loss: 0.6856 score: 0.5455 time: 0.04s
Test loss: 0.6848 score: 0.5349 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 1.0618;  Loss pred: 1.0618; Loss self: 0.0000; time: 0.15s
Val loss: 0.6853 score: 0.5455 time: 0.05s
Test loss: 0.6844 score: 0.5349 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 1.0640;  Loss pred: 1.0640; Loss self: 0.0000; time: 0.14s
Val loss: 0.6849 score: 0.5455 time: 0.04s
Test loss: 0.6840 score: 0.5349 time: 0.04s
Epoch 90/1000, LR 0.000266
Train loss: 1.0557;  Loss pred: 1.0557; Loss self: 0.0000; time: 0.14s
Val loss: 0.6846 score: 0.5455 time: 0.04s
Test loss: 0.6836 score: 0.5349 time: 0.04s
Epoch 91/1000, LR 0.000266
Train loss: 1.0499;  Loss pred: 1.0499; Loss self: 0.0000; time: 0.13s
Val loss: 0.6843 score: 0.5455 time: 0.04s
Test loss: 0.6832 score: 0.5349 time: 0.04s
Epoch 92/1000, LR 0.000266
Train loss: 1.0503;  Loss pred: 1.0503; Loss self: 0.0000; time: 0.13s
Val loss: 0.6839 score: 0.5455 time: 0.04s
Test loss: 0.6828 score: 0.5349 time: 0.04s
Epoch 93/1000, LR 0.000265
Train loss: 1.0446;  Loss pred: 1.0446; Loss self: 0.0000; time: 0.14s
Val loss: 0.6835 score: 0.5455 time: 0.05s
Test loss: 0.6824 score: 0.5349 time: 0.04s
Epoch 94/1000, LR 0.000265
Train loss: 1.0498;  Loss pred: 1.0498; Loss self: 0.0000; time: 0.13s
Val loss: 0.6831 score: 0.5455 time: 0.04s
Test loss: 0.6819 score: 0.5349 time: 0.04s
Epoch 95/1000, LR 0.000265
Train loss: 1.0441;  Loss pred: 1.0441; Loss self: 0.0000; time: 0.13s
Val loss: 0.6827 score: 0.5455 time: 0.04s
Test loss: 0.6815 score: 0.5349 time: 0.04s
Epoch 96/1000, LR 0.000265
Train loss: 1.0395;  Loss pred: 1.0395; Loss self: 0.0000; time: 0.13s
Val loss: 0.6823 score: 0.5455 time: 0.04s
Test loss: 0.6810 score: 0.5349 time: 0.04s
Epoch 97/1000, LR 0.000265
Train loss: 1.0448;  Loss pred: 1.0448; Loss self: 0.0000; time: 0.13s
Val loss: 0.6819 score: 0.5455 time: 0.04s
Test loss: 0.6805 score: 0.5349 time: 0.04s
Epoch 98/1000, LR 0.000265
Train loss: 1.0386;  Loss pred: 1.0386; Loss self: 0.0000; time: 0.13s
Val loss: 0.6815 score: 0.5455 time: 0.04s
Test loss: 0.6800 score: 0.5349 time: 0.04s
Epoch 99/1000, LR 0.000265
Train loss: 1.0358;  Loss pred: 1.0358; Loss self: 0.0000; time: 3.45s
Val loss: 0.6810 score: 0.5682 time: 0.88s
Test loss: 0.6795 score: 0.5581 time: 0.65s
Epoch 100/1000, LR 0.000265
Train loss: 1.0385;  Loss pred: 1.0385; Loss self: 0.0000; time: 0.26s
Val loss: 0.6805 score: 0.5682 time: 0.06s
Test loss: 0.6789 score: 0.5581 time: 0.04s
Epoch 101/1000, LR 0.000265
Train loss: 1.0252;  Loss pred: 1.0252; Loss self: 0.0000; time: 0.15s
Val loss: 0.6800 score: 0.5682 time: 0.04s
Test loss: 0.6783 score: 0.5814 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 1.0314;  Loss pred: 1.0314; Loss self: 0.0000; time: 0.15s
Val loss: 0.6795 score: 0.5682 time: 0.05s
Test loss: 0.6777 score: 0.5581 time: 0.04s
Epoch 103/1000, LR 0.000264
Train loss: 1.0240;  Loss pred: 1.0240; Loss self: 0.0000; time: 0.15s
Val loss: 0.6790 score: 0.5455 time: 0.05s
Test loss: 0.6771 score: 0.5581 time: 0.04s
Epoch 104/1000, LR 0.000264
Train loss: 1.0214;  Loss pred: 1.0214; Loss self: 0.0000; time: 0.14s
Val loss: 0.6785 score: 0.5455 time: 0.04s
Test loss: 0.6765 score: 0.5814 time: 0.04s
Epoch 105/1000, LR 0.000264
Train loss: 1.0212;  Loss pred: 1.0212; Loss self: 0.0000; time: 0.13s
Val loss: 0.6779 score: 0.5455 time: 0.04s
Test loss: 0.6759 score: 0.5814 time: 0.04s
Epoch 106/1000, LR 0.000264
Train loss: 1.0230;  Loss pred: 1.0230; Loss self: 0.0000; time: 0.14s
Val loss: 0.6774 score: 0.5455 time: 0.04s
Test loss: 0.6753 score: 0.5814 time: 0.04s
Epoch 107/1000, LR 0.000264
Train loss: 1.0161;  Loss pred: 1.0161; Loss self: 0.0000; time: 0.13s
Val loss: 0.6768 score: 0.5455 time: 0.04s
Test loss: 0.6746 score: 0.5814 time: 0.04s
Epoch 108/1000, LR 0.000264
Train loss: 1.0169;  Loss pred: 1.0169; Loss self: 0.0000; time: 0.13s
Val loss: 0.6762 score: 0.5682 time: 0.04s
Test loss: 0.6739 score: 0.5814 time: 0.04s
Epoch 109/1000, LR 0.000264
Train loss: 1.0181;  Loss pred: 1.0181; Loss self: 0.0000; time: 0.13s
Val loss: 0.6756 score: 0.5909 time: 0.04s
Test loss: 0.6732 score: 0.5814 time: 0.04s
Epoch 110/1000, LR 0.000263
Train loss: 1.0105;  Loss pred: 1.0105; Loss self: 0.0000; time: 0.13s
Val loss: 0.6750 score: 0.6364 time: 0.04s
Test loss: 0.6725 score: 0.5581 time: 0.04s
Epoch 111/1000, LR 0.000263
Train loss: 1.0100;  Loss pred: 1.0100; Loss self: 0.0000; time: 0.13s
Val loss: 0.6743 score: 0.6364 time: 0.04s
Test loss: 0.6718 score: 0.5814 time: 0.04s
Epoch 112/1000, LR 0.000263
Train loss: 1.0085;  Loss pred: 1.0085; Loss self: 0.0000; time: 0.13s
Val loss: 0.6737 score: 0.6591 time: 0.04s
Test loss: 0.6711 score: 0.5814 time: 0.04s
Epoch 113/1000, LR 0.000263
Train loss: 1.0086;  Loss pred: 1.0086; Loss self: 0.0000; time: 0.13s
Val loss: 0.6730 score: 0.6591 time: 0.04s
Test loss: 0.6704 score: 0.5814 time: 0.04s
Epoch 114/1000, LR 0.000263
Train loss: 1.0081;  Loss pred: 1.0081; Loss self: 0.0000; time: 0.15s
Val loss: 0.6723 score: 0.6591 time: 0.05s
Test loss: 0.6696 score: 0.5814 time: 0.05s
Epoch 115/1000, LR 0.000263
Train loss: 1.0044;  Loss pred: 1.0044; Loss self: 0.0000; time: 0.13s
Val loss: 0.6716 score: 0.6591 time: 0.04s
Test loss: 0.6688 score: 0.5814 time: 0.04s
Epoch 116/1000, LR 0.000263
Train loss: 1.0032;  Loss pred: 1.0032; Loss self: 0.0000; time: 0.13s
Val loss: 0.6709 score: 0.6591 time: 0.04s
Test loss: 0.6680 score: 0.5814 time: 0.04s
Epoch 117/1000, LR 0.000262
Train loss: 0.9996;  Loss pred: 0.9996; Loss self: 0.0000; time: 0.13s
Val loss: 0.6702 score: 0.6591 time: 0.04s
Test loss: 0.6672 score: 0.5814 time: 0.04s
Epoch 118/1000, LR 0.000262
Train loss: 0.9949;  Loss pred: 0.9949; Loss self: 0.0000; time: 0.13s
Val loss: 0.6694 score: 0.6818 time: 0.04s
Test loss: 0.6663 score: 0.5814 time: 0.04s
Epoch 119/1000, LR 0.000262
Train loss: 0.9977;  Loss pred: 0.9977; Loss self: 0.0000; time: 0.13s
Val loss: 0.6687 score: 0.6818 time: 0.04s
Test loss: 0.6655 score: 0.5814 time: 0.04s
Epoch 120/1000, LR 0.000262
Train loss: 0.9960;  Loss pred: 0.9960; Loss self: 0.0000; time: 0.14s
Val loss: 0.6679 score: 0.7045 time: 0.04s
Test loss: 0.6646 score: 0.5814 time: 0.04s
Epoch 121/1000, LR 0.000262
Train loss: 0.9925;  Loss pred: 0.9925; Loss self: 0.0000; time: 0.13s
Val loss: 0.6670 score: 0.7045 time: 0.04s
Test loss: 0.6637 score: 0.5814 time: 0.04s
Epoch 122/1000, LR 0.000262
Train loss: 0.9880;  Loss pred: 0.9880; Loss self: 0.0000; time: 0.13s
Val loss: 0.6662 score: 0.7045 time: 0.04s
Test loss: 0.6628 score: 0.5814 time: 0.04s
Epoch 123/1000, LR 0.000262
Train loss: 0.9906;  Loss pred: 0.9906; Loss self: 0.0000; time: 0.13s
Val loss: 0.6654 score: 0.7045 time: 0.04s
Test loss: 0.6618 score: 0.5814 time: 0.04s
Epoch 124/1000, LR 0.000261
Train loss: 0.9864;  Loss pred: 0.9864; Loss self: 0.0000; time: 0.13s
Val loss: 0.6645 score: 0.7045 time: 0.04s
Test loss: 0.6608 score: 0.6047 time: 0.04s
Epoch 125/1000, LR 0.000261
Train loss: 0.9850;  Loss pred: 0.9850; Loss self: 0.0000; time: 0.13s
Val loss: 0.6636 score: 0.7273 time: 0.04s
Test loss: 0.6598 score: 0.6279 time: 0.04s
Epoch 126/1000, LR 0.000261
Train loss: 0.9833;  Loss pred: 0.9833; Loss self: 0.0000; time: 0.13s
Val loss: 0.6627 score: 0.7273 time: 0.04s
Test loss: 0.6588 score: 0.6279 time: 0.04s
Epoch 127/1000, LR 0.000261
Train loss: 0.9807;  Loss pred: 0.9807; Loss self: 0.0000; time: 0.13s
Val loss: 0.6617 score: 0.7273 time: 0.04s
Test loss: 0.6577 score: 0.6744 time: 0.04s
Epoch 128/1000, LR 0.000261
Train loss: 0.9791;  Loss pred: 0.9791; Loss self: 0.0000; time: 0.13s
Val loss: 0.6607 score: 0.7273 time: 0.04s
Test loss: 0.6566 score: 0.6744 time: 0.04s
Epoch 129/1000, LR 0.000261
Train loss: 0.9789;  Loss pred: 0.9789; Loss self: 0.0000; time: 0.13s
Val loss: 0.6597 score: 0.7273 time: 0.04s
Test loss: 0.6555 score: 0.6744 time: 0.04s
Epoch 130/1000, LR 0.000260
Train loss: 0.9802;  Loss pred: 0.9802; Loss self: 0.0000; time: 0.13s
Val loss: 0.6587 score: 0.7500 time: 0.04s
Test loss: 0.6543 score: 0.6977 time: 0.04s
Epoch 131/1000, LR 0.000260
Train loss: 0.9753;  Loss pred: 0.9753; Loss self: 0.0000; time: 0.14s
Val loss: 0.6576 score: 0.7500 time: 0.04s
Test loss: 0.6532 score: 0.6977 time: 0.04s
Epoch 132/1000, LR 0.000260
Train loss: 0.9739;  Loss pred: 0.9739; Loss self: 0.0000; time: 0.14s
Val loss: 0.6565 score: 0.7727 time: 0.04s
Test loss: 0.6519 score: 0.6977 time: 0.04s
Epoch 133/1000, LR 0.000260
Train loss: 0.9703;  Loss pred: 0.9703; Loss self: 0.0000; time: 0.13s
Val loss: 0.6554 score: 0.7727 time: 0.04s
Test loss: 0.6507 score: 0.7209 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 0.9727;  Loss pred: 0.9727; Loss self: 0.0000; time: 0.13s
Val loss: 0.6543 score: 0.8182 time: 0.04s
Test loss: 0.6495 score: 0.6977 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9681;  Loss pred: 0.9681; Loss self: 0.0000; time: 0.13s
Val loss: 0.6531 score: 0.8182 time: 0.04s
Test loss: 0.6482 score: 0.6977 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9644;  Loss pred: 0.9644; Loss self: 0.0000; time: 0.13s
Val loss: 0.6519 score: 0.8182 time: 0.04s
Test loss: 0.6469 score: 0.6977 time: 0.04s
Epoch 137/1000, LR 0.000259
Train loss: 0.9637;  Loss pred: 0.9637; Loss self: 0.0000; time: 0.14s
Val loss: 0.6507 score: 0.8182 time: 0.04s
Test loss: 0.6455 score: 0.6744 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9614;  Loss pred: 0.9614; Loss self: 0.0000; time: 0.13s
Val loss: 0.6495 score: 0.7955 time: 0.04s
Test loss: 0.6442 score: 0.6744 time: 0.04s
Epoch 139/1000, LR 0.000259
Train loss: 0.9625;  Loss pred: 0.9625; Loss self: 0.0000; time: 0.13s
Val loss: 0.6482 score: 0.7955 time: 0.04s
Test loss: 0.6428 score: 0.6977 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9584;  Loss pred: 0.9584; Loss self: 0.0000; time: 0.15s
Val loss: 0.6469 score: 0.7955 time: 0.05s
Test loss: 0.6414 score: 0.6977 time: 0.05s
Epoch 141/1000, LR 0.000259
Train loss: 0.9573;  Loss pred: 0.9573; Loss self: 0.0000; time: 0.13s
Val loss: 0.6456 score: 0.8182 time: 0.04s
Test loss: 0.6399 score: 0.6977 time: 0.04s
Epoch 142/1000, LR 0.000259
Train loss: 0.9545;  Loss pred: 0.9545; Loss self: 0.0000; time: 0.13s
Val loss: 0.6442 score: 0.8182 time: 0.04s
Test loss: 0.6384 score: 0.6977 time: 0.04s
Epoch 143/1000, LR 0.000258
Train loss: 0.9522;  Loss pred: 0.9522; Loss self: 0.0000; time: 0.14s
Val loss: 0.6428 score: 0.8182 time: 0.04s
Test loss: 0.6369 score: 0.6977 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9519;  Loss pred: 0.9519; Loss self: 0.0000; time: 0.13s
Val loss: 0.6414 score: 0.8182 time: 0.04s
Test loss: 0.6353 score: 0.7209 time: 0.04s
Epoch 145/1000, LR 0.000258
Train loss: 0.9471;  Loss pred: 0.9471; Loss self: 0.0000; time: 0.13s
Val loss: 0.6399 score: 0.8182 time: 0.04s
Test loss: 0.6337 score: 0.7674 time: 0.04s
Epoch 146/1000, LR 0.000258
Train loss: 0.9470;  Loss pred: 0.9470; Loss self: 0.0000; time: 0.13s
Val loss: 0.6384 score: 0.8182 time: 0.04s
Test loss: 0.6321 score: 0.7674 time: 0.04s
Epoch 147/1000, LR 0.000258
Train loss: 0.9428;  Loss pred: 0.9428; Loss self: 0.0000; time: 0.15s
Val loss: 0.6369 score: 0.8409 time: 0.04s
Test loss: 0.6304 score: 0.7674 time: 0.04s
Epoch 148/1000, LR 0.000257
Train loss: 0.9447;  Loss pred: 0.9447; Loss self: 0.0000; time: 0.13s
Val loss: 0.6354 score: 0.8409 time: 0.04s
Test loss: 0.6287 score: 0.7674 time: 0.04s
Epoch 149/1000, LR 0.000257
Train loss: 0.9408;  Loss pred: 0.9408; Loss self: 0.0000; time: 0.13s
Val loss: 0.6338 score: 0.8409 time: 0.04s
Test loss: 0.6270 score: 0.7674 time: 0.04s
Epoch 150/1000, LR 0.000257
Train loss: 0.9391;  Loss pred: 0.9391; Loss self: 0.0000; time: 0.13s
Val loss: 0.6322 score: 0.8182 time: 0.04s
Test loss: 0.6253 score: 0.7674 time: 0.04s
Epoch 151/1000, LR 0.000257
Train loss: 0.9356;  Loss pred: 0.9356; Loss self: 0.0000; time: 0.13s
Val loss: 0.6306 score: 0.8182 time: 0.04s
Test loss: 0.6236 score: 0.7674 time: 0.04s
Epoch 152/1000, LR 0.000257
Train loss: 0.9367;  Loss pred: 0.9367; Loss self: 0.0000; time: 0.15s
Val loss: 0.6289 score: 0.8182 time: 0.04s
Test loss: 0.6218 score: 0.7674 time: 0.04s
Epoch 153/1000, LR 0.000257
Train loss: 0.9329;  Loss pred: 0.9329; Loss self: 0.0000; time: 0.13s
Val loss: 0.6272 score: 0.8182 time: 0.04s
Test loss: 0.6201 score: 0.7674 time: 0.04s
Epoch 154/1000, LR 0.000256
Train loss: 0.9342;  Loss pred: 0.9342; Loss self: 0.0000; time: 0.13s
Val loss: 0.6255 score: 0.8182 time: 0.04s
Test loss: 0.6183 score: 0.7674 time: 0.04s
Epoch 155/1000, LR 0.000256
Train loss: 0.9274;  Loss pred: 0.9274; Loss self: 0.0000; time: 0.13s
Val loss: 0.6238 score: 0.8409 time: 0.04s
Test loss: 0.6165 score: 0.7674 time: 0.04s
Epoch 156/1000, LR 0.000256
Train loss: 0.9288;  Loss pred: 0.9288; Loss self: 0.0000; time: 0.13s
Val loss: 0.6220 score: 0.8182 time: 0.04s
Test loss: 0.6147 score: 0.7674 time: 0.04s
Epoch 157/1000, LR 0.000256
Train loss: 0.9255;  Loss pred: 0.9255; Loss self: 0.0000; time: 0.15s
Val loss: 0.6203 score: 0.8182 time: 0.04s
Test loss: 0.6128 score: 0.7674 time: 0.04s
Epoch 158/1000, LR 0.000256
Train loss: 0.9218;  Loss pred: 0.9218; Loss self: 0.0000; time: 0.15s
Val loss: 0.6185 score: 0.8182 time: 0.04s
Test loss: 0.6109 score: 0.7674 time: 0.04s
Epoch 159/1000, LR 0.000255
Train loss: 0.9220;  Loss pred: 0.9220; Loss self: 0.0000; time: 0.13s
Val loss: 0.6166 score: 0.8182 time: 0.04s
Test loss: 0.6089 score: 0.7674 time: 0.04s
Epoch 160/1000, LR 0.000255
Train loss: 0.9182;  Loss pred: 0.9182; Loss self: 0.0000; time: 0.13s
Val loss: 0.6148 score: 0.8182 time: 0.04s
Test loss: 0.6069 score: 0.7674 time: 0.04s
Epoch 161/1000, LR 0.000255
Train loss: 0.9155;  Loss pred: 0.9155; Loss self: 0.0000; time: 0.13s
Val loss: 0.6129 score: 0.8182 time: 0.04s
Test loss: 0.6049 score: 0.7907 time: 0.04s
Epoch 162/1000, LR 0.000255
Train loss: 0.9147;  Loss pred: 0.9147; Loss self: 0.0000; time: 0.13s
Val loss: 0.6110 score: 0.8182 time: 0.04s
Test loss: 0.6029 score: 0.7907 time: 0.04s
Epoch 163/1000, LR 0.000255
Train loss: 0.9136;  Loss pred: 0.9136; Loss self: 0.0000; time: 0.15s
Val loss: 0.6091 score: 0.8182 time: 0.04s
Test loss: 0.6008 score: 0.7907 time: 0.04s
Epoch 164/1000, LR 0.000254
Train loss: 0.9117;  Loss pred: 0.9117; Loss self: 0.0000; time: 0.14s
Val loss: 0.6071 score: 0.8182 time: 0.04s
Test loss: 0.5987 score: 0.7907 time: 0.04s
Epoch 165/1000, LR 0.000254
Train loss: 0.9051;  Loss pred: 0.9051; Loss self: 0.0000; time: 0.13s
Val loss: 0.6052 score: 0.8182 time: 0.04s
Test loss: 0.5966 score: 0.7907 time: 0.04s
Epoch 166/1000, LR 0.000254
Train loss: 0.9052;  Loss pred: 0.9052; Loss self: 0.0000; time: 0.13s
Val loss: 0.6032 score: 0.8182 time: 0.04s
Test loss: 0.5945 score: 0.7907 time: 0.04s
Epoch 167/1000, LR 0.000254
Train loss: 0.9035;  Loss pred: 0.9035; Loss self: 0.0000; time: 0.13s
Val loss: 0.6012 score: 0.8182 time: 0.05s
Test loss: 0.5924 score: 0.7907 time: 0.04s
Epoch 168/1000, LR 0.000254
Train loss: 0.9004;  Loss pred: 0.9004; Loss self: 0.0000; time: 0.15s
Val loss: 0.5992 score: 0.8182 time: 0.05s
Test loss: 0.5902 score: 0.7907 time: 0.04s
Epoch 169/1000, LR 0.000253
Train loss: 0.8989;  Loss pred: 0.8989; Loss self: 0.0000; time: 0.13s
Val loss: 0.5971 score: 0.8182 time: 0.04s
Test loss: 0.5881 score: 0.7907 time: 0.04s
Epoch 170/1000, LR 0.000253
Train loss: 0.8968;  Loss pred: 0.8968; Loss self: 0.0000; time: 0.13s
Val loss: 0.5950 score: 0.8409 time: 0.04s
Test loss: 0.5859 score: 0.7907 time: 0.04s
Epoch 171/1000, LR 0.000253
Train loss: 0.8933;  Loss pred: 0.8933; Loss self: 0.0000; time: 0.14s
Val loss: 0.5929 score: 0.8409 time: 0.04s
Test loss: 0.5838 score: 0.7907 time: 0.04s
Epoch 172/1000, LR 0.000253
Train loss: 0.8935;  Loss pred: 0.8935; Loss self: 0.0000; time: 0.14s
Val loss: 0.5908 score: 0.8409 time: 0.04s
Test loss: 0.5816 score: 0.7907 time: 0.04s
Epoch 173/1000, LR 0.000253
Train loss: 0.8883;  Loss pred: 0.8883; Loss self: 0.0000; time: 0.13s
Val loss: 0.5887 score: 0.8409 time: 0.04s
Test loss: 0.5793 score: 0.7907 time: 0.04s
Epoch 174/1000, LR 0.000252
Train loss: 0.8876;  Loss pred: 0.8876; Loss self: 0.0000; time: 0.13s
Val loss: 0.5865 score: 0.8409 time: 0.04s
Test loss: 0.5771 score: 0.8372 time: 0.04s
Epoch 175/1000, LR 0.000252
Train loss: 0.8844;  Loss pred: 0.8844; Loss self: 0.0000; time: 0.13s
Val loss: 0.5844 score: 0.8409 time: 0.04s
Test loss: 0.5749 score: 0.8372 time: 0.04s
Epoch 176/1000, LR 0.000252
Train loss: 0.8831;  Loss pred: 0.8831; Loss self: 0.0000; time: 0.13s
Val loss: 0.5822 score: 0.8409 time: 0.04s
Test loss: 0.5726 score: 0.8372 time: 0.04s
Epoch 177/1000, LR 0.000252
Train loss: 0.8796;  Loss pred: 0.8796; Loss self: 0.0000; time: 0.13s
Val loss: 0.5800 score: 0.8182 time: 0.04s
Test loss: 0.5703 score: 0.8372 time: 0.04s
Epoch 178/1000, LR 0.000251
Train loss: 0.8774;  Loss pred: 0.8774; Loss self: 0.0000; time: 0.13s
Val loss: 0.5777 score: 0.8182 time: 0.04s
Test loss: 0.5680 score: 0.8372 time: 0.04s
Epoch 179/1000, LR 0.000251
Train loss: 0.8737;  Loss pred: 0.8737; Loss self: 0.0000; time: 0.13s
Val loss: 0.5755 score: 0.8182 time: 0.04s
Test loss: 0.5657 score: 0.8372 time: 0.04s
Epoch 180/1000, LR 0.000251
Train loss: 0.8729;  Loss pred: 0.8729; Loss self: 0.0000; time: 0.13s
Val loss: 0.5732 score: 0.8182 time: 0.04s
Test loss: 0.5634 score: 0.8372 time: 0.04s
Epoch 181/1000, LR 0.000251
Train loss: 0.8669;  Loss pred: 0.8669; Loss self: 0.0000; time: 0.13s
Val loss: 0.5710 score: 0.8182 time: 0.04s
Test loss: 0.5610 score: 0.8372 time: 0.04s
Epoch 182/1000, LR 0.000251
Train loss: 0.8675;  Loss pred: 0.8675; Loss self: 0.0000; time: 0.13s
Val loss: 0.5687 score: 0.8182 time: 0.04s
Test loss: 0.5586 score: 0.8372 time: 0.04s
Epoch 183/1000, LR 0.000250
Train loss: 0.8638;  Loss pred: 0.8638; Loss self: 0.0000; time: 0.14s
Val loss: 0.5664 score: 0.8182 time: 0.04s
Test loss: 0.5563 score: 0.8372 time: 0.04s
Epoch 184/1000, LR 0.000250
Train loss: 0.8622;  Loss pred: 0.8622; Loss self: 0.0000; time: 0.14s
Val loss: 0.5641 score: 0.8182 time: 0.04s
Test loss: 0.5539 score: 0.8372 time: 0.04s
Epoch 185/1000, LR 0.000250
Train loss: 0.8608;  Loss pred: 0.8608; Loss self: 0.0000; time: 0.13s
Val loss: 0.5618 score: 0.8182 time: 0.04s
Test loss: 0.5515 score: 0.8372 time: 0.04s
Epoch 186/1000, LR 0.000250
Train loss: 0.8555;  Loss pred: 0.8555; Loss self: 0.0000; time: 0.13s
Val loss: 0.5595 score: 0.8182 time: 0.04s
Test loss: 0.5491 score: 0.8372 time: 0.04s
Epoch 187/1000, LR 0.000249
Train loss: 0.8533;  Loss pred: 0.8533; Loss self: 0.0000; time: 0.13s
Val loss: 0.5572 score: 0.8409 time: 0.04s
Test loss: 0.5468 score: 0.8372 time: 0.04s
Epoch 188/1000, LR 0.000249
Train loss: 0.8540;  Loss pred: 0.8540; Loss self: 0.0000; time: 0.13s
Val loss: 0.5549 score: 0.8409 time: 0.04s
Test loss: 0.5444 score: 0.8372 time: 0.04s
Epoch 189/1000, LR 0.000249
Train loss: 0.8496;  Loss pred: 0.8496; Loss self: 0.0000; time: 0.13s
Val loss: 0.5525 score: 0.8409 time: 0.04s
Test loss: 0.5420 score: 0.8372 time: 0.04s
Epoch 190/1000, LR 0.000249
Train loss: 0.8469;  Loss pred: 0.8469; Loss self: 0.0000; time: 0.13s
Val loss: 0.5502 score: 0.8409 time: 0.04s
Test loss: 0.5397 score: 0.8372 time: 0.04s
Epoch 191/1000, LR 0.000249
Train loss: 0.8449;  Loss pred: 0.8449; Loss self: 0.0000; time: 0.14s
Val loss: 0.5478 score: 0.8409 time: 0.04s
Test loss: 0.5374 score: 0.8372 time: 0.04s
Epoch 192/1000, LR 0.000248
Train loss: 0.8417;  Loss pred: 0.8417; Loss self: 0.0000; time: 0.13s
Val loss: 0.5454 score: 0.8409 time: 0.04s
Test loss: 0.5351 score: 0.8372 time: 0.04s
Epoch 193/1000, LR 0.000248
Train loss: 0.8407;  Loss pred: 0.8407; Loss self: 0.0000; time: 0.15s
Val loss: 0.5430 score: 0.8409 time: 0.11s
Test loss: 0.5327 score: 0.8372 time: 0.04s
Epoch 194/1000, LR 0.000248
Train loss: 0.8384;  Loss pred: 0.8384; Loss self: 0.0000; time: 0.14s
Val loss: 0.5406 score: 0.8409 time: 0.04s
Test loss: 0.5304 score: 0.8372 time: 0.04s
Epoch 195/1000, LR 0.000248
Train loss: 0.8346;  Loss pred: 0.8346; Loss self: 0.0000; time: 0.14s
Val loss: 0.5383 score: 0.8409 time: 0.05s
Test loss: 0.5280 score: 0.8372 time: 0.04s
Epoch 196/1000, LR 0.000247
Train loss: 0.8327;  Loss pred: 0.8327; Loss self: 0.0000; time: 0.14s
Val loss: 0.5359 score: 0.8409 time: 0.04s
Test loss: 0.5256 score: 0.8372 time: 0.04s
Epoch 197/1000, LR 0.000247
Train loss: 0.8304;  Loss pred: 0.8304; Loss self: 0.0000; time: 0.13s
Val loss: 0.5335 score: 0.8409 time: 0.04s
Test loss: 0.5231 score: 0.8372 time: 0.04s
Epoch 198/1000, LR 0.000247
Train loss: 0.8291;  Loss pred: 0.8291; Loss self: 0.0000; time: 0.14s
Val loss: 0.5312 score: 0.8409 time: 0.04s
Test loss: 0.5206 score: 0.8372 time: 0.04s
Epoch 199/1000, LR 0.000247
Train loss: 0.8260;  Loss pred: 0.8260; Loss self: 0.0000; time: 0.13s
Val loss: 0.5289 score: 0.8409 time: 0.04s
Test loss: 0.5181 score: 0.8372 time: 0.04s
Epoch 200/1000, LR 0.000246
Train loss: 0.8204;  Loss pred: 0.8204; Loss self: 0.0000; time: 0.20s
Val loss: 0.5265 score: 0.8409 time: 0.04s
Test loss: 0.5156 score: 0.8372 time: 0.04s
Epoch 201/1000, LR 0.000246
Train loss: 0.8173;  Loss pred: 0.8173; Loss self: 0.0000; time: 0.13s
Val loss: 0.5241 score: 0.8409 time: 0.04s
Test loss: 0.5132 score: 0.8372 time: 0.04s
Epoch 202/1000, LR 0.000246
Train loss: 0.8173;  Loss pred: 0.8173; Loss self: 0.0000; time: 0.13s
Val loss: 0.5218 score: 0.8409 time: 0.04s
Test loss: 0.5107 score: 0.8372 time: 0.04s
Epoch 203/1000, LR 0.000246
Train loss: 0.8150;  Loss pred: 0.8150; Loss self: 0.0000; time: 0.13s
Val loss: 0.5194 score: 0.8409 time: 0.04s
Test loss: 0.5083 score: 0.8372 time: 0.04s
Epoch 204/1000, LR 0.000245
Train loss: 0.8139;  Loss pred: 0.8139; Loss self: 0.0000; time: 0.13s
Val loss: 0.5170 score: 0.8409 time: 0.04s
Test loss: 0.5060 score: 0.8372 time: 0.04s
Epoch 205/1000, LR 0.000245
Train loss: 0.8104;  Loss pred: 0.8104; Loss self: 0.0000; time: 0.14s
Val loss: 0.5146 score: 0.8409 time: 0.04s
Test loss: 0.5038 score: 0.8372 time: 0.04s
Epoch 206/1000, LR 0.000245
Train loss: 0.8078;  Loss pred: 0.8078; Loss self: 0.0000; time: 0.14s
Val loss: 0.5122 score: 0.8409 time: 0.04s
Test loss: 0.5017 score: 0.8372 time: 0.04s
Epoch 207/1000, LR 0.000245
Train loss: 0.8047;  Loss pred: 0.8047; Loss self: 0.0000; time: 0.13s
Val loss: 0.5098 score: 0.8409 time: 0.04s
Test loss: 0.4995 score: 0.8372 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.8013;  Loss pred: 0.8013; Loss self: 0.0000; time: 0.24s
Val loss: 0.5074 score: 0.8409 time: 0.04s
Test loss: 0.4973 score: 0.8372 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.8003;  Loss pred: 0.8003; Loss self: 0.0000; time: 0.13s
Val loss: 0.5051 score: 0.8409 time: 0.04s
Test loss: 0.4950 score: 0.8372 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.7992;  Loss pred: 0.7992; Loss self: 0.0000; time: 0.14s
Val loss: 0.5028 score: 0.8409 time: 0.04s
Test loss: 0.4927 score: 0.8372 time: 0.04s
Epoch 211/1000, LR 0.000244
Train loss: 0.7949;  Loss pred: 0.7949; Loss self: 0.0000; time: 0.14s
Val loss: 0.5005 score: 0.8409 time: 0.04s
Test loss: 0.4904 score: 0.8372 time: 0.04s
Epoch 212/1000, LR 0.000243
Train loss: 0.7937;  Loss pred: 0.7937; Loss self: 0.0000; time: 0.14s
Val loss: 0.4983 score: 0.8409 time: 0.04s
Test loss: 0.4881 score: 0.8372 time: 0.04s
Epoch 213/1000, LR 0.000243
Train loss: 0.7898;  Loss pred: 0.7898; Loss self: 0.0000; time: 0.14s
Val loss: 0.4961 score: 0.8409 time: 0.04s
Test loss: 0.4857 score: 0.8372 time: 0.04s
Epoch 214/1000, LR 0.000243
Train loss: 0.7895;  Loss pred: 0.7895; Loss self: 0.0000; time: 0.14s
Val loss: 0.4940 score: 0.8409 time: 0.04s
Test loss: 0.4833 score: 0.8372 time: 0.04s
Epoch 215/1000, LR 0.000243
Train loss: 0.7865;  Loss pred: 0.7865; Loss self: 0.0000; time: 0.15s
Val loss: 0.4918 score: 0.8409 time: 0.11s
Test loss: 0.4810 score: 0.8372 time: 0.04s
Epoch 216/1000, LR 0.000242
Train loss: 0.7819;  Loss pred: 0.7819; Loss self: 0.0000; time: 0.14s
Val loss: 0.4896 score: 0.8409 time: 0.04s
Test loss: 0.4788 score: 0.8372 time: 0.04s
Epoch 217/1000, LR 0.000242
Train loss: 0.7817;  Loss pred: 0.7817; Loss self: 0.0000; time: 0.14s
Val loss: 0.4874 score: 0.8409 time: 0.04s
Test loss: 0.4766 score: 0.8372 time: 0.04s
Epoch 218/1000, LR 0.000242
Train loss: 0.7763;  Loss pred: 0.7763; Loss self: 0.0000; time: 0.14s
Val loss: 0.4852 score: 0.8409 time: 0.04s
Test loss: 0.4745 score: 0.8372 time: 0.04s
Epoch 219/1000, LR 0.000242
Train loss: 0.7772;  Loss pred: 0.7772; Loss self: 0.0000; time: 0.14s
Val loss: 0.4830 score: 0.8409 time: 0.04s
Test loss: 0.4725 score: 0.8372 time: 0.04s
Epoch 220/1000, LR 0.000241
Train loss: 0.7738;  Loss pred: 0.7738; Loss self: 0.0000; time: 0.14s
Val loss: 0.4807 score: 0.8409 time: 0.04s
Test loss: 0.4705 score: 0.8372 time: 0.04s
Epoch 221/1000, LR 0.000241
Train loss: 0.7690;  Loss pred: 0.7690; Loss self: 0.0000; time: 0.14s
Val loss: 0.4785 score: 0.8409 time: 0.04s
Test loss: 0.4686 score: 0.8372 time: 0.04s
Epoch 222/1000, LR 0.000241
Train loss: 0.7681;  Loss pred: 0.7681; Loss self: 0.0000; time: 0.14s
Val loss: 0.4762 score: 0.8409 time: 0.04s
Test loss: 0.4666 score: 0.8372 time: 0.04s
Epoch 223/1000, LR 0.000241
Train loss: 0.7651;  Loss pred: 0.7651; Loss self: 0.0000; time: 0.14s
Val loss: 0.4741 score: 0.8409 time: 0.04s
Test loss: 0.4645 score: 0.8372 time: 0.04s
Epoch 224/1000, LR 0.000240
Train loss: 0.7660;  Loss pred: 0.7660; Loss self: 0.0000; time: 0.14s
Val loss: 0.4720 score: 0.8409 time: 0.04s
Test loss: 0.4625 score: 0.8372 time: 0.04s
Epoch 225/1000, LR 0.000240
Train loss: 0.7613;  Loss pred: 0.7613; Loss self: 0.0000; time: 0.15s
Val loss: 0.4699 score: 0.8409 time: 0.04s
Test loss: 0.4606 score: 0.8372 time: 0.04s
Epoch 226/1000, LR 0.000240
Train loss: 0.7572;  Loss pred: 0.7572; Loss self: 0.0000; time: 0.14s
Val loss: 0.4677 score: 0.8409 time: 0.04s
Test loss: 0.4587 score: 0.8372 time: 0.04s
Epoch 227/1000, LR 0.000240
Train loss: 0.7583;  Loss pred: 0.7583; Loss self: 0.0000; time: 0.14s
Val loss: 0.4656 score: 0.8409 time: 0.04s
Test loss: 0.4569 score: 0.8372 time: 0.04s
Epoch 228/1000, LR 0.000239
Train loss: 0.7545;  Loss pred: 0.7545; Loss self: 0.0000; time: 0.14s
Val loss: 0.4635 score: 0.8409 time: 0.04s
Test loss: 0.4550 score: 0.8372 time: 0.04s
Epoch 229/1000, LR 0.000239
Train loss: 0.7511;  Loss pred: 0.7511; Loss self: 0.0000; time: 0.14s
Val loss: 0.4614 score: 0.8409 time: 0.04s
Test loss: 0.4532 score: 0.8372 time: 0.04s
Epoch 230/1000, LR 0.000239
Train loss: 0.7493;  Loss pred: 0.7493; Loss self: 0.0000; time: 0.14s
Val loss: 0.4594 score: 0.8409 time: 0.04s
Test loss: 0.4514 score: 0.8372 time: 0.04s
Epoch 231/1000, LR 0.000238
Train loss: 0.7469;  Loss pred: 0.7469; Loss self: 0.0000; time: 0.13s
Val loss: 0.4573 score: 0.8409 time: 0.04s
Test loss: 0.4496 score: 0.8372 time: 0.04s
Epoch 232/1000, LR 0.000238
Train loss: 0.7446;  Loss pred: 0.7446; Loss self: 0.0000; time: 0.14s
Val loss: 0.4553 score: 0.8409 time: 0.04s
Test loss: 0.4478 score: 0.8372 time: 0.04s
Epoch 233/1000, LR 0.000238
Train loss: 0.7436;  Loss pred: 0.7436; Loss self: 0.0000; time: 0.13s
Val loss: 0.4533 score: 0.8409 time: 0.04s
Test loss: 0.4460 score: 0.8372 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.7409;  Loss pred: 0.7409; Loss self: 0.0000; time: 0.14s
Val loss: 0.4514 score: 0.8409 time: 0.04s
Test loss: 0.4441 score: 0.8372 time: 0.04s
Epoch 235/1000, LR 0.000237
Train loss: 0.7408;  Loss pred: 0.7408; Loss self: 0.0000; time: 0.13s
Val loss: 0.4495 score: 0.8409 time: 0.04s
Test loss: 0.4423 score: 0.8372 time: 0.04s
Epoch 236/1000, LR 0.000237
Train loss: 0.7369;  Loss pred: 0.7369; Loss self: 0.0000; time: 0.13s
Val loss: 0.4476 score: 0.8409 time: 0.04s
Test loss: 0.4406 score: 0.8372 time: 0.04s
Epoch 237/1000, LR 0.000237
Train loss: 0.7310;  Loss pred: 0.7310; Loss self: 0.0000; time: 0.13s
Val loss: 0.4457 score: 0.8409 time: 0.04s
Test loss: 0.4389 score: 0.8372 time: 0.04s
Epoch 238/1000, LR 0.000236
Train loss: 0.7315;  Loss pred: 0.7315; Loss self: 0.0000; time: 0.13s
Val loss: 0.4438 score: 0.8409 time: 0.04s
Test loss: 0.4372 score: 0.8372 time: 0.04s
Epoch 239/1000, LR 0.000236
Train loss: 0.7295;  Loss pred: 0.7295; Loss self: 0.0000; time: 0.13s
Val loss: 0.4419 score: 0.8409 time: 0.04s
Test loss: 0.4357 score: 0.8372 time: 0.04s
Epoch 240/1000, LR 0.000236
Train loss: 0.7292;  Loss pred: 0.7292; Loss self: 0.0000; time: 0.13s
Val loss: 0.4399 score: 0.8409 time: 0.04s
Test loss: 0.4342 score: 0.8372 time: 0.04s
Epoch 241/1000, LR 0.000236
Train loss: 0.7257;  Loss pred: 0.7257; Loss self: 0.0000; time: 0.13s
Val loss: 0.4381 score: 0.8409 time: 0.04s
Test loss: 0.4326 score: 0.8372 time: 0.04s
Epoch 242/1000, LR 0.000235
Train loss: 0.7230;  Loss pred: 0.7230; Loss self: 0.0000; time: 0.13s
Val loss: 0.4363 score: 0.8409 time: 0.04s
Test loss: 0.4310 score: 0.8372 time: 0.04s
Epoch 243/1000, LR 0.000235
Train loss: 0.7230;  Loss pred: 0.7230; Loss self: 0.0000; time: 0.14s
Val loss: 0.4345 score: 0.8409 time: 0.04s
Test loss: 0.4294 score: 0.8372 time: 0.04s
Epoch 244/1000, LR 0.000235
Train loss: 0.7208;  Loss pred: 0.7208; Loss self: 0.0000; time: 0.13s
Val loss: 0.4328 score: 0.8409 time: 0.04s
Test loss: 0.4277 score: 0.8372 time: 0.04s
Epoch 245/1000, LR 0.000234
Train loss: 0.7154;  Loss pred: 0.7154; Loss self: 0.0000; time: 0.13s
Val loss: 0.4312 score: 0.8409 time: 0.04s
Test loss: 0.4261 score: 0.8372 time: 0.04s
Epoch 246/1000, LR 0.000234
Train loss: 0.7136;  Loss pred: 0.7136; Loss self: 0.0000; time: 0.13s
Val loss: 0.4295 score: 0.8409 time: 0.04s
Test loss: 0.4244 score: 0.8372 time: 0.04s
Epoch 247/1000, LR 0.000234
Train loss: 0.7160;  Loss pred: 0.7160; Loss self: 0.0000; time: 0.13s
Val loss: 0.4279 score: 0.8409 time: 0.04s
Test loss: 0.4228 score: 0.8372 time: 0.04s
Epoch 248/1000, LR 0.000234
Train loss: 0.7080;  Loss pred: 0.7080; Loss self: 0.0000; time: 0.13s
Val loss: 0.4263 score: 0.8409 time: 0.04s
Test loss: 0.4212 score: 0.8372 time: 0.23s
Epoch 249/1000, LR 0.000233
Train loss: 0.7084;  Loss pred: 0.7084; Loss self: 0.0000; time: 1.50s
Val loss: 0.4247 score: 0.8409 time: 0.64s
Test loss: 0.4197 score: 0.8372 time: 0.30s
Epoch 250/1000, LR 0.000233
Train loss: 0.7090;  Loss pred: 0.7090; Loss self: 0.0000; time: 0.90s
Val loss: 0.4230 score: 0.8409 time: 0.26s
Test loss: 0.4183 score: 0.8372 time: 0.37s
Epoch 251/1000, LR 0.000233
Train loss: 0.7043;  Loss pred: 0.7043; Loss self: 0.0000; time: 0.95s
Val loss: 0.4214 score: 0.8409 time: 0.17s
Test loss: 0.4169 score: 0.8372 time: 0.26s
Epoch 252/1000, LR 0.000232
Train loss: 0.7029;  Loss pred: 0.7029; Loss self: 0.0000; time: 0.53s
Val loss: 0.4198 score: 0.8409 time: 1.35s
Test loss: 0.4154 score: 0.8372 time: 0.50s
Epoch 253/1000, LR 0.000232
Train loss: 0.7020;  Loss pred: 0.7020; Loss self: 0.0000; time: 0.77s
Val loss: 0.4182 score: 0.8409 time: 0.07s
Test loss: 0.4141 score: 0.8372 time: 0.23s
Epoch 254/1000, LR 0.000232
Train loss: 0.7008;  Loss pred: 0.7008; Loss self: 0.0000; time: 0.23s
Val loss: 0.4165 score: 0.8409 time: 0.04s
Test loss: 0.4128 score: 0.8372 time: 0.04s
Epoch 255/1000, LR 0.000232
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.14s
Val loss: 0.4149 score: 0.8409 time: 0.04s
Test loss: 0.4115 score: 0.8372 time: 0.04s
Epoch 256/1000, LR 0.000231
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.13s
Val loss: 0.4134 score: 0.8409 time: 0.04s
Test loss: 0.4103 score: 0.8372 time: 0.04s
Epoch 257/1000, LR 0.000231
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.13s
Val loss: 0.4117 score: 0.8409 time: 0.04s
Test loss: 0.4091 score: 0.8372 time: 0.04s
Epoch 258/1000, LR 0.000231
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.13s
Val loss: 0.4101 score: 0.8409 time: 0.04s
Test loss: 0.4080 score: 0.8372 time: 0.04s
Epoch 259/1000, LR 0.000230
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.13s
Val loss: 0.4086 score: 0.8409 time: 0.04s
Test loss: 0.4067 score: 0.8372 time: 0.04s
Epoch 260/1000, LR 0.000230
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.13s
Val loss: 0.4072 score: 0.8409 time: 0.04s
Test loss: 0.4055 score: 0.8372 time: 0.04s
Epoch 261/1000, LR 0.000230
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.13s
Val loss: 0.4057 score: 0.8409 time: 0.04s
Test loss: 0.4043 score: 0.8372 time: 0.04s
Epoch 262/1000, LR 0.000229
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.13s
Val loss: 0.4043 score: 0.8409 time: 0.04s
Test loss: 0.4031 score: 0.8372 time: 0.04s
Epoch 263/1000, LR 0.000229
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.13s
Val loss: 0.4028 score: 0.8409 time: 0.04s
Test loss: 0.4019 score: 0.8372 time: 0.04s
Epoch 264/1000, LR 0.000229
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.13s
Val loss: 0.4015 score: 0.8409 time: 0.04s
Test loss: 0.4008 score: 0.8372 time: 0.04s
Epoch 265/1000, LR 0.000228
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.13s
Val loss: 0.4001 score: 0.8409 time: 0.04s
Test loss: 0.3996 score: 0.8372 time: 0.04s
Epoch 266/1000, LR 0.000228
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.13s
Val loss: 0.3988 score: 0.8409 time: 0.04s
Test loss: 0.3984 score: 0.8372 time: 0.04s
Epoch 267/1000, LR 0.000228
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.14s
Val loss: 0.3975 score: 0.8409 time: 0.04s
Test loss: 0.3973 score: 0.8372 time: 0.04s
Epoch 268/1000, LR 0.000228
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.14s
Val loss: 0.3964 score: 0.8409 time: 0.04s
Test loss: 0.3961 score: 0.8372 time: 0.04s
Epoch 269/1000, LR 0.000227
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.14s
Val loss: 0.3953 score: 0.8409 time: 0.04s
Test loss: 0.3949 score: 0.8372 time: 0.04s
Epoch 270/1000, LR 0.000227
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.14s
Val loss: 0.3942 score: 0.8409 time: 0.04s
Test loss: 0.3937 score: 0.8372 time: 0.04s
Epoch 271/1000, LR 0.000227
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.14s
Val loss: 0.3930 score: 0.8409 time: 0.04s
Test loss: 0.3926 score: 0.8372 time: 0.04s
Epoch 272/1000, LR 0.000226
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.14s
Val loss: 0.3920 score: 0.8409 time: 0.04s
Test loss: 0.3914 score: 0.8372 time: 0.04s
Epoch 273/1000, LR 0.000226
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.14s
Val loss: 0.3909 score: 0.8409 time: 0.04s
Test loss: 0.3904 score: 0.8372 time: 0.04s
Epoch 274/1000, LR 0.000226
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.14s
Val loss: 0.3896 score: 0.8409 time: 0.04s
Test loss: 0.3894 score: 0.8372 time: 0.04s
Epoch 275/1000, LR 0.000225
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.13s
Val loss: 0.3885 score: 0.8409 time: 0.04s
Test loss: 0.3885 score: 0.8372 time: 0.04s
Epoch 276/1000, LR 0.000225
Train loss: 0.6621;  Loss pred: 0.6621; Loss self: 0.0000; time: 0.14s
Val loss: 0.3871 score: 0.8409 time: 0.04s
Test loss: 0.3876 score: 0.8372 time: 0.04s
Epoch 277/1000, LR 0.000225
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.14s
Val loss: 0.3858 score: 0.8409 time: 0.04s
Test loss: 0.3868 score: 0.8372 time: 0.04s
Epoch 278/1000, LR 0.000224
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.14s
Val loss: 0.3845 score: 0.8409 time: 0.04s
Test loss: 0.3861 score: 0.8372 time: 0.04s
Epoch 279/1000, LR 0.000224
Train loss: 0.6581;  Loss pred: 0.6581; Loss self: 0.0000; time: 0.14s
Val loss: 0.3833 score: 0.8409 time: 0.04s
Test loss: 0.3852 score: 0.8372 time: 0.04s
Epoch 280/1000, LR 0.000224
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.13s
Val loss: 0.3821 score: 0.8409 time: 0.04s
Test loss: 0.3843 score: 0.8372 time: 0.04s
Epoch 281/1000, LR 0.000223
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 0.14s
Val loss: 0.3811 score: 0.8409 time: 0.04s
Test loss: 0.3834 score: 0.8372 time: 0.04s
Epoch 282/1000, LR 0.000223
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.14s
Val loss: 0.3802 score: 0.8409 time: 0.04s
Test loss: 0.3824 score: 0.8372 time: 0.04s
Epoch 283/1000, LR 0.000223
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.14s
Val loss: 0.3794 score: 0.8409 time: 0.04s
Test loss: 0.3814 score: 0.8372 time: 0.04s
Epoch 284/1000, LR 0.000222
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.14s
Val loss: 0.3784 score: 0.8409 time: 0.04s
Test loss: 0.3806 score: 0.8372 time: 0.04s
Epoch 285/1000, LR 0.000222
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.14s
Val loss: 0.3773 score: 0.8409 time: 0.04s
Test loss: 0.3798 score: 0.8372 time: 0.04s
Epoch 286/1000, LR 0.000222
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.13s
Val loss: 0.3762 score: 0.8409 time: 0.04s
Test loss: 0.3790 score: 0.8605 time: 0.04s
Epoch 287/1000, LR 0.000221
Train loss: 0.6457;  Loss pred: 0.6457; Loss self: 0.0000; time: 0.13s
Val loss: 0.3751 score: 0.8409 time: 0.04s
Test loss: 0.3783 score: 0.8605 time: 0.04s
Epoch 288/1000, LR 0.000221
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.14s
Val loss: 0.3739 score: 0.8409 time: 0.04s
Test loss: 0.3777 score: 0.8605 time: 0.04s
Epoch 289/1000, LR 0.000221
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.14s
Val loss: 0.3726 score: 0.8409 time: 0.04s
Test loss: 0.3771 score: 0.8605 time: 0.04s
Epoch 290/1000, LR 0.000220
Train loss: 0.6446;  Loss pred: 0.6446; Loss self: 0.0000; time: 0.13s
Val loss: 0.3713 score: 0.8409 time: 0.04s
Test loss: 0.3766 score: 0.8605 time: 0.04s
Epoch 291/1000, LR 0.000220
Train loss: 0.6412;  Loss pred: 0.6412; Loss self: 0.0000; time: 0.13s
Val loss: 0.3701 score: 0.8409 time: 0.04s
Test loss: 0.3760 score: 0.8605 time: 0.04s
Epoch 292/1000, LR 0.000220
Train loss: 0.6419;  Loss pred: 0.6419; Loss self: 0.0000; time: 0.13s
Val loss: 0.3692 score: 0.8409 time: 0.04s
Test loss: 0.3753 score: 0.8605 time: 0.04s
Epoch 293/1000, LR 0.000219
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.14s
Val loss: 0.3682 score: 0.8409 time: 0.04s
Test loss: 0.3746 score: 0.8605 time: 0.04s
Epoch 294/1000, LR 0.000219
Train loss: 0.6394;  Loss pred: 0.6394; Loss self: 0.0000; time: 0.13s
Val loss: 0.3674 score: 0.8409 time: 0.04s
Test loss: 0.3738 score: 0.8605 time: 0.04s
Epoch 295/1000, LR 0.000219
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.14s
Val loss: 0.3666 score: 0.8409 time: 0.04s
Test loss: 0.3730 score: 0.8605 time: 0.04s
Epoch 296/1000, LR 0.000218
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 0.14s
Val loss: 0.3657 score: 0.8409 time: 0.04s
Test loss: 0.3724 score: 0.8605 time: 0.04s
Epoch 297/1000, LR 0.000218
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.14s
Val loss: 0.3650 score: 0.8409 time: 0.04s
Test loss: 0.3716 score: 0.8605 time: 0.04s
Epoch 298/1000, LR 0.000218
Train loss: 0.6344;  Loss pred: 0.6344; Loss self: 0.0000; time: 0.14s
Val loss: 0.3643 score: 0.8409 time: 0.04s
Test loss: 0.3708 score: 0.8605 time: 0.04s
Epoch 299/1000, LR 0.000217
Train loss: 0.6335;  Loss pred: 0.6335; Loss self: 0.0000; time: 0.14s
Val loss: 0.3638 score: 0.8409 time: 0.04s
Test loss: 0.3700 score: 0.8605 time: 0.04s
Epoch 300/1000, LR 0.000217
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 0.14s
Val loss: 0.3632 score: 0.8409 time: 0.04s
Test loss: 0.3693 score: 0.8605 time: 0.04s
Epoch 301/1000, LR 0.000217
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.13s
Val loss: 0.3625 score: 0.8409 time: 0.04s
Test loss: 0.3686 score: 0.8605 time: 0.04s
Epoch 302/1000, LR 0.000216
Train loss: 0.6311;  Loss pred: 0.6311; Loss self: 0.0000; time: 0.13s
Val loss: 0.3615 score: 0.8409 time: 0.04s
Test loss: 0.3681 score: 0.8605 time: 0.04s
Epoch 303/1000, LR 0.000216
Train loss: 0.6289;  Loss pred: 0.6289; Loss self: 0.0000; time: 0.13s
Val loss: 0.3606 score: 0.8409 time: 0.04s
Test loss: 0.3675 score: 0.8605 time: 0.05s
Epoch 304/1000, LR 0.000216
Train loss: 0.6301;  Loss pred: 0.6301; Loss self: 0.0000; time: 0.14s
Val loss: 0.3596 score: 0.8409 time: 0.04s
Test loss: 0.3671 score: 0.8605 time: 0.04s
Epoch 305/1000, LR 0.000215
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.13s
Val loss: 0.3583 score: 0.8409 time: 0.04s
Test loss: 0.3667 score: 0.8605 time: 0.04s
Epoch 306/1000, LR 0.000215
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.13s
Val loss: 0.3571 score: 0.8409 time: 0.04s
Test loss: 0.3664 score: 0.8605 time: 0.04s
Epoch 307/1000, LR 0.000215
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.13s
Val loss: 0.3560 score: 0.8409 time: 0.04s
Test loss: 0.3660 score: 0.8605 time: 0.04s
Epoch 308/1000, LR 0.000214
Train loss: 0.6257;  Loss pred: 0.6257; Loss self: 0.0000; time: 0.13s
Val loss: 0.3553 score: 0.8409 time: 0.04s
Test loss: 0.3654 score: 0.8605 time: 0.04s
Epoch 309/1000, LR 0.000214
Train loss: 0.6250;  Loss pred: 0.6250; Loss self: 0.0000; time: 0.13s
Val loss: 0.3547 score: 0.8409 time: 0.04s
Test loss: 0.3648 score: 0.8605 time: 0.04s
Epoch 310/1000, LR 0.000214
Train loss: 0.6219;  Loss pred: 0.6219; Loss self: 0.0000; time: 0.13s
Val loss: 0.3542 score: 0.8409 time: 0.04s
Test loss: 0.3641 score: 0.8605 time: 0.04s
Epoch 311/1000, LR 0.000213
Train loss: 0.6235;  Loss pred: 0.6235; Loss self: 0.0000; time: 0.13s
Val loss: 0.3539 score: 0.8409 time: 0.04s
Test loss: 0.3634 score: 0.8605 time: 0.04s
Epoch 312/1000, LR 0.000213
Train loss: 0.6202;  Loss pred: 0.6202; Loss self: 0.0000; time: 0.15s
Val loss: 0.3534 score: 0.8409 time: 0.04s
Test loss: 0.3628 score: 0.8837 time: 0.04s
Epoch 313/1000, LR 0.000213
Train loss: 0.6176;  Loss pred: 0.6176; Loss self: 0.0000; time: 0.13s
Val loss: 0.3530 score: 0.8409 time: 0.04s
Test loss: 0.3621 score: 0.8837 time: 0.04s
Epoch 314/1000, LR 0.000212
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 0.14s
Val loss: 0.3526 score: 0.8409 time: 0.04s
Test loss: 0.3615 score: 0.8837 time: 0.04s
Epoch 315/1000, LR 0.000212
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.14s
Val loss: 0.3520 score: 0.8409 time: 0.04s
Test loss: 0.3610 score: 0.8837 time: 0.04s
Epoch 316/1000, LR 0.000212
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.13s
Val loss: 0.3511 score: 0.8409 time: 0.04s
Test loss: 0.3606 score: 0.8837 time: 0.04s
Epoch 317/1000, LR 0.000211
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.13s
Val loss: 0.3505 score: 0.8409 time: 0.04s
Test loss: 0.3601 score: 0.8837 time: 0.04s
Epoch 318/1000, LR 0.000211
Train loss: 0.6127;  Loss pred: 0.6127; Loss self: 0.0000; time: 0.13s
Val loss: 0.3499 score: 0.8409 time: 0.04s
Test loss: 0.3597 score: 0.8837 time: 0.04s
Epoch 319/1000, LR 0.000210
Train loss: 0.6154;  Loss pred: 0.6154; Loss self: 0.0000; time: 0.13s
Val loss: 0.3492 score: 0.8409 time: 0.04s
Test loss: 0.3592 score: 0.8837 time: 0.04s
Epoch 320/1000, LR 0.000210
Train loss: 0.6140;  Loss pred: 0.6140; Loss self: 0.0000; time: 5.13s
Val loss: 0.3486 score: 0.8409 time: 0.06s
Test loss: 0.3588 score: 0.8837 time: 0.81s
Epoch 321/1000, LR 0.000210
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.95s
Val loss: 0.3481 score: 0.8409 time: 0.25s
Test loss: 0.3583 score: 0.8837 time: 0.83s
Epoch 322/1000, LR 0.000209
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.75s
Val loss: 0.3473 score: 0.8409 time: 0.05s
Test loss: 0.3579 score: 0.8837 time: 0.04s
Epoch 323/1000, LR 0.000209
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.15s
Val loss: 0.3465 score: 0.8409 time: 0.04s
Test loss: 0.3576 score: 0.8837 time: 0.04s
Epoch 324/1000, LR 0.000209
Train loss: 0.6099;  Loss pred: 0.6099; Loss self: 0.0000; time: 0.15s
Val loss: 0.3456 score: 0.8409 time: 0.04s
Test loss: 0.3572 score: 0.8837 time: 0.04s
Epoch 325/1000, LR 0.000208
Train loss: 0.6117;  Loss pred: 0.6117; Loss self: 0.0000; time: 0.14s
Val loss: 0.3447 score: 0.8409 time: 0.05s
Test loss: 0.3569 score: 0.8837 time: 0.05s
Epoch 326/1000, LR 0.000208
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 0.15s
Val loss: 0.3439 score: 0.8409 time: 0.05s
Test loss: 0.3566 score: 0.8837 time: 0.05s
Epoch 327/1000, LR 0.000208
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.14s
Val loss: 0.3429 score: 0.8409 time: 0.04s
Test loss: 0.3563 score: 0.8837 time: 0.04s
Epoch 328/1000, LR 0.000207
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.14s
Val loss: 0.3422 score: 0.8409 time: 0.04s
Test loss: 0.3560 score: 0.8837 time: 0.04s
Epoch 329/1000, LR 0.000207
Train loss: 0.6038;  Loss pred: 0.6038; Loss self: 0.0000; time: 0.15s
Val loss: 0.3418 score: 0.8409 time: 0.04s
Test loss: 0.3555 score: 0.8837 time: 0.04s
Epoch 330/1000, LR 0.000207
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.13s
Val loss: 0.3416 score: 0.8409 time: 0.04s
Test loss: 0.3550 score: 0.8837 time: 0.04s
Epoch 331/1000, LR 0.000206
Train loss: 0.6032;  Loss pred: 0.6032; Loss self: 0.0000; time: 0.14s
Val loss: 0.3414 score: 0.8409 time: 0.04s
Test loss: 0.3545 score: 0.8837 time: 0.04s
Epoch 332/1000, LR 0.000206
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.14s
Val loss: 0.3410 score: 0.8409 time: 0.04s
Test loss: 0.3540 score: 0.8837 time: 0.04s
Epoch 333/1000, LR 0.000205
Train loss: 0.6037;  Loss pred: 0.6037; Loss self: 0.0000; time: 0.14s
Val loss: 0.3406 score: 0.8409 time: 0.04s
Test loss: 0.3536 score: 0.8837 time: 0.04s
Epoch 334/1000, LR 0.000205
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.13s
Val loss: 0.3400 score: 0.8409 time: 0.04s
Test loss: 0.3533 score: 0.8837 time: 0.04s
Epoch 335/1000, LR 0.000205
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 0.15s
Val loss: 0.3394 score: 0.8409 time: 0.04s
Test loss: 0.3529 score: 0.8837 time: 0.04s
Epoch 336/1000, LR 0.000204
Train loss: 0.6048;  Loss pred: 0.6048; Loss self: 0.0000; time: 0.15s
Val loss: 0.3388 score: 0.8409 time: 0.04s
Test loss: 0.3526 score: 0.8837 time: 0.04s
Epoch 337/1000, LR 0.000204
Train loss: 0.6009;  Loss pred: 0.6009; Loss self: 0.0000; time: 0.13s
Val loss: 0.3383 score: 0.8409 time: 0.04s
Test loss: 0.3522 score: 0.8837 time: 0.04s
Epoch 338/1000, LR 0.000204
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.13s
Val loss: 0.3376 score: 0.8409 time: 0.04s
Test loss: 0.3519 score: 0.8837 time: 0.04s
Epoch 339/1000, LR 0.000203
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.13s
Val loss: 0.3371 score: 0.8409 time: 0.04s
Test loss: 0.3516 score: 0.8837 time: 0.04s
Epoch 340/1000, LR 0.000203
Train loss: 0.6006;  Loss pred: 0.6006; Loss self: 0.0000; time: 0.14s
Val loss: 0.3364 score: 0.8409 time: 0.04s
Test loss: 0.3513 score: 0.8837 time: 0.04s
Epoch 341/1000, LR 0.000203
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.13s
Val loss: 0.3358 score: 0.8409 time: 0.04s
Test loss: 0.3509 score: 0.8837 time: 0.04s
Epoch 342/1000, LR 0.000202
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.14s
Val loss: 0.3357 score: 0.8409 time: 0.04s
Test loss: 0.3505 score: 0.8837 time: 0.04s
Epoch 343/1000, LR 0.000202
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 0.15s
Val loss: 0.3354 score: 0.8409 time: 0.04s
Test loss: 0.3501 score: 0.8837 time: 0.04s
Epoch 344/1000, LR 0.000201
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.14s
Val loss: 0.3353 score: 0.8409 time: 0.05s
Test loss: 0.3497 score: 0.8837 time: 0.04s
Epoch 345/1000, LR 0.000201
Train loss: 0.5915;  Loss pred: 0.5915; Loss self: 0.0000; time: 0.14s
Val loss: 0.3355 score: 0.8409 time: 0.04s
Test loss: 0.3493 score: 0.8837 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 346/1000, LR 0.000201
Train loss: 0.5946;  Loss pred: 0.5946; Loss self: 0.0000; time: 0.14s
Val loss: 0.3354 score: 0.8409 time: 0.04s
Test loss: 0.3489 score: 0.8837 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 343,   Train_Loss: 0.5930,   Val_Loss: 0.3353,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3353,   Test_Precision: 0.9000,   Test_Recall: 0.8571,   Test_accuracy: 0.8780,   Test_Score: 0.8837,   Test_loss: 0.3497


[0.04922064999118447, 0.04941669595427811, 0.04924111801665276, 0.049747382989153266, 0.049422334996052086, 0.049253578996285796, 0.049656921066343784, 0.049624510924331844, 0.04941847105510533, 0.04981841496191919, 0.04920222098007798, 0.048916789004579186, 0.049421956995502114, 0.04967916198074818, 0.049651845009066164, 0.0494889150140807, 0.04979349207133055, 0.04955250897910446, 0.04921476298477501, 0.04957458202261478, 0.051353024085983634, 0.04950558696873486, 0.04897504497785121, 0.04956593899987638, 0.04918252502102405, 0.049170505022630095, 0.04965343698859215, 0.049291520030237734, 0.04899183998350054, 0.049021818093024194, 0.054138557985424995, 0.05422956799156964, 0.04967658198438585, 0.0487282678950578, 0.049015368917025626, 0.04886267299298197, 0.04886375507339835, 0.048661505919881165, 0.04884719604160637, 0.04872864997014403, 0.049043782986700535, 0.051025092019699514, 0.0531561030074954, 0.04926647793035954, 0.048869961057789624, 0.048978606006130576, 0.04875357390847057, 0.04872388602234423, 0.04884985601529479, 0.04902935295831412, 0.0490850149653852, 0.04902375303208828, 0.048582189017906785, 0.04863453097641468, 0.04892378195654601, 0.049488153075799346, 0.0483997690025717, 0.0484091219259426, 0.048583543044514954, 0.04856289294548333, 0.048826897982507944, 0.04832498903851956, 0.04875470302067697, 0.05018236709292978, 0.04849700501654297, 0.04877716698683798, 0.0486322189681232, 0.04840682400390506, 0.04883570899255574, 0.04902175010647625, 0.04866177495568991, 0.048763645929284394, 0.04898058401886374, 0.04855458007659763, 0.04852651199325919, 0.048703090054914355, 0.04872171301394701, 0.04852223990019411, 0.057644085958600044, 0.04941553995013237, 0.04876890406012535, 0.048995697987265885, 0.04859578295145184, 0.0488334329565987, 0.04879172402434051, 0.04880735604092479, 0.048909430974163115, 0.048781379009597, 0.04895205399952829, 0.04862294008489698, 0.060744834947399795, 0.04898693994618952, 0.04906703601591289, 0.04886687803082168, 0.05206469795666635, 0.048269725986756384, 0.04908407502807677, 0.04853798798285425, 0.05188106093555689, 0.049352950998581946, 0.048838256974704564, 0.0491840090835467, 0.04917822091374546, 0.04886202106717974, 0.052633660030551255, 0.050360189052298665, 0.05081832897849381, 0.05090985691640526, 0.05051422503311187, 0.05445338599383831, 0.051394575042650104, 0.05101532791741192, 0.050891519989818335, 0.051028690999373794, 0.051450675004161894, 0.05085875291842967, 0.05130895401816815, 0.051750886952504516, 0.0553202040027827, 1.6320026649627835, 1.1890135350404307, 0.21809672401286662, 0.05045364401303232, 0.04817296506371349, 0.04741288803052157, 0.04697693698108196, 0.04705099493730813, 0.05109367205295712, 0.04697785899043083, 0.04745295504108071, 0.04739968595094979, 0.04714998498093337, 0.04735149699263275, 0.050252343993633986, 0.047257171012461185, 0.050606470089405775, 0.046967274975031614, 0.046911550918594, 0.049634119030088186, 0.04671421798411757, 0.049022478982806206, 0.0491070969728753, 0.048724394058808684, 0.04896201903466135, 0.049843472079373896, 0.048960992018692195, 0.048587990924715996, 0.048574209096841514, 0.048867576057091355, 0.04900996305514127, 0.04848380200564861, 0.04859815095551312, 0.04893369204364717, 0.048831686028279364, 0.04920670704450458, 0.04843733995221555, 0.04853672906756401, 0.04871616803575307, 0.049378890078514814, 0.048486235085874796, 0.05235349596478045, 0.05274222698062658, 0.04963420901913196, 0.04763965599704534, 0.04782193596474826, 0.04776431410573423, 0.04774418903980404, 0.04799273191019893, 0.047783607034944, 0.04747210699133575, 0.054642876028083265, 0.04817644995637238, 0.0481116030132398, 0.05200566095300019, 0.048233981942757964, 0.048657121951691806, 0.04847503104247153, 0.05680633708834648, 0.04860574600752443, 0.051963796955533326, 0.05112795205786824, 0.04877728992141783, 0.0490876529365778, 0.04892161290626973, 0.04917198501061648, 0.052201297017745674, 1.2123389809858054, 0.04946600797120482, 0.04962431103922427, 0.04879154602531344, 0.049221316003240645, 0.04876709496602416, 0.04870954796206206, 0.04835848498623818, 0.04920786607544869, 0.048726508975960314, 0.04812199797015637, 0.047821801039390266, 0.04856651392765343, 0.05131046799942851, 0.048049286007881165, 0.05164725298527628, 0.05145609600003809, 0.04833760601468384, 0.048224042053334415, 0.04886943404562771, 0.047672220040112734, 0.04791681398637593, 0.0515491790138185, 0.0463056480512023, 0.052274529938586056, 0.051531131961382926, 0.05140866804867983, 0.05155245109926909, 0.0467433090088889, 0.049791830009780824, 0.050736612058244646, 0.04659940500278026, 0.04689398698974401, 0.047288903035223484, 0.046894517028704286, 0.04982420802116394, 0.05158586299512535, 0.052360652945935726, 0.05559466499835253, 0.04835569206625223, 0.04867534909863025, 0.04864098795223981, 0.048137011006474495, 0.05337206204421818, 0.048029719037003815, 0.04748962598387152, 0.04780523106455803, 0.04785729700233787, 0.05063087004236877, 0.04818917403463274, 0.04815867205616087, 0.04847988695837557, 0.048324229079298675, 0.04832670500036329, 0.048194619943387806, 0.05225338996388018, 0.048430958064273, 0.051732346997596323, 0.04912716511171311, 0.05177642998751253, 0.04930500499904156, 0.04871988599188626, 0.049124004086479545, 0.047767264069989324, 0.0479331340175122, 0.05372519791126251, 0.05322086100932211, 0.05320592294447124, 0.05312361603137106, 0.05193437694106251, 0.047587974928319454, 0.04753601795528084, 0.04571136797312647, 0.04685731395147741, 0.04701671004295349, 0.0504841529764235, 0.05070089700166136, 0.05424257495906204, 0.05044817691668868, 0.05047270900104195, 0.047409993945620954, 0.04717209795489907, 0.04725920397322625, 0.05080418905708939, 0.047456349013373256, 0.047319949022494256, 0.05053976993076503, 0.04730846895836294, 0.05008306202944368, 0.050264724995940924, 0.05049548007082194, 0.05078498797956854, 0.05355830304324627, 0.05065505800303072, 0.04778049804735929, 0.047645945101976395, 0.0473485360853374, 0.04742443689610809, 0.05185207596514374, 0.04764196393080056, 0.04799645801540464, 0.04766341403592378, 0.051274716039188206, 0.047679720097221434, 0.04745074000675231, 0.04733020404819399, 0.0479344540508464, 0.0479714839020744, 0.048080409062094986, 0.047148940968327224, 0.047501410939730704, 0.0476705749752, 0.047117594978772104, 0.050434182048775256, 0.050861166906543076, 0.04763863096013665, 0.04747937305364758, 0.04781302798073739, 0.047992025036364794, 0.04742154397536069, 0.05185983399860561, 0.048035344923846424, 0.04808577103540301, 0.04808476194739342, 0.048557470086961985, 0.049155042972415686, 0.04686610703356564, 0.04647403210401535, 0.04696266900282353, 0.0473943951074034, 0.0472661629319191, 0.05993368092458695, 0.04999486403539777, 0.047554571996442974, 0.04769750696141273, 0.04753842996433377, 0.04761332902126014, 0.04706378700211644, 0.06663567596115172, 0.050083913025446236, 0.05082681297790259, 0.04772041703108698, 0.04722999699879438, 0.04471017001196742, 0.04518367501441389, 0.04528224701061845, 0.04838208598084748, 0.047129768994636834, 0.05108322901651263, 0.04693022300489247, 0.04717294895090163, 0.047057429095730186, 0.047561461105942726, 0.0484802071005106, 0.04858155094552785, 0.04850308201275766, 0.04830153600778431, 0.05006090400274843, 0.04802280990406871, 0.04912369605153799, 0.048674780991859734, 0.04836007591802627, 0.5043647269485518, 0.31199599406681955, 0.53512189805042, 0.3844317140756175, 0.1893289100844413, 0.5408303570002317, 0.04699280497152358, 0.04747244296595454, 0.04683519201353192, 0.04825452796649188, 0.047800478991121054, 0.048341943067498505, 0.04830304300412536, 0.04856859601568431, 0.04507982300128788, 0.04519196902401745, 0.044862849987111986, 0.0446440790547058, 0.04470956709701568, 0.044283594004809856, 0.04513174400199205, 0.04504974698647857, 0.0449534859508276, 0.04481090698391199, 0.04472518397960812, 0.04529593698680401, 0.046129439026117325, 0.04547084600199014, 0.045682250056415796, 0.04553955502342433, 0.04513059905730188, 0.04527083190623671, 0.04528978106100112, 0.044968863017857075, 0.04503482102882117, 0.04451776994392276, 0.04391819902230054, 0.04396993305999786, 0.04422455909661949, 0.043989983038045466, 0.0436852709390223, 0.044165999977849424, 0.04439328704029322, 0.047898468910716474, 0.047548249014653265, 0.04466174799017608, 0.044467192026786506, 0.04463741404470056, 0.047523692017421126, 0.04256527009420097, 0.043361978023312986, 0.04369430907536298, 0.04372978303581476, 0.044660152052529156, 0.04368079802952707, 0.04434874397702515, 0.04381259810179472, 0.043717916938476264, 0.0435058269649744, 0.04367420496419072, 0.047094768029637635, 0.04794994799885899, 0.04808035690803081, 0.0434113759547472, 0.043422317947261035, 0.04382808902300894, 0.04424841795116663, 0.05191129201557487, 0.0462307829875499, 0.044702135026454926, 0.04508133698254824, 0.04227287392131984, 0.04208106698933989, 0.0423580160131678, 0.04251492198091, 0.042162353987805545, 0.04272718797437847, 0.046343364054337144, 0.04176651895977557, 0.6576429030392319, 0.04779797699302435, 0.0515244510024786, 0.047035519033670425, 0.04690041905269027, 0.04392869200091809, 0.044320544926449656, 0.04359396698419005, 0.04333988297730684, 0.04371103295125067, 0.044062771019525826, 0.04336882394272834, 0.04331486998125911, 0.04345804394688457, 0.043265062966383994, 0.054241789039224386, 0.04340792098082602, 0.04432625800836831, 0.0432959710014984, 0.04314508498646319, 0.047743991017341614, 0.042906304937787354, 0.04295323498081416, 0.04340669699013233, 0.04308986396063119, 0.04303727101068944, 0.04355022800154984, 0.04332578799221665, 0.04271421593148261, 0.04323577496688813, 0.04272532893810421, 0.04294779000338167, 0.04317364899907261, 0.04458906897343695, 0.04300063906703144, 0.04354465601500124, 0.04310675198212266, 0.04691933502908796, 0.04203646501991898, 0.04291673796251416, 0.04289576201699674, 0.05212106602266431, 0.04376827401574701, 0.04680967307649553, 0.043663550983183086, 0.042904770001769066, 0.04321318003349006, 0.04742106900084764, 0.04286853305529803, 0.043153731035999954, 0.0427493309834972, 0.04349566996097565, 0.04769311205018312, 0.04299431701656431, 0.04306647798512131, 0.04326884099282324, 0.04279107600450516, 0.047466331045143306, 0.046833685017190874, 0.04409828898496926, 0.044097632984630764, 0.04412008903454989, 0.044082163949497044, 0.04843674995936453, 0.04782661399804056, 0.044013298000209033, 0.04375363199505955, 0.0442395139252767, 0.04834434692747891, 0.04389990295749158, 0.0436203139834106, 0.04384078294970095, 0.04377736698370427, 0.042775308014824986, 0.04355021007359028, 0.04345924290828407, 0.043376769055612385, 0.04335637402255088, 0.04302609502337873, 0.04363988898694515, 0.043158016982488334, 0.04350699100177735, 0.04367565596476197, 0.04366384702734649, 0.04297893005423248, 0.04338212090078741, 0.04324844595976174, 0.04326712095644325, 0.04381013894453645, 0.04363517602905631, 0.04347983200568706, 0.04360306996386498, 0.04389178298879415, 0.04371834103949368, 0.04757006908766925, 0.04709889297373593, 0.043648085091263056, 0.0440602779854089, 0.04383549699559808, 0.04814132698811591, 0.043918305076658726, 0.04356887494213879, 0.04364400403574109, 0.04360821400769055, 0.044018571032211185, 0.04383538803085685, 0.044086670968681574, 0.043732505990192294, 0.04403917596209794, 0.04416410205885768, 0.044032438076101243, 0.04418991191778332, 0.04445486003533006, 0.0443721329793334, 0.04443412600085139, 0.043938903021626174, 0.047396039008162916, 0.047488564043305814, 0.04747299896553159, 0.04769748402759433, 0.047418665955774486, 0.04691225802525878, 0.047241947962902486, 0.04706756304949522, 0.04689494601916522, 0.047090195934288204, 0.04766884702257812, 0.04769286094233394, 0.04704541410319507, 0.04719901003409177, 0.047037444077432156, 0.04388497897889465, 0.04482981702312827, 0.04274591093417257, 0.04253864393103868, 0.04380778595805168, 0.04324651893693954, 0.04325452703051269, 0.04322502599097788, 0.043097002897411585, 0.04282144992612302, 0.04302166402339935, 0.04290246998425573, 0.04312368098180741, 0.043454178026877344, 0.04359734302852303, 0.04351447196677327, 0.04370084498077631, 0.04380259697791189, 0.23588998604100198, 0.3049249129835516, 0.3798565589822829, 0.2644270040327683, 0.5096841939957812, 0.2369533630553633, 0.046184338978491724, 0.042630870011635125, 0.042527683079242706, 0.04269884491804987, 0.04264756396878511, 0.04260878101922572, 0.04477180098183453, 0.04338076198473573, 0.04355732095427811, 0.04331964498851448, 0.04323188902344555, 0.044010398094542325, 0.0446137860417366, 0.04491772595793009, 0.045073846937157214, 0.04511946497950703, 0.0451625999994576, 0.04506826004944742, 0.04529218305833638, 0.04422125092241913, 0.04400835500564426, 0.044248309917747974, 0.044686137000098825, 0.04399381799157709, 0.045049073989503086, 0.04419255303218961, 0.04545252793468535, 0.04528161301277578, 0.0450852639041841, 0.04478891100734472, 0.04409380699507892, 0.0439573290059343, 0.044114393065683544, 0.044113322044722736, 0.04467873997054994, 0.04412516695447266, 0.04379744699690491, 0.043620951008051634, 0.045055684982798994, 0.04417247697710991, 0.04412271489854902, 0.043997036991640925, 0.04488459101412445, 0.04467975499574095, 0.04469357000198215, 0.04469676502048969, 0.044212565990164876, 0.04357716906815767, 0.043524842942133546, 0.050904177012853324, 0.04377690306864679, 0.04394918994512409, 0.04366998502518982, 0.04383573401719332, 0.043788678012788296, 0.04400517791509628, 0.04376557900104672, 0.04712014505639672, 0.04378853889647871, 0.04410921304952353, 0.045232721022330225, 0.04393261205404997, 0.04458850994706154, 0.043818466016091406, 0.044133962015621364, 0.043851842056028545, 0.8103329390287399, 0.8357664090581238, 0.04836166906170547, 0.04829722398426384, 0.04720782698132098, 0.05241583602037281, 0.04968568298500031, 0.04744739702437073, 0.0475711360340938, 0.0442400979809463, 0.044472850975580513, 0.04457342403475195, 0.04466208606027067, 0.04470539093017578, 0.048075426020659506, 0.048248249921016395, 0.04442642198409885, 0.04364440799690783, 0.04383741598576307, 0.04494274302851409, 0.04419676901306957, 0.044575197040103376, 0.04767866397742182, 0.04732333996798843, 0.046961041050963104, 0.04752481204923242, 0.04381318110972643]
[0.0011186511361632836, 0.0011231067262335935, 0.00111911631856029, 0.0011306223406625743, 0.0011232348862739111, 0.001119399522642859, 0.0011285663878714497, 0.0011278297937348145, 0.001123147069434212, 0.0011585677898120741, 0.0011442376972111159, 0.0011375997442925393, 0.0011493478371047002, 0.0011553293483894925, 0.0011546940699782829, 0.0011509050003274581, 0.0011579881877053617, 0.0011523839297466153, 0.0011445293717389537, 0.0011528972563398785, 0.0011942563740926426, 0.0011512927202031362, 0.0011389545343686329, 0.0011526962558110786, 0.001143779651651722, 0.001143500116805351, 0.001154731092757957, 0.0011463144193078543, 0.0011393451158953614, 0.0011400422812331207, 0.001259036232219186, 0.0012611527439899916, 0.0011552693484740894, 0.0011332155324432047, 0.0011398923003959449, 0.0011363412323949296, 0.0011363663970557756, 0.0011316629283693294, 0.0011359813032931713, 0.0011332244179103263, 0.001140553092713966, 0.0011866300469697561, 0.0012361884420347769, 0.0011457320448920825, 0.0011365107222741774, 0.001139037348979781, 0.0011338040443830365, 0.00113311362842661, 0.0011360431631463905, 0.001140217510658468, 0.0011415119759391906, 0.0011400872798160064, 0.001129818349253646, 0.001131035604102667, 0.0011377623710824654, 0.001150887280832543, 0.001125576023315621, 0.0011257935331614559, 0.0011298498382445337, 0.0011293696033833332, 0.0011355092554071614, 0.0011238369543841759, 0.0011338303028064411, 0.0011670317928588322, 0.0011278373259661157, 0.001134352720624139, 0.0011309818364679813, 0.0011257400931140712, 0.0011357141626175753, 0.0011400407001506104, 0.0011316691850160444, 0.0011340382774252184, 0.0011390833492759009, 0.0011291762808511077, 0.001128523534726958, 0.001132630001277078, 0.001133063093347605, 0.0011284241837254444, 0.001340560138572094, 0.0011491986034914505, 0.0011341605595377988, 0.00113943483691316, 0.001130134487243066, 0.0011356612315488069, 0.0011346912563800118, 0.0011350547916494137, 0.001137428627306119, 0.0011344506746417907, 0.0011384198604541462, 0.0011307660484859764, 0.0014126705801720883, 0.0011392311615392912, 0.0011410938608351834, 0.0011364390239725973, 0.001210806929224799, 0.0011225517671338695, 0.0011414901169320179, 0.0011287904182059128, 0.0012065363008269044, 0.0011477430464786498, 0.0011357734180163853, 0.0011438141647336442, 0.0011436795561336154, 0.0011363260713297613, 0.001224038605361657, 0.0011711671872627596, 0.001181821604151019, 0.001183950160846634, 0.0011747494193746946, 0.0012663578138101934, 0.0011952226754104675, 0.0011864029748235331, 0.0011835237206934497, 0.0011867137441714836, 0.0011965273256781836, 0.0011827616957774342, 0.0011932314887946082, 0.0012035089988954538, 0.0012865163721577373, 0.03795355034797171, 0.027651477559079783, 0.005072016837508526, 0.001173340558442612, 0.001120301513109616, 0.0011026253030353853, 0.0010924869065367898, 0.0010942091845885613, 0.001188224931464119, 0.0010925083486146706, 0.0011035570939786213, 0.0011023182779290649, 0.0010965112786263573, 0.0011011976044798314, 0.001168659162642651, 0.0010990039770339812, 0.0011768946532419947, 0.0010922622087216655, 0.0010909663004324186, 0.0011542818379090276, 0.0010863771624213389, 0.001140057650762935, 0.0011420255109971, 0.001133125443228109, 0.0011386516054572408, 0.0011591505134738115, 0.0011386277213649347, 0.0011299532773189765, 0.0011296327696939887, 0.0011364552571416595, 0.001139766582677704, 0.0011275302792011306, 0.0011301895571049563, 0.0011379928382243528, 0.0011356206053088223, 0.0011443420242908042, 0.0011264497663305943, 0.0011287611411061398, 0.0011329341403663504, 0.0011483462808956934, 0.0011275868624622046, 0.0012175231619716384, 0.0012265634181541066, 0.0011542839306774874, 0.0011078989766754729, 0.00111213804569182, 0.0011107980024589355, 0.0011103299776698615, 0.001116110044423231, 0.0011112466752312559, 0.0011040024881705988, 0.001270764558792634, 0.0011203825571249391, 0.0011188744886799953, 0.001209433975651167, 0.001121720510296697, 0.0011315609756207398, 0.0011273263033132913, 0.001321077606705732, 0.0011303661862214983, 0.0012084603943147284, 0.0011890221408806568, 0.0011343555795678565, 0.0011415733241064603, 0.0011377119280527844, 0.001143534535130616, 0.0012139836515754807, 0.028193929790367566, 0.001150372278400112, 0.0011540537450982387, 0.0011346871168677543, 0.0011446817675172243, 0.0011341184875819572, 0.0011327801851642339, 0.001124615929912516, 0.0011443689784988067, 0.0011331746273479143, 0.0011191162318641016, 0.0011121349078927968, 0.00112945381227101, 0.0011932666976611282, 0.0011174252559972364, 0.0012010989066343321, 0.001196653395349723, 0.001124130372434508, 0.0011214893500775445, 0.0011364984661773887, 0.0011086562800026217, 0.001114344511311068, 0.0011988181166004302, 0.001076875536074472, 0.0012156867427578152, 0.0011983984177065797, 0.0011955504197367402, 0.0011988942116109092, 0.0010870536978811371, 0.001157949535111182, 0.0011799212106568522, 0.001083707093087913, 0.0010905578369707909, 0.001099741931051709, 0.0010905701634582393, 0.0011587025121200917, 0.0011996712324447757, 0.001217689603393854, 0.0012928991860081983, 0.0011245509782849357, 0.001131984862758843, 0.0011311857663311583, 0.001119465372243593, 0.0012412107452143763, 0.0011169702101628794, 0.0011044099066016633, 0.0011117495596408844, 0.0011129603954032063, 0.001177462094008576, 0.0011206784659216916, 0.0011199691175851365, 0.0011274392315901297, 0.0011238192809139226, 0.0011238768604735648, 0.001120805114962507, 0.001215195115439074, 0.0011263013503319303, 0.001203077837153403, 0.001142492211900305, 0.0012041030229654076, 0.0011466280232335245, 0.0011330206044624712, 0.0011424186996855708, 0.0011108666062788215, 0.0011147240469188884, 0.001249423207238663, 0.0012376944420772584, 0.0012373470452202613, 0.0012354329309621176, 0.0012077762079316862, 0.0011066970913562663, 0.001105488789657694, 0.001063055069142476, 0.0010897049756157537, 0.0010934118614640347, 0.0011740500692191513, 0.001179090627945613, 0.001261455231606094, 0.0011732134166671786, 0.0011737839302567894, 0.001102557998735371, 0.0010970255338348621, 0.0010990512551913082, 0.0011814927687695206, 0.00110363602356682, 0.0011004639307556803, 0.0011753434867619774, 0.0011001969525200684, 0.00116472237277776, 0.0011689470929288587, 0.0011743134900191149, 0.0011810462320829894, 0.0012455419312382852, 0.0011780246047216447, 0.001111174373194402, 0.0011080452349296836, 0.0011011287461706373, 0.0011028938813048394, 0.0012058622317475288, 0.0011079526495535014, 0.001116196698032666, 0.0011084514892075298, 0.0011924352567253072, 0.001108830699935382, 0.0011035055815523795, 0.0011007024197254417, 0.001114754745368521, 0.0011156159046994045, 0.0011181490479556973, 0.0010964869992634238, 0.0011046839753425745, 0.0011086180226790698, 0.0010957580227621418, 0.0011728879546226804, 0.0011828178350358856, 0.0011078751386078291, 0.001104171466363897, 0.0011119308832729625, 0.0011160936054968556, 0.0011028266040781557, 0.001206042651130363, 0.0011171010447406145, 0.0011182737450093725, 0.0011182502778463584, 0.0011292434903944647, 0.0011431405342422253, 0.0010899094658968753, 0.0010807914442794268, 0.0010921550930889194, 0.001102195235055893, 0.001099213091439979, 0.0013938065331299291, 0.0011626712566371574, 0.001105920278987046, 0.001109244347939831, 0.001105544882891483, 0.0011072867214246545, 0.0010945066744678241, 0.0015496668828174818, 0.0011647421633824706, 0.001182018906462851, 0.0011097771402578368, 0.0010983720232277763, 0.0010397713956271494, 0.0010507831398700906, 0.0010530755118748477, 0.001125164790252267, 0.0010960411394101588, 0.0011879820701514567, 0.0010914005349974992, 0.0010970453244395727, 0.0010943588161797717, 0.0011060804908358773, 0.0011274466767560603, 0.001129803510361113, 0.0011279786514594805, 0.0011232915350647514, 0.0011642070698313588, 0.0011168095326527607, 0.001142411536082279, 0.0011319716509734822, 0.0011246529283261922, 0.011729412254617485, 0.007255720792251618, 0.012444695303498138, 0.008940272420363198, 0.004402997908940495, 0.012577450162796088, 0.001092855929570316, 0.0011040103015338267, 0.001089190511942603, 0.0011221983248021366, 0.0011116390463051407, 0.0011242312341278723, 0.0011233265814912874, 0.001129502232922891, 0.0010483679767741368, 0.0010509760238143595, 0.0010433220927235345, 0.001038234396621065, 0.001039757374349202, 0.0010298510233676711, 0.0010495754419067918, 0.001047668534569269, 0.0010454299058332, 0.00104211411590493, 0.001040120557665305, 0.0010533938834140467, 0.0010727776517701704, 0.0010574615349300033, 0.0010623779082887393, 0.001059059419149403, 0.0010495488152860901, 0.0010528100443310863, 0.0010532507223488634, 0.0010457875120431879, 0.001047321419274911, 0.0010352969754400642, 0.0010213534656348963, 0.0010225565827906478, 0.0010284781185260347, 0.0010230228613498946, 0.001015936533465635, 0.0010271162785546377, 0.0010324020241928656, 0.001113917881644569, 0.001105773232898913, 0.001038645302097118, 0.0010341207448089885, 0.0010380793963883851, 0.0011052021399400262, 0.0009898900021907203, 0.0010084180935654183, 0.00101614672268286, 0.00101697169850732, 0.00103860818726812, 0.001015832512314583, 0.0010313661390005848, 0.0010188976302742958, 0.001016695742755262, 0.0010117634177901024, 0.0010156791852137378, 0.001095227163479945, 0.0011151150697409067, 0.001118147835070484, 0.0010095668826685395, 0.0010098213476107218, 0.001019257884256022, 0.0010290329756085263, 0.0012072393491994156, 0.0010751344880825559, 0.0010395845354989517, 0.0010484031856406568, 0.0009830900911934847, 0.0009786294648683695, 0.0009850701398411116, 0.0009887191158351163, 0.0009805198601815243, 0.0009936555342878713, 0.0010777526524264453, 0.0009713143944133852, 0.01529402100091237, 0.001111580860302892, 0.0011982430465692698, 0.0010938492798528007, 0.0010907074198300063, 0.001021597488393444, 0.0010307103471267362, 0.0010138131856788383, 0.0010079042552862056, 0.0010165356500290853, 0.0010247156051052518, 0.0010085773009936823, 0.0010073225577037002, 0.001010652184811269, 0.0010061642550321858, 0.0012614369544005672, 0.0010094865344378145, 0.0010308432094969374, 0.0010068830465464744, 0.0010033740694526322, 0.0011103253724963167, 0.0009978210450648222, 0.0009989124414142828, 0.0010094580695379613, 0.0010020898595495627, 0.001000866767690452, 0.0010127960000360428, 0.0010075764649352709, 0.0009933538588716886, 0.0010054831387648403, 0.0009936123008861444, 0.000998785814032132, 0.001004038348815642, 0.0010369550924055105, 0.0010000148620239872, 0.0010126664189535172, 0.0010024826042354107, 0.0010911473262578595, 0.0009775922097655576, 0.000998063673546841, 0.0009975758608603894, 0.0012121178144805652, 0.0010178668375755119, 0.0010885970482905937, 0.0010154314182135602, 0.0009977853488783504, 0.0010049576751974432, 0.0011028155581592474, 0.0009969426291929775, 0.001003575140372092, 0.000994170487988307, 0.0010115272083947825, 0.001109142140701933, 0.000999867837594519, 0.001001545999653984, 0.0010062521161121685, 0.0009951413024303524, 0.001103868163840542, 0.0010891554655160667, 0.0010255416043016107, 0.0010255263484797853, 0.0010260485821988346, 0.0010251666034766754, 0.001126436045566617, 0.0011122468371637338, 0.001023565069772303, 0.0010175263254665011, 0.0010288259052389928, 0.0011242871378483467, 0.001020927975755618, 0.001014425906590944, 0.0010195530918535106, 0.001018078301946611, 0.00099477460499593, 0.0010127955831067507, 0.0010106800676345131, 0.001008762071060753, 0.0010082877679662996, 0.0010006068610088077, 0.0010148811392312826, 0.0010036748135462403, 0.0010117904884134267, 0.001015712929413069, 0.0010154383029615463, 0.0009995100012612204, 0.0010088865325764515, 0.0010057778130177148, 0.0010062121152661222, 0.001018840440570615, 0.001014771535559449, 0.0010111588838531873, 0.001014024882880581, 0.0010207391392742825, 0.0010167056055696204, 0.0011062806764574244, 0.0010953230924124634, 0.0010150717463084432, 0.0010246576275676489, 0.0010194301626883274, 0.0011195657439096722, 0.0010213559320153191, 0.0010132296498171812, 0.0010149768380404904, 0.001014144511806757, 0.001023687698423516, 0.0010194276286245778, 0.0010252714178763157, 0.0010170350230277278, 0.001024166882839487, 0.001027072140903667, 0.001024010187816308, 0.0010276723701810075, 0.0010338339543100013, 0.0010319100692868233, 0.0010333517674616603, 0.0010218349539913064, 0.0011022334653061144, 0.0011043852103094374, 0.0011040232317565486, 0.001109243814595217, 0.0011027596733901043, 0.00109098274477346, 0.0010986499526256391, 0.0010945944895231446, 0.0010905801399805864, 0.001095120835681121, 0.0011085778377343748, 0.0011091363009845103, 0.0010940793977487226, 0.001097651396141669, 0.0010938940483123758, 0.001020580906485922, 0.001042553884258797, 0.0009940909519575018, 0.0009892707890939228, 0.0010187857199546902, 0.0010057329985334775, 0.001005919233267737, 0.001005233162580881, 0.0010022558813351532, 0.0009958476727005355, 0.0010005038144976594, 0.0009977318600989705, 0.001002876301902498, 0.001010562279694822, 0.0010138916983377449, 0.0010119644643435644, 0.00101629872048317, 0.0010186650459979509, 0.005485813628860511, 0.0070912770461291075, 0.008833873464704253, 0.006149465210064379, 0.011853120790599563, 0.005510543326868914, 0.0010740543948486447, 0.0009914155816659331, 0.000989015885563784, 0.0009929963934430202, 0.0009918038132275607, 0.00099090188416804, 0.0010412046739961519, 0.0010088549298775751, 0.0010129609524250724, 0.0010074336043840578, 0.0010053927679871057, 0.0010234976301056354, 0.0010375299079473628, 0.0010445982780913974, 0.0010482289985385398, 0.0010492898832443495, 0.0010502930232432, 0.0010480990709173818, 0.0010533065827520087, 0.0010284011842423054, 0.0010234501164103316, 0.0010290304632034413, 0.0010392124883743914, 0.0010231120463157462, 0.0010476528834768158, 0.00102773379144627, 0.0010570355333647755, 0.0010530607677389715, 0.0010484945093996304, 0.0010416025815661563, 0.0010254373719785796, 0.001022263465254286, 0.001025916117806594, 0.0010258912103423893, 0.001039040464431394, 0.0010261666733598293, 0.0010185452789977887, 0.0010144407211174799, 0.0010478066275069534, 0.0010272669064444165, 0.0010261096488034657, 0.001023186906782347, 0.0010438276980028942, 0.0010390640696683942, 0.0010393853488833058, 0.0010394596516392952, 0.0010281992090736018, 0.0010134225364687832, 0.0010122056498170593, 0.0011838180700663565, 0.001018067513224344, 0.0010220741847703277, 0.0010155810470974377, 0.0010194356748184493, 0.0010183413491346115, 0.0010233762305836345, 0.0010178041628150399, 0.001095817326892947, 0.001018338113871598, 0.0010257956523145007, 0.001051923744705354, 0.0010216886524197666, 0.0010369420917921287, 0.0010190340933974745, 0.001026371209665613, 0.0010198102803727568, 0.01884495207043581, 0.019436428117630786, 0.001124689978179197, 0.0011231912554479963, 0.0010978564414260692, 0.0012189729307063443, 0.00115548099965117, 0.0011034278377760635, 0.0011063054891649721, 0.0010288394879289838, 0.0010342523482693143, 0.0010365912566221384, 0.001038653164192341, 0.0010396602541901344, 0.0011180331632711512, 0.0011220523237445673, 0.0010331726042813686, 0.0010149862324862287, 0.0010194747903665832, 0.0010451800704305602, 0.001027831837513246, 0.0010366324893047298, 0.0011088061390098097, 0.0011005427899532193, 0.001092117233743328, 0.0011052281871914518, 0.001018911188598289]
[893.9337454479064, 890.3873306444889, 893.5621645536098, 884.4686364626175, 890.285738290487, 893.3360965163167, 886.0799069925038, 886.6586124564899, 890.3553481235119, 863.1346467539938, 873.944288356632, 879.043798152301, 870.058626045776, 865.5540529581294, 866.0302551123413, 868.8814452239565, 863.5666672745369, 867.7663530242727, 873.7215703609587, 867.3799807406222, 837.3411452459424, 868.5888327545041, 877.9981727315737, 867.5312294619748, 874.2942738628973, 874.5079998712603, 866.002488606765, 872.3610059828096, 877.6971841531473, 877.1604496267938, 794.2583179178197, 792.9253651197193, 865.5990062584337, 882.4446642061172, 877.2758616341624, 880.0173499754299, 879.9978621252011, 883.6553490719633, 880.2961783799028, 882.4377450708369, 876.7676019539631, 842.7226350400068, 808.9381569965105, 872.8044261816829, 879.8861114121149, 877.9343371801508, 881.986622780265, 882.5240248752056, 880.2482444684542, 877.0256469947622, 876.0311070562705, 877.1258286132141, 885.0980342641774, 884.1454648930994, 878.9181514665506, 868.894822850599, 888.4339922720543, 888.2623416673907, 885.0733665225074, 885.4497208037374, 880.6621304389354, 889.8087895213997, 881.9661968151793, 856.8746850934876, 886.6526909307519, 881.5600137581404, 884.1874977612079, 888.3045084001197, 880.5032400892285, 877.1616661298938, 883.650463616558, 881.8044504374701, 877.898883023517, 885.601315718621, 886.1135538852066, 882.9008580670358, 882.5633858089284, 886.191570884756, 745.9568364200029, 870.1716108615508, 881.7093766754921, 877.6280727988775, 884.850441507607, 880.5442787161159, 881.2969998467113, 881.0147381051466, 879.1760432198683, 881.4838955565482, 878.4105361628821, 884.3562303085915, 707.8791149442515, 877.7849779397127, 876.3520989133052, 879.9416237083678, 825.8955047773266, 890.827513953524, 876.0478826463249, 885.9040472627232, 828.8188256869239, 871.2751543719344, 880.4573026074938, 874.2678931877596, 874.3707926201407, 880.0290913239114, 816.9676966230471, 853.8490583374266, 846.1513958516324, 844.6301483543086, 851.2453664648656, 789.6662294768167, 836.664180301446, 842.8839283286028, 844.9344804125096, 842.6631990330241, 835.7519118363692, 845.4788513781687, 838.0603507289194, 830.9036333901711, 777.2928674998564, 26.347996190913438, 36.16443272745238, 197.1602287683295, 852.267479211075, 892.6168431427932, 906.9264030556245, 915.3427780384338, 913.9020345328346, 841.5914979732078, 915.3248130945877, 906.1606376836653, 907.1790063017996, 911.9833233751496, 908.1022297286656, 855.6814783693927, 909.9148145931415, 849.6937234316575, 915.531080371585, 916.618597296394, 866.33954304565, 920.490631238216, 877.1486243092993, 875.6371818059495, 882.5148230289013, 878.2317569371332, 862.7007350435797, 878.2501789094383, 884.9923444380684, 885.2434409023869, 879.9290545895637, 877.3726262886703, 886.8941423981205, 884.8073260927627, 878.740152319704, 880.5757797324032, 873.8646128282679, 887.744868781407, 885.9270252872462, 882.6638410566704, 870.8174673757942, 886.8496373010189, 821.3396108051163, 815.2860139143323, 866.3379723332603, 902.6093723822629, 899.1689510792132, 900.2536894974012, 900.6331632138764, 895.9689996489255, 899.8902064583413, 905.7950599885536, 786.927832603476, 892.5522747927656, 893.7552961635233, 826.8330641708601, 891.4876663309815, 883.7349657197489, 887.0546150310958, 756.9578009074136, 884.669067590144, 827.4991921163139, 841.0272320575491, 881.5577919411825, 875.9840291316603, 878.9571202892449, 874.4816787590755, 823.7343218767587, 35.468627730698586, 869.2838125330669, 866.5107706182913, 881.3002149530425, 873.6052485303106, 881.7420851079592, 882.7838031568472, 889.1924553103122, 873.8440300188919, 882.476518504834, 893.5622337764765, 899.1714880119508, 885.3837041722713, 838.0356226818849, 894.9144424944635, 832.5709019269339, 835.6638638105808, 889.5765335779682, 891.6714188421457, 879.8956001792936, 901.9928160219642, 897.3885453282868, 834.1548948524132, 928.6124222352511, 822.5803283265847, 834.4470296562455, 836.4348198883989, 834.1019502098835, 919.9177574660566, 863.5954933079046, 847.514216176611, 922.7585630639381, 916.9619126094856, 909.3042392624574, 916.951548379943, 863.0342901132476, 833.5617067037014, 821.2273449759891, 773.4555105471766, 889.2438131396323, 883.4040391342575, 884.0280966789028, 893.2835483743775, 805.6649556536706, 895.2790243655446, 905.4609108651163, 899.4831536726837, 898.5045686533322, 849.2842403066918, 892.316601423727, 892.8817628080564, 886.9657645225006, 889.8227828826454, 889.7771946106558, 892.2157711900269, 822.9131168278933, 887.8618494999512, 831.2014145037328, 875.2794895089062, 830.4937209917865, 872.1224143641376, 882.5964824129753, 875.3358118833586, 900.1980925052719, 897.0830070132719, 800.3693177831148, 807.9538584027824, 808.180699071366, 809.4328513820903, 827.9679575014128, 903.5896161744599, 904.5772416286937, 940.6850397756533, 917.6795760108697, 914.5684579103066, 851.7524305118349, 848.1112276690288, 792.7352274934028, 852.3598399008794, 851.9455533704822, 906.9817652649524, 911.5558108335994, 909.875672564455, 846.3868983654141, 906.0958310948562, 908.7076568818638, 850.8151117210497, 908.9281675516723, 858.5737025168395, 855.4707103932712, 851.5613662785416, 846.7069051448719, 802.8633761095671, 848.8787042239156, 899.9487606298927, 902.4902309727995, 908.159017260852, 906.705547062148, 829.2821299750018, 902.5656470093685, 895.8994429588739, 902.1594627609139, 838.6199538801148, 901.8509318494479, 906.202937907418, 908.5107673783793, 897.058302873082, 896.3658511747764, 894.335153107085, 912.0035172982078, 905.2362687617417, 902.0239429117479, 912.6102471777856, 852.5963593186541, 845.4387230047652, 902.6287937615547, 905.6564405644895, 899.3364740949594, 895.9821963632039, 906.7608600500641, 829.1580725298151, 895.1741695239352, 894.2354271150476, 894.2541931900101, 885.5486071039314, 874.7830822593379, 917.5073997335276, 925.247886900797, 915.6208731964258, 907.2802786606943, 909.7417123098406, 717.4596877189275, 860.0883476661681, 904.2243089311444, 901.5146228667044, 904.5313451088145, 903.1084547942402, 913.6536334839843, 645.2999745221883, 858.5591141441545, 846.0101564639639, 901.0818151900912, 910.4383386070858, 961.7498656008316, 951.6711508367284, 949.5995194301362, 888.7587033147345, 912.3745122725558, 841.7635460377861, 916.2539030662023, 911.5393664439995, 913.7770767825831, 904.0933352366507, 886.9599073875887, 885.1096591834588, 886.541601391223, 890.2408402306314, 858.9537255987073, 895.4078298603857, 875.3413007621957, 883.4143497675157, 889.1632029876881, 85.25576374096092, 137.82228239376298, 80.35552302505191, 111.85341486041374, 227.117982038886, 79.50737129199568, 915.0336956063143, 905.788649445279, 918.1130289286775, 891.1080848176437, 899.5725755799907, 889.4967241999417, 890.2130657964459, 885.3457486420642, 953.8635499693852, 951.4964921565484, 958.476780060849, 963.1736371425384, 961.7628349362884, 971.0142314855827, 952.7661948561537, 954.500366292983, 956.5442832850733, 959.5878078396819, 961.4270121192862, 949.3125180858301, 932.1596123389769, 945.6608746210264, 941.284633460407, 944.2340834881226, 952.7903661416797, 949.8389622938679, 949.441551551839, 956.2171937263514, 954.8167177678149, 965.9064246516698, 979.0929718717678, 977.940993026431, 972.310428376592, 977.4952621102567, 984.3134556729926, 973.5996019917084, 968.614916056371, 897.7322444304577, 904.3445529770906, 962.7925895210908, 967.005066883857, 963.3174528645222, 904.8118564575572, 1010.2132537826479, 991.6521791713843, 984.1098511441055, 983.311533121098, 962.8269950676279, 984.4142492756916, 969.5877750737683, 981.4528665954317, 983.5784275932775, 988.3733513356352, 984.5628566165425, 913.052591594448, 896.7684386440466, 894.3361232166261, 990.5237752616728, 990.2741731159086, 981.105974696405, 971.7861562294859, 828.3361544362789, 930.1161957732803, 961.9227353358493, 953.8315160583202, 1017.2007722974671, 1021.8372079513312, 1015.156139197658, 1011.4095944785646, 1019.8671547712142, 1006.3849749668789, 927.8566819099045, 1029.5327710076192, 65.3850285637992, 899.6196639509541, 834.5552288937823, 914.2027319655685, 916.836157725833, 978.8591019077309, 970.2046775679064, 986.3750187174877, 992.1577320020729, 983.7333299342603, 975.8805223789744, 991.4956434323559, 992.730672367357, 989.4600882763061, 993.8735102133113, 792.7467135883921, 990.6026141863324, 970.0796307209616, 993.1640059189766, 996.6372766096367, 900.6368986702746, 1002.1837131477181, 1001.0887426571417, 990.6305473962973, 997.914498854921, 999.1339829451505, 987.3656688656083, 992.4805062454911, 1006.6906078523322, 994.5467620953088, 1006.4287641247585, 1001.2156620076195, 995.9778938519572, 964.3619162718197, 999.9851381968895, 987.4920124569683, 997.5235438251777, 916.4665265043049, 1022.921408344504, 1001.9400830873624, 1002.4300298701293, 825.0023125256473, 982.4467829032828, 918.6135508729183, 984.8030916349737, 1002.2195666875037, 995.0667820946098, 906.7699422640892, 1003.0667469897415, 996.4375957232595, 1005.8636944891504, 988.6041538980693, 901.5976972682139, 1000.1321798746914, 998.4563867715332, 993.7867299734736, 1004.8824197707215, 905.9052817691859, 918.1425716173376, 975.0945215733062, 975.1090271667571, 974.6127204395992, 975.4512062806891, 887.755682123065, 899.0810012551108, 976.9774580354278, 982.7755557494143, 971.9817462874861, 889.4524951284184, 979.5010262696262, 985.7792407535968, 980.8218993108405, 982.2427195314502, 1005.2528431846041, 987.3660753263752, 989.4327908736641, 991.3140359732802, 991.7803545479721, 999.3935070481169, 985.3370619907723, 996.3386412644387, 988.3469072417204, 984.5301472905845, 984.796414596022, 1000.4902389552493, 991.191742292607, 994.2553783321396, 993.8262368621162, 981.5079576542297, 985.4434864973765, 988.9642626580458, 986.1690939568072, 979.6822337105369, 983.5688861376337, 903.9297361698835, 912.9726260016, 985.15203840196, 975.9357399932877, 980.9401728539322, 893.2034634320511, 979.0906075483597, 986.9430885490094, 985.2441578180201, 986.0527650230462, 976.8604248541864, 980.9426112466759, 975.3514850451393, 983.2503083551496, 976.4033740551298, 973.641441700631, 976.5527842379093, 973.0727700929301, 967.2733187288449, 969.0766955022766, 967.7246717798857, 978.631623525875, 907.2488102348427, 905.4811588067266, 905.7780409285019, 901.5150563313423, 906.8158948230302, 916.6047811394558, 910.2080217726511, 913.5803346092539, 916.943160195271, 913.1412419690109, 902.056640464437, 901.6024442734074, 914.0104475577285, 911.0360570897815, 914.1653175120274, 979.8341254915433, 959.1830360988481, 1005.9441724430371, 1010.8455753716373, 981.5606760217195, 994.2996813847838, 994.1155978810463, 994.7940808404637, 997.7491962111034, 1004.1696410136747, 999.4964392035703, 1002.2732960545176, 997.1319474824148, 989.5481160270383, 986.2986368657323, 988.1769916186479, 983.9626675162779, 981.6769544893283, 182.28836552869106, 141.0183234267891, 113.20062529710219, 162.61576671144886, 84.3659672137212, 181.47030894831911, 931.0515415198497, 1008.6587486547689, 1011.1061051663035, 1007.053003015143, 1008.2639193993085, 1009.1816515613943, 960.4259613645336, 991.2227916865613, 987.2048844587313, 992.6212463514132, 994.6361579684897, 977.0418324239693, 963.8276374879529, 957.3058092983997, 953.9900168705677, 953.0254851100367, 952.1152458121638, 954.1082782610603, 949.3911994618576, 972.383166533175, 977.0871916136166, 971.7885288710818, 962.267112055466, 977.4100535723596, 954.514625761663, 973.0146155774038, 946.0419904870946, 949.6128149822745, 953.7484374358829, 960.0590644623762, 975.1936367118177, 978.2214018098084, 974.7385606320304, 974.7622261684569, 962.4264253724145, 974.500562102494, 981.7923862785594, 985.7648447889865, 954.3745704102868, 973.4568433253702, 974.5547185586728, 977.3385423243308, 958.0125167336068, 962.4045611731517, 962.1070771051172, 962.0383036733896, 972.574177431036, 986.755241781426, 987.941531625253, 844.7243924431286, 982.2531286092001, 978.402561086808, 984.6579973681385, 980.9348688705537, 981.9889969604022, 977.1577354593209, 982.5072804125724, 912.560857962864, 981.9921167421705, 974.8530301758464, 950.6392502624817, 978.7717595097104, 964.3740069146171, 981.3214361317249, 974.306362632478, 980.574543369463, 53.0646082973494, 51.44978253966838, 889.1339119238332, 890.3203218058708, 910.8659040165963, 820.362761805171, 865.4404531981846, 906.2667858874032, 903.9094624350004, 971.9689142306963, 966.8820203051691, 964.7004000965862, 962.7853016532252, 961.8526782857265, 894.4278513833983, 891.2240354912786, 967.8924855886572, 985.2350386571062, 980.8972320349576, 956.7729315658034, 972.9217985885875, 964.6620285562349, 901.8709085548749, 908.6425435966073, 915.6526141176364, 904.7905324792231, 981.4398067173008]
Elapsed: 0.06202561699405865~0.10914725854928954
Time per graph: 0.001442107762163928~0.0025383502698696175
Speed: 893.7421494776459~145.66715971154963
Total Time: 0.0455
best val loss: 0.3353176414966583 test_score: 0.8837

Testing...
Test loss: 0.6304 score: 0.7674 time: 0.04s
test Score 0.7674
Epoch Time List: [0.23813166713807732, 0.21787546400446445, 0.21795870806090534, 0.21790761197917163, 0.21779315907042474, 0.2183288859669119, 0.21822236198931932, 0.21912875003181398, 0.2178315658820793, 0.2293252928648144, 0.22305703908205032, 0.22217801690567285, 0.22349613602273166, 0.2270448449999094, 0.22481186187360436, 0.2235944310668856, 0.22430307499598712, 0.22434541897382587, 0.22276867413893342, 0.22341025713831186, 0.22989780188072473, 0.22800555697176605, 0.22295724204741418, 0.22522015182767063, 0.23285500216297805, 0.22203104989603162, 0.22194739500992, 0.22235608496703207, 0.22171535005327314, 0.22188551304861903, 0.22601664392277598, 0.22882700199261308, 0.22964408807456493, 0.22081053198780864, 0.22028645710088313, 0.22075844008941203, 0.22037347592413425, 0.2217377859633416, 0.22000217298045754, 0.21992720500566065, 0.2207843429641798, 0.22330255305860192, 0.2273284998955205, 0.22129533987026662, 0.21990900009404868, 0.2212669770233333, 0.22002193506341428, 0.2223352069268003, 0.2196739410283044, 0.22038937313482165, 0.22090945404488593, 0.22018742794170976, 0.23567817907314748, 0.21976327488664538, 0.21919648395851254, 0.2207454979652539, 0.2194248678861186, 0.2193827579030767, 0.21937898290343583, 0.21941289398819208, 0.219468007911928, 0.21977219393011183, 0.21952972398139536, 0.22578598489053547, 0.21937466086819768, 0.21966476913075894, 0.21939015306998044, 0.21927297196816653, 0.21942798304371536, 0.22052942495793104, 0.21866370178759098, 0.21870325005147606, 0.21974813705310225, 0.2193316709017381, 0.21977342886384577, 0.2198807019740343, 0.21964150213170797, 0.21947308001108468, 0.23327766510192305, 0.22237078903708607, 0.2198719420703128, 0.22098350303713232, 0.22132594289723784, 0.22071762499399483, 0.22128706087823957, 0.22076440684031695, 0.22102576296310872, 0.22103593405336142, 0.2207026258111, 0.22062094695866108, 0.23867329803761095, 0.2267324860440567, 0.2212409790372476, 0.2212268158327788, 0.2269628179492429, 0.22847850993275642, 0.2283724050503224, 0.2202083331067115, 0.22706031589768827, 0.2287506009452045, 0.21961057803127915, 0.22018752014264464, 0.22073196107521653, 0.22045714512933046, 0.22599502187222242, 0.22598708292935044, 0.23347120499238372, 0.22804869711399078, 0.23380189400631934, 0.23066436813678592, 0.23250174487475306, 0.23128900292795151, 0.23132441088091582, 0.2307833330705762, 0.23629935504868627, 0.23236647713929415, 0.23396026494447142, 0.23540832905564457, 0.23542386991903186, 3.4044338408857584, 8.31708962121047, 4.751387019990943, 0.40355133498087525, 0.21851674292702228, 0.2129443408921361, 0.2111917679430917, 0.21072265703696758, 0.21535606298130006, 0.210354253067635, 0.20867011707741767, 0.2122363978996873, 0.21043146192096174, 0.21448558394331485, 0.2113388340221718, 0.21237197588197887, 0.21459599409718066, 0.21143290190957487, 0.20999059290625155, 0.21388627297710627, 0.21065894491039217, 0.22202645288780332, 0.22588130400981754, 0.22123831510543823, 0.22057268302887678, 0.22591909405309707, 0.22108357807155699, 0.22048462100792676, 0.21991980099119246, 0.22037834592629224, 0.2277351119555533, 0.22056486003566533, 0.22051419597119093, 0.22080940392334014, 0.2243153640301898, 0.22081458498723805, 0.21976078604348004, 0.2196500200079754, 0.21980625588912517, 0.22127176995854825, 0.22012361395172775, 0.21999057196080685, 0.235896008904092, 0.230898872949183, 0.21619865915272385, 0.21629684208892286, 0.2163176869507879, 0.21661262400448322, 0.2160312820924446, 0.21648012183140963, 0.215624077944085, 0.22443556098733097, 0.21887521480675787, 0.21709406294394284, 0.2288813820341602, 0.2171285559888929, 0.21736095112282783, 0.22113178484141827, 0.23633675405289978, 0.23611182300373912, 0.2268109960714355, 0.22675222402904183, 0.2323338621063158, 0.2202021380653605, 0.22613018390256912, 0.22236141096800566, 0.23313715495169163, 2.217121317051351, 3.132935312925838, 0.228103480883874, 0.22377683594822884, 0.22629045299254358, 0.22052037599496543, 0.21857453405391425, 0.21798051695805043, 0.22331432602368295, 0.22268705302849412, 0.22306406497955322, 0.2185059170005843, 0.2177193600218743, 0.2172505451599136, 0.2180559280095622, 0.22187124204356223, 0.21801849792245775, 0.21921814803499728, 0.2173702041618526, 0.22194710292387754, 0.2168629610678181, 0.22111894586123526, 0.21732949011493474, 0.21318403189070523, 0.22224833478685468, 0.24037967494223267, 0.23923362896312028, 0.22842865821439773, 0.20995938195846975, 0.2134257589932531, 0.2103546588914469, 0.21010717493481934, 0.20982031500898302, 0.21024163509719074, 0.2089225830277428, 0.21087995497509837, 0.22045292903203517, 0.24328081298153847, 0.24090550490655005, 0.22037320991512388, 0.21730068395845592, 0.21750130399595946, 0.22043247905094177, 0.22018267202656716, 0.2241115040378645, 0.21539489598944783, 0.21600535907782614, 0.21635254099965096, 0.22306914709042758, 0.23036545608192682, 0.2187027158215642, 0.21892246592324227, 0.22034479898866266, 0.21853994892444462, 0.21854416304267943, 0.2224766359431669, 0.22072938701603562, 0.2219292160589248, 0.22218219202477485, 0.22019299410749227, 0.2217483768472448, 0.22407105891034007, 0.22275189694482833, 0.22042198304552585, 0.21847894205711782, 0.2270788410678506, 0.24142572993878275, 0.24728125403635204, 0.24214859085623175, 0.24118583789095283, 0.21248258301056921, 0.21378145506605506, 0.2101878869580105, 0.2118182589765638, 0.2141191988484934, 0.21138517395593226, 0.2206074489513412, 0.23330433294177055, 0.2309353039599955, 0.23037926596589386, 0.22287207806948572, 0.2136638859519735, 0.2149029620923102, 0.21318442199844867, 0.21497709990944713, 0.21841056598350406, 0.21390215889550745, 0.21478096686769277, 0.21945742692332715, 0.23069599992595613, 0.23251527396496385, 0.23222634301055223, 0.24088248808402568, 0.23418286792002618, 0.22355355904437602, 0.21951329102739692, 0.2138344410341233, 0.21333783504087478, 0.21996184904128313, 0.21520148299168795, 0.21476116590201855, 0.2189927069703117, 0.21935578098054975, 0.21648752293549478, 0.21447979903314263, 0.21468854695558548, 0.2165042501874268, 0.22051772009581327, 0.21892137394752353, 0.21791634208057076, 0.21418235590681434, 0.2185620239470154, 0.21395884605590254, 0.21448842098470777, 0.21502134599722922, 0.21514609304722399, 0.2183258100412786, 0.2178922750754282, 0.21400811290368438, 0.21746469591744244, 0.21605725912377238, 0.21692015614826232, 0.2189915479393676, 0.21924916794523597, 0.22342689288780093, 0.21341609803494066, 0.2128630301449448, 0.2090352000668645, 0.20909889496397227, 0.218366821995005, 0.2144517379347235, 0.22375106089748442, 0.2602780129527673, 0.21465725800953805, 0.21800793090369552, 0.21742138906847686, 0.21361728094052523, 0.21367876313161105, 0.23123857111204416, 0.27355366095434874, 0.22875113296322525, 0.23033525294158608, 0.21467701299116015, 0.22130667907185853, 0.2207559699891135, 0.2295322410063818, 0.3226628809934482, 0.24237600597552955, 0.2387346550822258, 0.23725868295878172, 0.23235802596900612, 0.23582891002297401, 0.23584970203228295, 0.23710889113135636, 0.23837643605656922, 0.2389393231132999, 0.23767825006507337, 0.2444382441462949, 0.2374078040011227, 0.23773321404587477, 0.24503223004285246, 0.23975848499685526, 1.03109301708173, 1.4898998910794035, 1.9681345819262788, 1.0827947568614036, 1.0189301070058718, 1.2020840350305662, 0.3711240260163322, 0.23148625495377928, 0.22858458489645272, 0.23697089008055627, 0.23431931692175567, 0.24094150715973228, 0.2349749191198498, 0.23906076583079994, 0.22633909108117223, 0.21856109995860606, 0.21792545705102384, 0.21775663807056844, 0.21780284494161606, 0.2165112610673532, 0.22292053804267198, 0.21895003190729767, 0.21855496405623853, 0.21835313295014203, 0.21858048788271844, 0.21841847395990044, 0.22042005707044154, 0.22219415102154016, 0.22048697900027037, 0.2236613309942186, 0.22322168003302068, 0.22281549195758998, 0.21957020205445588, 0.22269749897532165, 0.21915321401320398, 0.21867073699831963, 0.216497010900639, 0.21985747094731778, 0.2153841519029811, 0.22391041705850512, 0.2148826450575143, 0.21550389006733894, 0.21979790285695344, 0.22172718006186187, 0.2329374448163435, 0.2210281629813835, 0.2193078480195254, 0.22253163205459714, 0.2229912499897182, 0.22633578686509281, 0.2103860720526427, 0.21120315603911877, 0.21578364889137447, 0.21394184709060937, 0.21251235192175955, 0.3091700100339949, 0.21880170598160475, 0.21322765795048326, 0.2117687261197716, 0.2111660490045324, 0.2187767680734396, 0.238265132997185, 0.24474668805487454, 0.2276734570041299, 0.21540221897885203, 0.21548830112442374, 0.22281706996727735, 0.22162997908890247, 0.2357579239178449, 0.21947914408519864, 0.2207146380096674, 0.2101655580336228, 0.20525472704321146, 0.22165226703509688, 0.2059734829235822, 0.20651854202151299, 0.20764629589393735, 0.21124515403062105, 0.20606708119157702, 4.99014465813525, 0.36490854306612164, 0.23916988202836365, 0.24389565689489245, 0.2375390090746805, 0.2201559579698369, 0.21297540492378175, 0.21484950406011194, 0.21331393998116255, 0.21390067099127918, 0.2124004999641329, 0.21118358301464468, 0.21688641293440014, 0.21336889697704464, 0.21565952501259744, 0.24818172305822372, 0.21244354103691876, 0.21134282310958952, 0.2111507608788088, 0.21007196803111583, 0.21848248410969973, 0.21956960204988718, 0.20947371190413833, 0.20960930897854269, 0.20997577300295234, 0.20977347891312093, 0.2158303561154753, 0.2104398509254679, 0.2098525109468028, 0.21031852590385824, 0.20867985009681433, 0.21114003600087017, 0.21468803100287914, 0.224189389962703, 0.21042157290503383, 0.21056260797195137, 0.21407751191873103, 0.21910855697933584, 0.22086514392867684, 0.20580307906493545, 0.2101211779518053, 0.2449673799565062, 0.21328107302542776, 0.217510157963261, 0.22143516992218792, 0.21063632296863943, 0.21087079006247222, 0.2200819590361789, 0.2233155780704692, 0.21090475900564343, 0.21101147285662591, 0.21702918387018144, 0.21949711604975164, 0.22463190893176943, 0.21094182401429862, 0.2104383910773322, 0.21000323200132698, 0.2195428799604997, 0.233929565991275, 0.22486091405153275, 0.21383054496254772, 0.2148630479350686, 0.21416192303877324, 0.22243047202937305, 0.23265086207538843, 0.21564615983515978, 0.21317085705231875, 0.2138959039002657, 0.22336426307447255, 0.23816067702136934, 0.2136962249642238, 0.2142145250691101, 0.21476143796462566, 0.21680769114755094, 0.21280476299580187, 0.2118574019987136, 0.21348096197471023, 0.21134692209307104, 0.2108525630319491, 0.21551588200964034, 0.21209853992331773, 0.21201552893035114, 0.21271930704824626, 0.21291535801719874, 0.21679916605353355, 0.21621321397833526, 0.21013869903981686, 0.21117740485351533, 0.21105345897376537, 0.21151477098464966, 0.21092687698546797, 0.21735558297950774, 0.21937664388678968, 0.2128522398415953, 0.303347724955529, 0.2306754799792543, 0.23098152596503496, 0.21716601389925927, 0.21483577089384198, 0.21974349103402346, 0.21385948185343295, 0.27697814197745174, 0.2137796930037439, 0.21189729496836662, 0.2134935868671164, 0.21462494402658194, 0.21764940104912966, 0.2178536750143394, 0.21329372783657163, 0.3208164859097451, 0.2143855100730434, 0.21546374692115933, 0.21686399902682751, 0.21600329782813787, 0.21699105389416218, 0.21811519097536802, 0.2976167050655931, 0.23019014799501747, 0.2301726599689573, 0.23134513292461634, 0.23151588707696646, 0.23062413500156254, 0.22937289415858686, 0.22923348797485232, 0.22819489310495555, 0.22897321113850921, 0.2353049429366365, 0.23122451803646982, 0.22934813401661813, 0.2298288248712197, 0.2286972759757191, 0.22580526897218078, 0.2137769308174029, 0.21336808311752975, 0.21152408292982727, 0.2184690278954804, 0.2104216261068359, 0.2138146918732673, 0.21377362811472267, 0.21131689520552754, 0.20827298192307353, 0.20960881398059428, 0.20785624580457807, 0.21261704503558576, 0.21648777986411005, 0.2172713140025735, 0.21334436687175184, 0.2134103279095143, 0.21281059295870364, 0.40900358301587403, 2.441142024239525, 1.5375146459555253, 1.383371974923648, 2.3886127308942378, 1.0657982982229441, 0.3197955770883709, 0.21917809813749045, 0.21034504403360188, 0.207776892115362, 0.21463261207100004, 0.20819341810420156, 0.2109066309640184, 0.2134041889803484, 0.2114347490714863, 0.2153914381051436, 0.21309648593887687, 0.2122280071489513, 0.21551466814707965, 0.22179332515224814, 0.2246952928835526, 0.22257591085508466, 0.22390675300266594, 0.22371787205338478, 0.2243268609745428, 0.21822697506286204, 0.2149178320541978, 0.2147020819829777, 0.22035869990941137, 0.21594411612022668, 0.22487066395115107, 0.21596885693725199, 0.21617471205536276, 0.22354266594629735, 0.22312651097308844, 0.22389230295084417, 0.21787858114112169, 0.21498324908316135, 0.21337449888233095, 0.21514605905395, 0.2177093579666689, 0.21532352222129703, 0.2139489189721644, 0.21729564003180712, 0.21515049098525196, 0.21489723504055291, 0.21436431107576936, 0.21626950800418854, 0.21905919595155865, 0.22076938208192587, 0.22006195003632456, 0.21679246006533504, 0.2192068659933284, 0.21234016004018486, 0.2116652288241312, 0.2205547090852633, 0.2145948311081156, 0.2121835278812796, 0.21140847797505558, 0.21154179680161178, 0.21235437807627022, 0.21248580887913704, 0.21180982305668294, 0.21882907999679446, 0.23004629893694073, 0.2118560599628836, 0.21758885798044503, 0.21499614091590047, 0.21503402304369956, 0.21384765394032001, 0.2125755388988182, 0.21283690899144858, 5.992469344986603, 2.0332577930530533, 0.8420419180765748, 0.2368602561764419, 0.23438705201260746, 0.2426069510402158, 0.2511734188301489, 0.23031989904120564, 0.22988041199278086, 0.22687037894502282, 0.21532054501585662, 0.22003314190078527, 0.21679030708037317, 0.21614608692470938, 0.2192716260906309, 0.23502067697700113, 0.22821721411310136, 0.21362941607367247, 0.21636170195415616, 0.2143911289749667, 0.2151376138208434, 0.21504062600433826, 0.22216088406275958, 0.23279811104293913, 0.23440376296639442, 0.23176815500482917, 0.21473398804664612]
Total Epoch List: [9, 320, 346]
Total Time List: [0.04973241395782679, 0.04826219903770834, 0.04552458901889622]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2adbbeb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.3145;  Loss pred: 2.3145; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.4037;  Loss pred: 2.4037; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.3370;  Loss pred: 2.3370; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.3400;  Loss pred: 2.3400; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.3379;  Loss pred: 2.3379; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.2965;  Loss pred: 2.2965; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.2687;  Loss pred: 2.2687; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.2490;  Loss pred: 2.2490; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 2.1885;  Loss pred: 2.1885; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.1800;  Loss pred: 2.1800; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 11/1000, LR 0.000270
Train loss: 2.1472;  Loss pred: 2.1472; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 2.1885,   Val_Loss: 0.6927,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.6927,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6931


[0.0463027679361403, 0.046259362949058414, 0.04596019291784614, 0.04561654897406697, 0.04588835802860558, 0.04604867403395474, 0.04535726201720536, 0.046053358004428446, 0.04566571593750268, 0.04678334796335548, 0.0465407760348171]
[0.0010523356349122796, 0.0010513491579331458, 0.0010445498390419577, 0.001036739749410613, 0.0010429172279228542, 0.0010465607734989715, 0.0010308468640273945, 0.0010466672273733739, 0.0010378571803977882, 0.0010632579082580792, 0.001057744909882207]
[950.2671646041501, 951.1587967273466, 957.3502025687753, 964.5622255426209, 958.8488647288616, 955.510683490167, 970.0761916208587, 955.4135009171101, 963.5237091260684, 940.5055840480757, 945.4075275213213]
Elapsed: 0.046043305890634656~0.0003972105375175199
Time per graph: 0.0010464387702416968~9.027512216307287e-06
Speed: 955.6931318995779~8.235862631911514
Total Time: 0.0468
best val loss: 0.6927101612091064 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.04s
test Score 0.5000
Epoch Time List: [0.20359622710384429, 0.20337672717869282, 0.20284737402107567, 0.2023511049337685, 0.20266417390666902, 0.20245250896550715, 0.2015137701528147, 0.20231087296269834, 0.20180388388689607, 0.20334205694962293, 0.2040402750717476]
Total Epoch List: [11]
Total Time List: [0.046794573077932]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2af35ed0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.0209;  Loss pred: 2.0209; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.0337;  Loss pred: 2.0337; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.0222;  Loss pred: 2.0222; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.0280;  Loss pred: 2.0280; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.0272;  Loss pred: 2.0272; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.0099;  Loss pred: 2.0099; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.0183;  Loss pred: 2.0183; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 1.9484;  Loss pred: 1.9484; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 1.9247;  Loss pred: 1.9247; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 1.9002;  Loss pred: 1.9002; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 1.8428;  Loss pred: 1.8428; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 1.8677;  Loss pred: 1.8677; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 1.8333;  Loss pred: 1.8333; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 1.7826;  Loss pred: 1.7826; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 1.7839;  Loss pred: 1.7839; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 1.7513;  Loss pred: 1.7513; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 1.7258;  Loss pred: 1.7258; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 1.6965;  Loss pred: 1.6965; Loss self: 0.0000; time: 0.13s
Val loss: 0.6928 score: 0.5000 time: 0.14s
Test loss: 0.6928 score: 0.5349 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 1.6588;  Loss pred: 1.6588; Loss self: 0.0000; time: 0.13s
Val loss: 0.6928 score: 0.5682 time: 0.04s
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 1.6220;  Loss pred: 1.6220; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 1.5984;  Loss pred: 1.5984; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 1.5933;  Loss pred: 1.5933; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 1.5656;  Loss pred: 1.5656; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 1.5568;  Loss pred: 1.5568; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 25/1000, LR 0.000270
Train loss: 1.5412;  Loss pred: 1.5412; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 1.5120;  Loss pred: 1.5120; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 1.5022;  Loss pred: 1.5022; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 1.4740;  Loss pred: 1.4740; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 1.4569;  Loss pred: 1.4569; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 1.4530;  Loss pred: 1.4530; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 1.4379;  Loss pred: 1.4379; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 1.4372;  Loss pred: 1.4372; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 1.3906;  Loss pred: 1.3906; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 1.3943;  Loss pred: 1.3943; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 1.3706;  Loss pred: 1.3706; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 1.3475;  Loss pred: 1.3475; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 1.3373;  Loss pred: 1.3373; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 1.3332;  Loss pred: 1.3332; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 1.3165;  Loss pred: 1.3165; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 1.3039;  Loss pred: 1.3039; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 1.2835;  Loss pred: 1.2835; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 1.2821;  Loss pred: 1.2821; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 1.2808;  Loss pred: 1.2808; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 1.2488;  Loss pred: 1.2488; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 1.2414;  Loss pred: 1.2414; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 1.2497;  Loss pred: 1.2497; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 1.2257;  Loss pred: 1.2257; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 1.2320;  Loss pred: 1.2320; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 1.2139;  Loss pred: 1.2139; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 1.2076;  Loss pred: 1.2076; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 1.2052;  Loss pred: 1.2052; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 1.1941;  Loss pred: 1.1941; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 1.1799;  Loss pred: 1.1799; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 1.1807;  Loss pred: 1.1807; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 1.1658;  Loss pred: 1.1658; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 1.1582;  Loss pred: 1.1582; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 1.1600;  Loss pred: 1.1600; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 1.1564;  Loss pred: 1.1564; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 1.1383;  Loss pred: 1.1383; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 1.1355;  Loss pred: 1.1355; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 1.1328;  Loss pred: 1.1328; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 1.1317;  Loss pred: 1.1317; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 1.1144;  Loss pred: 1.1144; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 1.1211;  Loss pred: 1.1211; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 1.1130;  Loss pred: 1.1130; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 1.1090;  Loss pred: 1.1090; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 1.1052;  Loss pred: 1.1052; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 1.1037;  Loss pred: 1.1037; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.0974;  Loss pred: 1.0974; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 1.0946;  Loss pred: 1.0946; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 1.0896;  Loss pred: 1.0896; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4884 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 1.0880;  Loss pred: 1.0880; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 1.0787;  Loss pred: 1.0787; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 1.0831;  Loss pred: 1.0831; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 1.0761;  Loss pred: 1.0761; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4884 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 1.0718;  Loss pred: 1.0718; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 1.0652;  Loss pred: 1.0652; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 1.0640;  Loss pred: 1.0640; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 1.0629;  Loss pred: 1.0629; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4884 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 1.0683;  Loss pred: 1.0683; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 1.0594;  Loss pred: 1.0594; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4884 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 1.0546;  Loss pred: 1.0546; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4884 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 1.0508;  Loss pred: 1.0508; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.0441;  Loss pred: 1.0441; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.4884 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 1.0489;  Loss pred: 1.0489; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 1.0472;  Loss pred: 1.0472; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4884 time: 0.04s
Epoch 87/1000, LR 0.000266
Train loss: 1.0438;  Loss pred: 1.0438; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4884 time: 0.04s
Epoch 88/1000, LR 0.000266
Train loss: 1.0409;  Loss pred: 1.0409; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4884 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 1.0418;  Loss pred: 1.0418; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.4884 time: 0.04s
Epoch 90/1000, LR 0.000266
Train loss: 1.0357;  Loss pred: 1.0357; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.4884 time: 0.04s
Epoch 91/1000, LR 0.000266
Train loss: 1.0346;  Loss pred: 1.0346; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4884 time: 0.04s
Epoch 92/1000, LR 0.000266
Train loss: 1.0326;  Loss pred: 1.0326; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.4884 time: 0.04s
Epoch 93/1000, LR 0.000265
Train loss: 1.0323;  Loss pred: 1.0323; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.4884 time: 0.04s
Epoch 94/1000, LR 0.000265
Train loss: 1.0252;  Loss pred: 1.0252; Loss self: 0.0000; time: 0.13s
Val loss: 0.6836 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4884 time: 0.05s
Epoch 95/1000, LR 0.000265
Train loss: 1.0260;  Loss pred: 1.0260; Loss self: 0.0000; time: 0.14s
Val loss: 0.6832 score: 0.5227 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4884 time: 0.04s
Epoch 96/1000, LR 0.000265
Train loss: 1.0224;  Loss pred: 1.0224; Loss self: 0.0000; time: 0.13s
Val loss: 0.6829 score: 0.5227 time: 0.04s
Test loss: 0.6859 score: 0.5116 time: 0.04s
Epoch 97/1000, LR 0.000265
Train loss: 1.0219;  Loss pred: 1.0219; Loss self: 0.0000; time: 0.13s
Val loss: 0.6825 score: 0.5227 time: 0.04s
Test loss: 0.6856 score: 0.5116 time: 0.04s
Epoch 98/1000, LR 0.000265
Train loss: 1.0196;  Loss pred: 1.0196; Loss self: 0.0000; time: 0.13s
Val loss: 0.6821 score: 0.5227 time: 0.04s
Test loss: 0.6853 score: 0.5116 time: 0.04s
Epoch 99/1000, LR 0.000265
Train loss: 1.0178;  Loss pred: 1.0178; Loss self: 0.0000; time: 0.13s
Val loss: 0.6816 score: 0.5455 time: 0.04s
Test loss: 0.6849 score: 0.5116 time: 0.04s
Epoch 100/1000, LR 0.000265
Train loss: 1.0143;  Loss pred: 1.0143; Loss self: 0.0000; time: 0.13s
Val loss: 0.6812 score: 0.5682 time: 0.04s
Test loss: 0.6846 score: 0.5116 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 1.0136;  Loss pred: 1.0136; Loss self: 0.0000; time: 0.13s
Val loss: 0.6807 score: 0.5682 time: 0.05s
Test loss: 0.6842 score: 0.5349 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 1.0131;  Loss pred: 1.0131; Loss self: 0.0000; time: 0.13s
Val loss: 0.6803 score: 0.5682 time: 0.04s
Test loss: 0.6839 score: 0.5349 time: 0.04s
Epoch 103/1000, LR 0.000264
Train loss: 1.0105;  Loss pred: 1.0105; Loss self: 0.0000; time: 0.13s
Val loss: 0.6798 score: 0.5909 time: 0.04s
Test loss: 0.6835 score: 0.5349 time: 0.04s
Epoch 104/1000, LR 0.000264
Train loss: 1.0086;  Loss pred: 1.0086; Loss self: 0.0000; time: 0.13s
Val loss: 0.6792 score: 0.5909 time: 0.04s
Test loss: 0.6831 score: 0.5349 time: 0.04s
Epoch 105/1000, LR 0.000264
Train loss: 1.0096;  Loss pred: 1.0096; Loss self: 0.0000; time: 0.13s
Val loss: 0.6787 score: 0.5909 time: 0.04s
Test loss: 0.6827 score: 0.5349 time: 0.04s
Epoch 106/1000, LR 0.000264
Train loss: 1.0073;  Loss pred: 1.0073; Loss self: 0.0000; time: 0.13s
Val loss: 0.6782 score: 0.6136 time: 0.04s
Test loss: 0.6823 score: 0.5349 time: 0.04s
Epoch 107/1000, LR 0.000264
Train loss: 1.0060;  Loss pred: 1.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.6776 score: 0.6136 time: 0.04s
Test loss: 0.6818 score: 0.5349 time: 0.04s
Epoch 108/1000, LR 0.000264
Train loss: 1.0005;  Loss pred: 1.0005; Loss self: 0.0000; time: 0.13s
Val loss: 0.6770 score: 0.6136 time: 0.04s
Test loss: 0.6814 score: 0.5349 time: 0.04s
Epoch 109/1000, LR 0.000264
Train loss: 1.0007;  Loss pred: 1.0007; Loss self: 0.0000; time: 0.13s
Val loss: 0.6764 score: 0.6364 time: 0.04s
Test loss: 0.6810 score: 0.5349 time: 0.04s
Epoch 110/1000, LR 0.000263
Train loss: 1.0029;  Loss pred: 1.0029; Loss self: 0.0000; time: 0.13s
Val loss: 0.6758 score: 0.6364 time: 0.04s
Test loss: 0.6805 score: 0.5349 time: 0.04s
Epoch 111/1000, LR 0.000263
Train loss: 1.0005;  Loss pred: 1.0005; Loss self: 0.0000; time: 0.13s
Val loss: 0.6752 score: 0.6364 time: 0.04s
Test loss: 0.6800 score: 0.5349 time: 0.04s
Epoch 112/1000, LR 0.000263
Train loss: 0.9946;  Loss pred: 0.9946; Loss self: 0.0000; time: 0.13s
Val loss: 0.6745 score: 0.6364 time: 0.04s
Test loss: 0.6795 score: 0.5349 time: 0.04s
Epoch 113/1000, LR 0.000263
Train loss: 0.9950;  Loss pred: 0.9950; Loss self: 0.0000; time: 0.13s
Val loss: 0.6739 score: 0.6364 time: 0.04s
Test loss: 0.6790 score: 0.5581 time: 0.04s
Epoch 114/1000, LR 0.000263
Train loss: 0.9922;  Loss pred: 0.9922; Loss self: 0.0000; time: 0.13s
Val loss: 0.6732 score: 0.6591 time: 0.04s
Test loss: 0.6784 score: 0.5581 time: 0.04s
Epoch 115/1000, LR 0.000263
Train loss: 0.9907;  Loss pred: 0.9907; Loss self: 0.0000; time: 0.13s
Val loss: 0.6725 score: 0.6591 time: 0.04s
Test loss: 0.6779 score: 0.5814 time: 0.04s
Epoch 116/1000, LR 0.000263
Train loss: 0.9900;  Loss pred: 0.9900; Loss self: 0.0000; time: 0.13s
Val loss: 0.6718 score: 0.6591 time: 0.04s
Test loss: 0.6773 score: 0.5814 time: 0.04s
Epoch 117/1000, LR 0.000262
Train loss: 0.9852;  Loss pred: 0.9852; Loss self: 0.0000; time: 0.13s
Val loss: 0.6710 score: 0.6591 time: 0.04s
Test loss: 0.6768 score: 0.6047 time: 0.04s
Epoch 118/1000, LR 0.000262
Train loss: 0.9856;  Loss pred: 0.9856; Loss self: 0.0000; time: 0.13s
Val loss: 0.6702 score: 0.6818 time: 0.04s
Test loss: 0.6762 score: 0.6047 time: 0.04s
Epoch 119/1000, LR 0.000262
Train loss: 0.9844;  Loss pred: 0.9844; Loss self: 0.0000; time: 0.13s
Val loss: 0.6695 score: 0.6818 time: 0.04s
Test loss: 0.6756 score: 0.6279 time: 0.04s
Epoch 120/1000, LR 0.000262
Train loss: 0.9866;  Loss pred: 0.9866; Loss self: 0.0000; time: 0.13s
Val loss: 0.6686 score: 0.6818 time: 0.04s
Test loss: 0.6749 score: 0.6279 time: 0.04s
Epoch 121/1000, LR 0.000262
Train loss: 0.9814;  Loss pred: 0.9814; Loss self: 0.0000; time: 0.13s
Val loss: 0.6678 score: 0.6818 time: 0.04s
Test loss: 0.6743 score: 0.6279 time: 0.04s
Epoch 122/1000, LR 0.000262
Train loss: 0.9796;  Loss pred: 0.9796; Loss self: 0.0000; time: 4.60s
Val loss: 0.6670 score: 0.6818 time: 0.37s
Test loss: 0.6736 score: 0.6279 time: 0.16s
Epoch 123/1000, LR 0.000262
Train loss: 0.9785;  Loss pred: 0.9785; Loss self: 0.0000; time: 1.42s
Val loss: 0.6661 score: 0.6818 time: 0.07s
Test loss: 0.6730 score: 0.6279 time: 0.05s
Epoch 124/1000, LR 0.000261
Train loss: 0.9778;  Loss pred: 0.9778; Loss self: 0.0000; time: 0.15s
Val loss: 0.6652 score: 0.6818 time: 0.05s
Test loss: 0.6723 score: 0.6279 time: 0.05s
Epoch 125/1000, LR 0.000261
Train loss: 0.9732;  Loss pred: 0.9732; Loss self: 0.0000; time: 0.13s
Val loss: 0.6642 score: 0.6818 time: 0.04s
Test loss: 0.6715 score: 0.6279 time: 0.04s
Epoch 126/1000, LR 0.000261
Train loss: 0.9747;  Loss pred: 0.9747; Loss self: 0.0000; time: 0.13s
Val loss: 0.6633 score: 0.6818 time: 0.04s
Test loss: 0.6708 score: 0.6279 time: 0.04s
Epoch 127/1000, LR 0.000261
Train loss: 0.9724;  Loss pred: 0.9724; Loss self: 0.0000; time: 0.13s
Val loss: 0.6623 score: 0.6818 time: 0.04s
Test loss: 0.6700 score: 0.6279 time: 0.04s
Epoch 128/1000, LR 0.000261
Train loss: 0.9703;  Loss pred: 0.9703; Loss self: 0.0000; time: 0.13s
Val loss: 0.6613 score: 0.6818 time: 0.05s
Test loss: 0.6692 score: 0.6279 time: 0.05s
Epoch 129/1000, LR 0.000261
Train loss: 0.9694;  Loss pred: 0.9694; Loss self: 0.0000; time: 0.14s
Val loss: 0.6602 score: 0.6818 time: 0.04s
Test loss: 0.6684 score: 0.6279 time: 0.04s
Epoch 130/1000, LR 0.000260
Train loss: 0.9679;  Loss pred: 0.9679; Loss self: 0.0000; time: 0.13s
Val loss: 0.6592 score: 0.6818 time: 0.04s
Test loss: 0.6676 score: 0.6512 time: 0.04s
Epoch 131/1000, LR 0.000260
Train loss: 0.9659;  Loss pred: 0.9659; Loss self: 0.0000; time: 0.13s
Val loss: 0.6581 score: 0.7045 time: 0.04s
Test loss: 0.6668 score: 0.6512 time: 0.04s
Epoch 132/1000, LR 0.000260
Train loss: 0.9645;  Loss pred: 0.9645; Loss self: 0.0000; time: 0.13s
Val loss: 0.6570 score: 0.7500 time: 0.04s
Test loss: 0.6659 score: 0.6512 time: 0.04s
Epoch 133/1000, LR 0.000260
Train loss: 0.9601;  Loss pred: 0.9601; Loss self: 0.0000; time: 0.13s
Val loss: 0.6558 score: 0.7500 time: 0.04s
Test loss: 0.6650 score: 0.6744 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 0.9613;  Loss pred: 0.9613; Loss self: 0.0000; time: 0.13s
Val loss: 0.6547 score: 0.7500 time: 0.05s
Test loss: 0.6641 score: 0.6977 time: 0.05s
Epoch 135/1000, LR 0.000260
Train loss: 0.9602;  Loss pred: 0.9602; Loss self: 0.0000; time: 0.14s
Val loss: 0.6535 score: 0.7500 time: 0.04s
Test loss: 0.6632 score: 0.6977 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9578;  Loss pred: 0.9578; Loss self: 0.0000; time: 0.13s
Val loss: 0.6523 score: 0.7500 time: 0.04s
Test loss: 0.6623 score: 0.6977 time: 0.04s
Epoch 137/1000, LR 0.000259
Train loss: 0.9559;  Loss pred: 0.9559; Loss self: 0.0000; time: 0.13s
Val loss: 0.6511 score: 0.7500 time: 0.04s
Test loss: 0.6613 score: 0.6977 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9560;  Loss pred: 0.9560; Loss self: 0.0000; time: 0.13s
Val loss: 0.6498 score: 0.7500 time: 0.04s
Test loss: 0.6603 score: 0.6977 time: 0.04s
Epoch 139/1000, LR 0.000259
Train loss: 0.9516;  Loss pred: 0.9516; Loss self: 0.0000; time: 0.13s
Val loss: 0.6485 score: 0.7500 time: 0.04s
Test loss: 0.6593 score: 0.6977 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9500;  Loss pred: 0.9500; Loss self: 0.0000; time: 0.13s
Val loss: 0.6471 score: 0.7500 time: 0.04s
Test loss: 0.6583 score: 0.6977 time: 0.04s
Epoch 141/1000, LR 0.000259
Train loss: 0.9491;  Loss pred: 0.9491; Loss self: 0.0000; time: 0.13s
Val loss: 0.6458 score: 0.7500 time: 0.05s
Test loss: 0.6572 score: 0.6977 time: 0.05s
Epoch 142/1000, LR 0.000259
Train loss: 0.9492;  Loss pred: 0.9492; Loss self: 0.0000; time: 0.14s
Val loss: 0.6443 score: 0.7500 time: 0.04s
Test loss: 0.6561 score: 0.6977 time: 0.04s
Epoch 143/1000, LR 0.000258
Train loss: 0.9453;  Loss pred: 0.9453; Loss self: 0.0000; time: 0.13s
Val loss: 0.6429 score: 0.7500 time: 0.04s
Test loss: 0.6550 score: 0.6977 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9424;  Loss pred: 0.9424; Loss self: 0.0000; time: 0.13s
Val loss: 0.6415 score: 0.7500 time: 0.04s
Test loss: 0.6538 score: 0.6977 time: 0.04s
Epoch 145/1000, LR 0.000258
Train loss: 0.9392;  Loss pred: 0.9392; Loss self: 0.0000; time: 0.13s
Val loss: 0.6400 score: 0.7500 time: 0.04s
Test loss: 0.6527 score: 0.6977 time: 0.04s
Epoch 146/1000, LR 0.000258
Train loss: 0.9417;  Loss pred: 0.9417; Loss self: 0.0000; time: 0.13s
Val loss: 0.6386 score: 0.7500 time: 0.04s
Test loss: 0.6516 score: 0.6977 time: 0.05s
Epoch 147/1000, LR 0.000258
Train loss: 0.9364;  Loss pred: 0.9364; Loss self: 0.0000; time: 0.13s
Val loss: 0.6371 score: 0.7727 time: 0.05s
Test loss: 0.6505 score: 0.6977 time: 0.05s
Epoch 148/1000, LR 0.000257
Train loss: 0.9353;  Loss pred: 0.9353; Loss self: 0.0000; time: 0.14s
Val loss: 0.6355 score: 0.7727 time: 0.05s
Test loss: 0.6493 score: 0.6977 time: 0.05s
Epoch 149/1000, LR 0.000257
Train loss: 0.9328;  Loss pred: 0.9328; Loss self: 0.0000; time: 0.14s
Val loss: 0.6340 score: 0.7727 time: 0.05s
Test loss: 0.6481 score: 0.6977 time: 0.05s
Epoch 150/1000, LR 0.000257
Train loss: 0.9306;  Loss pred: 0.9306; Loss self: 0.0000; time: 0.15s
Val loss: 0.6324 score: 0.7727 time: 0.05s
Test loss: 0.6469 score: 0.6977 time: 0.04s
Epoch 151/1000, LR 0.000257
Train loss: 0.9291;  Loss pred: 0.9291; Loss self: 0.0000; time: 0.13s
Val loss: 0.6308 score: 0.7727 time: 0.04s
Test loss: 0.6456 score: 0.6977 time: 0.04s
Epoch 152/1000, LR 0.000257
Train loss: 0.9277;  Loss pred: 0.9277; Loss self: 0.0000; time: 0.13s
Val loss: 0.6291 score: 0.7727 time: 0.04s
Test loss: 0.6442 score: 0.6977 time: 0.04s
Epoch 153/1000, LR 0.000257
Train loss: 0.9245;  Loss pred: 0.9245; Loss self: 0.0000; time: 0.13s
Val loss: 0.6274 score: 0.7727 time: 0.04s
Test loss: 0.6429 score: 0.6977 time: 0.04s
Epoch 154/1000, LR 0.000256
Train loss: 0.9226;  Loss pred: 0.9226; Loss self: 0.0000; time: 0.13s
Val loss: 0.6257 score: 0.7727 time: 0.04s
Test loss: 0.6415 score: 0.6977 time: 0.04s
Epoch 155/1000, LR 0.000256
Train loss: 0.9200;  Loss pred: 0.9200; Loss self: 0.0000; time: 0.13s
Val loss: 0.6239 score: 0.7727 time: 0.04s
Test loss: 0.6402 score: 0.6977 time: 0.04s
Epoch 156/1000, LR 0.000256
Train loss: 0.9198;  Loss pred: 0.9198; Loss self: 0.0000; time: 0.13s
Val loss: 0.6222 score: 0.7727 time: 0.04s
Test loss: 0.6388 score: 0.6977 time: 0.05s
Epoch 157/1000, LR 0.000256
Train loss: 0.9171;  Loss pred: 0.9171; Loss self: 0.0000; time: 0.14s
Val loss: 0.6205 score: 0.7727 time: 0.04s
Test loss: 0.6375 score: 0.6977 time: 0.04s
Epoch 158/1000, LR 0.000256
Train loss: 0.9131;  Loss pred: 0.9131; Loss self: 0.0000; time: 0.13s
Val loss: 0.6187 score: 0.7727 time: 0.04s
Test loss: 0.6361 score: 0.6977 time: 0.04s
Epoch 159/1000, LR 0.000255
Train loss: 0.9126;  Loss pred: 0.9126; Loss self: 0.0000; time: 0.13s
Val loss: 0.6169 score: 0.7727 time: 0.04s
Test loss: 0.6348 score: 0.6977 time: 0.04s
Epoch 160/1000, LR 0.000255
Train loss: 0.9106;  Loss pred: 0.9106; Loss self: 0.0000; time: 0.13s
Val loss: 0.6151 score: 0.7727 time: 0.04s
Test loss: 0.6333 score: 0.6977 time: 0.04s
Epoch 161/1000, LR 0.000255
Train loss: 0.9082;  Loss pred: 0.9082; Loss self: 0.0000; time: 0.13s
Val loss: 0.6132 score: 0.7727 time: 0.04s
Test loss: 0.6318 score: 0.6977 time: 0.04s
Epoch 162/1000, LR 0.000255
Train loss: 0.9053;  Loss pred: 0.9053; Loss self: 0.0000; time: 0.13s
Val loss: 0.6113 score: 0.7727 time: 0.04s
Test loss: 0.6304 score: 0.6977 time: 0.04s
Epoch 163/1000, LR 0.000255
Train loss: 0.9025;  Loss pred: 0.9025; Loss self: 0.0000; time: 0.13s
Val loss: 0.6094 score: 0.7955 time: 0.04s
Test loss: 0.6289 score: 0.6977 time: 0.04s
Epoch 164/1000, LR 0.000254
Train loss: 0.9001;  Loss pred: 0.9001; Loss self: 0.0000; time: 0.13s
Val loss: 0.6075 score: 0.7955 time: 0.04s
Test loss: 0.6274 score: 0.6977 time: 0.04s
Epoch 165/1000, LR 0.000254
Train loss: 0.8986;  Loss pred: 0.8986; Loss self: 0.0000; time: 0.13s
Val loss: 0.6055 score: 0.7955 time: 0.04s
Test loss: 0.6258 score: 0.6977 time: 0.04s
Epoch 166/1000, LR 0.000254
Train loss: 0.8963;  Loss pred: 0.8963; Loss self: 0.0000; time: 0.13s
Val loss: 0.6035 score: 0.7955 time: 0.04s
Test loss: 0.6242 score: 0.6977 time: 0.04s
Epoch 167/1000, LR 0.000254
Train loss: 0.8944;  Loss pred: 0.8944; Loss self: 0.0000; time: 0.13s
Val loss: 0.6015 score: 0.7955 time: 0.04s
Test loss: 0.6226 score: 0.6977 time: 0.04s
Epoch 168/1000, LR 0.000254
Train loss: 0.8916;  Loss pred: 0.8916; Loss self: 0.0000; time: 0.13s
Val loss: 0.5994 score: 0.7955 time: 0.04s
Test loss: 0.6210 score: 0.6977 time: 0.04s
Epoch 169/1000, LR 0.000253
Train loss: 0.8894;  Loss pred: 0.8894; Loss self: 0.0000; time: 0.13s
Val loss: 0.5974 score: 0.7955 time: 0.04s
Test loss: 0.6193 score: 0.6977 time: 0.04s
Epoch 170/1000, LR 0.000253
Train loss: 0.8862;  Loss pred: 0.8862; Loss self: 0.0000; time: 0.13s
Val loss: 0.5953 score: 0.7955 time: 0.04s
Test loss: 0.6177 score: 0.6977 time: 0.04s
Epoch 171/1000, LR 0.000253
Train loss: 0.8823;  Loss pred: 0.8823; Loss self: 0.0000; time: 0.13s
Val loss: 0.5933 score: 0.7955 time: 0.04s
Test loss: 0.6161 score: 0.6977 time: 0.04s
Epoch 172/1000, LR 0.000253
Train loss: 0.8834;  Loss pred: 0.8834; Loss self: 0.0000; time: 0.13s
Val loss: 0.5913 score: 0.7955 time: 0.04s
Test loss: 0.6146 score: 0.6977 time: 0.36s
Epoch 173/1000, LR 0.000253
Train loss: 0.8800;  Loss pred: 0.8800; Loss self: 0.0000; time: 2.12s
Val loss: 0.5893 score: 0.7955 time: 0.40s
Test loss: 0.6130 score: 0.6977 time: 0.47s
Epoch 174/1000, LR 0.000252
Train loss: 0.8764;  Loss pred: 0.8764; Loss self: 0.0000; time: 1.87s
Val loss: 0.5873 score: 0.7955 time: 0.10s
Test loss: 0.6115 score: 0.6977 time: 0.14s
Epoch 175/1000, LR 0.000252
Train loss: 0.8747;  Loss pred: 0.8747; Loss self: 0.0000; time: 0.25s
Val loss: 0.5853 score: 0.7955 time: 0.05s
Test loss: 0.6100 score: 0.6977 time: 0.04s
Epoch 176/1000, LR 0.000252
Train loss: 0.8721;  Loss pred: 0.8721; Loss self: 0.0000; time: 0.13s
Val loss: 0.5832 score: 0.7955 time: 0.04s
Test loss: 0.6083 score: 0.6977 time: 0.04s
Epoch 177/1000, LR 0.000252
Train loss: 0.8683;  Loss pred: 0.8683; Loss self: 0.0000; time: 0.13s
Val loss: 0.5811 score: 0.7955 time: 0.04s
Test loss: 0.6068 score: 0.6977 time: 0.04s
Epoch 178/1000, LR 0.000251
Train loss: 0.8687;  Loss pred: 0.8687; Loss self: 0.0000; time: 0.13s
Val loss: 0.5790 score: 0.7955 time: 0.04s
Test loss: 0.6051 score: 0.6977 time: 0.04s
Epoch 179/1000, LR 0.000251
Train loss: 0.8656;  Loss pred: 0.8656; Loss self: 0.0000; time: 0.13s
Val loss: 0.5769 score: 0.7955 time: 0.04s
Test loss: 0.6034 score: 0.6977 time: 0.04s
Epoch 180/1000, LR 0.000251
Train loss: 0.8630;  Loss pred: 0.8630; Loss self: 0.0000; time: 0.13s
Val loss: 0.5748 score: 0.7955 time: 0.04s
Test loss: 0.6017 score: 0.6977 time: 0.04s
Epoch 181/1000, LR 0.000251
Train loss: 0.8598;  Loss pred: 0.8598; Loss self: 0.0000; time: 0.12s
Val loss: 0.5727 score: 0.7955 time: 0.04s
Test loss: 0.6001 score: 0.6977 time: 0.04s
Epoch 182/1000, LR 0.000251
Train loss: 0.8570;  Loss pred: 0.8570; Loss self: 0.0000; time: 0.12s
Val loss: 0.5706 score: 0.7955 time: 0.04s
Test loss: 0.5985 score: 0.6977 time: 0.04s
Epoch 183/1000, LR 0.000250
Train loss: 0.8558;  Loss pred: 0.8558; Loss self: 0.0000; time: 0.12s
Val loss: 0.5685 score: 0.7955 time: 0.04s
Test loss: 0.5967 score: 0.6977 time: 0.04s
Epoch 184/1000, LR 0.000250
Train loss: 0.8514;  Loss pred: 0.8514; Loss self: 0.0000; time: 0.13s
Val loss: 0.5664 score: 0.7955 time: 0.04s
Test loss: 0.5949 score: 0.6977 time: 0.04s
Epoch 185/1000, LR 0.000250
Train loss: 0.8491;  Loss pred: 0.8491; Loss self: 0.0000; time: 0.13s
Val loss: 0.5643 score: 0.7955 time: 0.04s
Test loss: 0.5933 score: 0.6977 time: 0.04s
Epoch 186/1000, LR 0.000250
Train loss: 0.8465;  Loss pred: 0.8465; Loss self: 0.0000; time: 0.13s
Val loss: 0.5623 score: 0.7955 time: 0.04s
Test loss: 0.5917 score: 0.6977 time: 0.04s
Epoch 187/1000, LR 0.000249
Train loss: 0.8453;  Loss pred: 0.8453; Loss self: 0.0000; time: 0.13s
Val loss: 0.5602 score: 0.7955 time: 0.04s
Test loss: 0.5901 score: 0.6977 time: 0.04s
Epoch 188/1000, LR 0.000249
Train loss: 0.8427;  Loss pred: 0.8427; Loss self: 0.0000; time: 0.13s
Val loss: 0.5583 score: 0.7955 time: 0.04s
Test loss: 0.5885 score: 0.6977 time: 0.05s
Epoch 189/1000, LR 0.000249
Train loss: 0.8388;  Loss pred: 0.8388; Loss self: 0.0000; time: 0.13s
Val loss: 0.5563 score: 0.7955 time: 0.04s
Test loss: 0.5869 score: 0.6977 time: 0.04s
Epoch 190/1000, LR 0.000249
Train loss: 0.8359;  Loss pred: 0.8359; Loss self: 0.0000; time: 0.13s
Val loss: 0.5542 score: 0.7955 time: 0.04s
Test loss: 0.5852 score: 0.6977 time: 0.05s
Epoch 191/1000, LR 0.000249
Train loss: 0.8357;  Loss pred: 0.8357; Loss self: 0.0000; time: 0.14s
Val loss: 0.5522 score: 0.7955 time: 0.04s
Test loss: 0.5836 score: 0.6977 time: 0.05s
Epoch 192/1000, LR 0.000248
Train loss: 0.8316;  Loss pred: 0.8316; Loss self: 0.0000; time: 0.14s
Val loss: 0.5503 score: 0.7955 time: 0.04s
Test loss: 0.5821 score: 0.6977 time: 0.05s
Epoch 193/1000, LR 0.000248
Train loss: 0.8274;  Loss pred: 0.8274; Loss self: 0.0000; time: 0.14s
Val loss: 0.5484 score: 0.7955 time: 0.04s
Test loss: 0.5806 score: 0.6977 time: 0.05s
Epoch 194/1000, LR 0.000248
Train loss: 0.8267;  Loss pred: 0.8267; Loss self: 0.0000; time: 0.14s
Val loss: 0.5466 score: 0.7955 time: 0.04s
Test loss: 0.5792 score: 0.6977 time: 0.04s
Epoch 195/1000, LR 0.000248
Train loss: 0.8256;  Loss pred: 0.8256; Loss self: 0.0000; time: 0.12s
Val loss: 0.5447 score: 0.7955 time: 0.04s
Test loss: 0.5777 score: 0.6977 time: 0.04s
Epoch 196/1000, LR 0.000247
Train loss: 0.8238;  Loss pred: 0.8238; Loss self: 0.0000; time: 0.12s
Val loss: 0.5428 score: 0.7955 time: 0.04s
Test loss: 0.5761 score: 0.6977 time: 0.04s
Epoch 197/1000, LR 0.000247
Train loss: 0.8187;  Loss pred: 0.8187; Loss self: 0.0000; time: 0.12s
Val loss: 0.5408 score: 0.7955 time: 0.04s
Test loss: 0.5744 score: 0.6977 time: 0.04s
Epoch 198/1000, LR 0.000247
Train loss: 0.8167;  Loss pred: 0.8167; Loss self: 0.0000; time: 0.12s
Val loss: 0.5388 score: 0.7955 time: 0.04s
Test loss: 0.5728 score: 0.6977 time: 0.04s
Epoch 199/1000, LR 0.000247
Train loss: 0.8147;  Loss pred: 0.8147; Loss self: 0.0000; time: 0.13s
Val loss: 0.5369 score: 0.7955 time: 0.04s
Test loss: 0.5711 score: 0.6977 time: 0.04s
Epoch 200/1000, LR 0.000246
Train loss: 0.8153;  Loss pred: 0.8153; Loss self: 0.0000; time: 0.13s
Val loss: 0.5348 score: 0.7955 time: 0.04s
Test loss: 0.5693 score: 0.6977 time: 0.04s
Epoch 201/1000, LR 0.000246
Train loss: 0.8094;  Loss pred: 0.8094; Loss self: 0.0000; time: 0.13s
Val loss: 0.5326 score: 0.7955 time: 0.04s
Test loss: 0.5673 score: 0.6977 time: 0.04s
Epoch 202/1000, LR 0.000246
Train loss: 0.8091;  Loss pred: 0.8091; Loss self: 0.0000; time: 0.13s
Val loss: 0.5305 score: 0.7955 time: 0.04s
Test loss: 0.5653 score: 0.6977 time: 0.05s
Epoch 203/1000, LR 0.000246
Train loss: 0.8057;  Loss pred: 0.8057; Loss self: 0.0000; time: 0.14s
Val loss: 0.5283 score: 0.7955 time: 0.04s
Test loss: 0.5634 score: 0.7209 time: 0.05s
Epoch 204/1000, LR 0.000245
Train loss: 0.8012;  Loss pred: 0.8012; Loss self: 0.0000; time: 0.14s
Val loss: 0.5264 score: 0.7955 time: 0.04s
Test loss: 0.5617 score: 0.7209 time: 0.05s
Epoch 205/1000, LR 0.000245
Train loss: 0.8001;  Loss pred: 0.8001; Loss self: 0.0000; time: 0.14s
Val loss: 0.5245 score: 0.7955 time: 0.04s
Test loss: 0.5601 score: 0.7209 time: 0.04s
Epoch 206/1000, LR 0.000245
Train loss: 0.7967;  Loss pred: 0.7967; Loss self: 0.0000; time: 0.13s
Val loss: 0.5227 score: 0.7955 time: 0.04s
Test loss: 0.5586 score: 0.7209 time: 0.04s
Epoch 207/1000, LR 0.000245
Train loss: 0.7965;  Loss pred: 0.7965; Loss self: 0.0000; time: 0.13s
Val loss: 0.5210 score: 0.7955 time: 0.04s
Test loss: 0.5572 score: 0.7209 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.7919;  Loss pred: 0.7919; Loss self: 0.0000; time: 0.13s
Val loss: 0.5193 score: 0.7955 time: 0.04s
Test loss: 0.5558 score: 0.7209 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.7902;  Loss pred: 0.7902; Loss self: 0.0000; time: 0.13s
Val loss: 0.5176 score: 0.7955 time: 0.04s
Test loss: 0.5545 score: 0.7209 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.7888;  Loss pred: 0.7888; Loss self: 0.0000; time: 0.13s
Val loss: 0.5159 score: 0.7955 time: 0.04s
Test loss: 0.5530 score: 0.7209 time: 0.04s
Epoch 211/1000, LR 0.000244
Train loss: 0.7866;  Loss pred: 0.7866; Loss self: 0.0000; time: 0.13s
Val loss: 0.5143 score: 0.7955 time: 0.04s
Test loss: 0.5518 score: 0.7209 time: 0.04s
Epoch 212/1000, LR 0.000243
Train loss: 0.7840;  Loss pred: 0.7840; Loss self: 0.0000; time: 0.13s
Val loss: 0.5127 score: 0.7955 time: 0.04s
Test loss: 0.5504 score: 0.7209 time: 0.04s
Epoch 213/1000, LR 0.000243
Train loss: 0.7847;  Loss pred: 0.7847; Loss self: 0.0000; time: 0.13s
Val loss: 0.5109 score: 0.7955 time: 0.05s
Test loss: 0.5489 score: 0.7209 time: 0.05s
Epoch 214/1000, LR 0.000243
Train loss: 0.7813;  Loss pred: 0.7813; Loss self: 0.0000; time: 0.15s
Val loss: 0.5090 score: 0.7955 time: 0.05s
Test loss: 0.5472 score: 0.7209 time: 0.05s
Epoch 215/1000, LR 0.000243
Train loss: 0.7784;  Loss pred: 0.7784; Loss self: 0.0000; time: 0.15s
Val loss: 0.5071 score: 0.7955 time: 0.05s
Test loss: 0.5454 score: 0.7674 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.7767;  Loss pred: 0.7767; Loss self: 0.0000; time: 0.14s
Val loss: 0.5051 score: 0.7955 time: 0.04s
Test loss: 0.5435 score: 0.7674 time: 0.04s
Epoch 217/1000, LR 0.000242
Train loss: 0.7731;  Loss pred: 0.7731; Loss self: 0.0000; time: 0.13s
Val loss: 0.5030 score: 0.7955 time: 0.04s
Test loss: 0.5414 score: 0.7907 time: 0.04s
Epoch 218/1000, LR 0.000242
Train loss: 0.7722;  Loss pred: 0.7722; Loss self: 0.0000; time: 0.13s
Val loss: 0.5008 score: 0.7955 time: 0.04s
Test loss: 0.5393 score: 0.8140 time: 0.04s
Epoch 219/1000, LR 0.000242
Train loss: 0.7675;  Loss pred: 0.7675; Loss self: 0.0000; time: 0.13s
Val loss: 0.4987 score: 0.8182 time: 0.05s
Test loss: 0.5372 score: 0.8140 time: 0.05s
Epoch 220/1000, LR 0.000241
Train loss: 0.7670;  Loss pred: 0.7670; Loss self: 0.0000; time: 0.14s
Val loss: 0.4968 score: 0.8182 time: 0.04s
Test loss: 0.5353 score: 0.8140 time: 0.04s
Epoch 221/1000, LR 0.000241
Train loss: 0.7660;  Loss pred: 0.7660; Loss self: 0.0000; time: 0.13s
Val loss: 0.4948 score: 0.8182 time: 0.04s
Test loss: 0.5335 score: 0.8140 time: 0.04s
Epoch 222/1000, LR 0.000241
Train loss: 0.7619;  Loss pred: 0.7619; Loss self: 0.0000; time: 0.13s
Val loss: 0.4929 score: 0.8182 time: 0.04s
Test loss: 0.5316 score: 0.8140 time: 0.04s
Epoch 223/1000, LR 0.000241
Train loss: 0.7609;  Loss pred: 0.7609; Loss self: 0.0000; time: 0.13s
Val loss: 0.4909 score: 0.8182 time: 0.04s
Test loss: 0.5297 score: 0.8140 time: 0.04s
Epoch 224/1000, LR 0.000240
Train loss: 0.7575;  Loss pred: 0.7575; Loss self: 0.0000; time: 0.13s
Val loss: 0.4889 score: 0.8182 time: 0.05s
Test loss: 0.5277 score: 0.8140 time: 0.05s
Epoch 225/1000, LR 0.000240
Train loss: 0.7558;  Loss pred: 0.7558; Loss self: 0.0000; time: 0.14s
Val loss: 0.4870 score: 0.8182 time: 0.04s
Test loss: 0.5259 score: 0.8140 time: 0.04s
Epoch 226/1000, LR 0.000240
Train loss: 0.7527;  Loss pred: 0.7527; Loss self: 0.0000; time: 0.13s
Val loss: 0.4852 score: 0.8182 time: 0.04s
Test loss: 0.5242 score: 0.8140 time: 0.04s
Epoch 227/1000, LR 0.000240
Train loss: 0.7501;  Loss pred: 0.7501; Loss self: 0.0000; time: 0.13s
Val loss: 0.4835 score: 0.8182 time: 0.04s
Test loss: 0.5226 score: 0.8140 time: 0.04s
Epoch 228/1000, LR 0.000239
Train loss: 0.7488;  Loss pred: 0.7488; Loss self: 0.0000; time: 0.13s
Val loss: 0.4817 score: 0.8182 time: 0.05s
Test loss: 0.5209 score: 0.8140 time: 0.05s
Epoch 229/1000, LR 0.000239
Train loss: 0.7481;  Loss pred: 0.7481; Loss self: 0.0000; time: 0.14s
Val loss: 0.4801 score: 0.8182 time: 0.05s
Test loss: 0.5193 score: 0.8140 time: 0.05s
Epoch 230/1000, LR 0.000239
Train loss: 0.7438;  Loss pred: 0.7438; Loss self: 0.0000; time: 0.14s
Val loss: 0.4785 score: 0.8182 time: 0.05s
Test loss: 0.5178 score: 0.8140 time: 0.06s
Epoch 231/1000, LR 0.000238
Train loss: 0.7432;  Loss pred: 0.7432; Loss self: 0.0000; time: 0.14s
Val loss: 0.4768 score: 0.8182 time: 0.05s
Test loss: 0.5163 score: 0.8140 time: 0.05s
Epoch 232/1000, LR 0.000238
Train loss: 0.7402;  Loss pred: 0.7402; Loss self: 0.0000; time: 0.14s
Val loss: 0.4751 score: 0.8182 time: 0.05s
Test loss: 0.5147 score: 0.8140 time: 0.04s
Epoch 233/1000, LR 0.000238
Train loss: 0.7368;  Loss pred: 0.7368; Loss self: 0.0000; time: 0.14s
Val loss: 0.4735 score: 0.8182 time: 0.10s
Test loss: 0.5131 score: 0.8140 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.7390;  Loss pred: 0.7390; Loss self: 0.0000; time: 0.13s
Val loss: 0.4717 score: 0.8182 time: 0.04s
Test loss: 0.5114 score: 0.8140 time: 0.04s
Epoch 235/1000, LR 0.000237
Train loss: 0.7355;  Loss pred: 0.7355; Loss self: 0.0000; time: 0.13s
Val loss: 0.4700 score: 0.8182 time: 0.04s
Test loss: 0.5096 score: 0.8140 time: 0.04s
Epoch 236/1000, LR 0.000237
Train loss: 0.7307;  Loss pred: 0.7307; Loss self: 0.0000; time: 0.13s
Val loss: 0.4683 score: 0.8182 time: 0.04s
Test loss: 0.5080 score: 0.8140 time: 0.05s
Epoch 237/1000, LR 0.000237
Train loss: 0.7298;  Loss pred: 0.7298; Loss self: 0.0000; time: 0.13s
Val loss: 0.4666 score: 0.8182 time: 0.05s
Test loss: 0.5063 score: 0.8140 time: 0.05s
Epoch 238/1000, LR 0.000236
Train loss: 0.7292;  Loss pred: 0.7292; Loss self: 0.0000; time: 0.14s
Val loss: 0.4649 score: 0.8182 time: 0.04s
Test loss: 0.5046 score: 0.8140 time: 0.05s
Epoch 239/1000, LR 0.000236
Train loss: 0.7256;  Loss pred: 0.7256; Loss self: 0.0000; time: 0.14s
Val loss: 0.4633 score: 0.8182 time: 0.05s
Test loss: 0.5031 score: 0.8140 time: 0.04s
Epoch 240/1000, LR 0.000236
Train loss: 0.7244;  Loss pred: 0.7244; Loss self: 0.0000; time: 0.20s
Val loss: 0.4617 score: 0.8182 time: 0.05s
Test loss: 0.5015 score: 0.8140 time: 0.05s
Epoch 241/1000, LR 0.000236
Train loss: 0.7222;  Loss pred: 0.7222; Loss self: 0.0000; time: 0.14s
Val loss: 0.4602 score: 0.8182 time: 0.05s
Test loss: 0.5001 score: 0.8140 time: 0.05s
Epoch 242/1000, LR 0.000235
Train loss: 0.7226;  Loss pred: 0.7226; Loss self: 0.0000; time: 0.14s
Val loss: 0.4587 score: 0.8182 time: 0.05s
Test loss: 0.4987 score: 0.8140 time: 0.05s
Epoch 243/1000, LR 0.000235
Train loss: 0.7185;  Loss pred: 0.7185; Loss self: 0.0000; time: 0.14s
Val loss: 0.4570 score: 0.8182 time: 0.05s
Test loss: 0.4970 score: 0.8140 time: 0.05s
Epoch 244/1000, LR 0.000235
Train loss: 0.7154;  Loss pred: 0.7154; Loss self: 0.0000; time: 0.14s
Val loss: 0.4554 score: 0.8182 time: 0.05s
Test loss: 0.4954 score: 0.8140 time: 0.05s
Epoch 245/1000, LR 0.000234
Train loss: 0.7134;  Loss pred: 0.7134; Loss self: 0.0000; time: 0.14s
Val loss: 0.4541 score: 0.8182 time: 0.05s
Test loss: 0.4942 score: 0.8140 time: 0.05s
Epoch 246/1000, LR 0.000234
Train loss: 0.7131;  Loss pred: 0.7131; Loss self: 0.0000; time: 0.15s
Val loss: 0.4526 score: 0.8182 time: 0.12s
Test loss: 0.4928 score: 0.8140 time: 0.05s
Epoch 247/1000, LR 0.000234
Train loss: 0.7101;  Loss pred: 0.7101; Loss self: 0.0000; time: 0.14s
Val loss: 0.4511 score: 0.8182 time: 0.05s
Test loss: 0.4913 score: 0.8140 time: 0.05s
Epoch 248/1000, LR 0.000234
Train loss: 0.7093;  Loss pred: 0.7093; Loss self: 0.0000; time: 0.14s
Val loss: 0.4498 score: 0.8182 time: 0.05s
Test loss: 0.4900 score: 0.8140 time: 0.05s
Epoch 249/1000, LR 0.000233
Train loss: 0.7085;  Loss pred: 0.7085; Loss self: 0.0000; time: 0.14s
Val loss: 0.4483 score: 0.8182 time: 0.05s
Test loss: 0.4886 score: 0.8140 time: 0.05s
Epoch 250/1000, LR 0.000233
Train loss: 0.7048;  Loss pred: 0.7048; Loss self: 0.0000; time: 0.14s
Val loss: 0.4468 score: 0.8182 time: 0.05s
Test loss: 0.4871 score: 0.8140 time: 0.05s
Epoch 251/1000, LR 0.000233
Train loss: 0.7040;  Loss pred: 0.7040; Loss self: 0.0000; time: 0.14s
Val loss: 0.4454 score: 0.8182 time: 0.05s
Test loss: 0.4857 score: 0.8140 time: 0.05s
Epoch 252/1000, LR 0.000232
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.14s
Val loss: 0.4441 score: 0.8182 time: 0.05s
Test loss: 0.4846 score: 0.8140 time: 0.05s
Epoch 253/1000, LR 0.000232
Train loss: 0.7013;  Loss pred: 0.7013; Loss self: 0.0000; time: 0.16s
Val loss: 0.4428 score: 0.8182 time: 0.12s
Test loss: 0.4833 score: 0.8140 time: 0.05s
Epoch 254/1000, LR 0.000232
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.14s
Val loss: 0.4416 score: 0.8182 time: 0.04s
Test loss: 0.4822 score: 0.8140 time: 0.04s
Epoch 255/1000, LR 0.000232
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.13s
Val loss: 0.4402 score: 0.8182 time: 0.04s
Test loss: 0.4809 score: 0.8140 time: 0.04s
Epoch 256/1000, LR 0.000231
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.13s
Val loss: 0.4388 score: 0.8182 time: 0.04s
Test loss: 0.4794 score: 0.8140 time: 0.04s
Epoch 257/1000, LR 0.000231
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.13s
Val loss: 0.4374 score: 0.8182 time: 0.05s
Test loss: 0.4781 score: 0.8140 time: 0.05s
Epoch 258/1000, LR 0.000231
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.14s
Val loss: 0.4363 score: 0.8182 time: 0.05s
Test loss: 0.4770 score: 0.8140 time: 0.05s
Epoch 259/1000, LR 0.000230
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.14s
Val loss: 0.4350 score: 0.8182 time: 0.04s
Test loss: 0.4758 score: 0.8140 time: 0.04s
Epoch 260/1000, LR 0.000230
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.13s
Val loss: 0.4339 score: 0.8182 time: 0.04s
Test loss: 0.4747 score: 0.8140 time: 0.04s
Epoch 261/1000, LR 0.000230
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.13s
Val loss: 0.4327 score: 0.8182 time: 0.04s
Test loss: 0.4736 score: 0.8140 time: 0.04s
Epoch 262/1000, LR 0.000229
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.13s
Val loss: 0.4317 score: 0.8182 time: 0.04s
Test loss: 0.4726 score: 0.8140 time: 0.04s
Epoch 263/1000, LR 0.000229
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.13s
Val loss: 0.4304 score: 0.8182 time: 0.05s
Test loss: 0.4713 score: 0.8140 time: 0.05s
Epoch 264/1000, LR 0.000229
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.15s
Val loss: 0.4292 score: 0.8182 time: 0.05s
Test loss: 0.4702 score: 0.8140 time: 0.05s
Epoch 265/1000, LR 0.000228
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.15s
Val loss: 0.4278 score: 0.8182 time: 0.04s
Test loss: 0.4688 score: 0.8140 time: 0.04s
Epoch 266/1000, LR 0.000228
Train loss: 0.6762;  Loss pred: 0.6762; Loss self: 0.0000; time: 0.13s
Val loss: 0.4266 score: 0.8182 time: 0.05s
Test loss: 0.4676 score: 0.8140 time: 0.05s
Epoch 267/1000, LR 0.000228
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.13s
Val loss: 0.4255 score: 0.8182 time: 0.04s
Test loss: 0.4666 score: 0.8140 time: 0.04s
Epoch 268/1000, LR 0.000228
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.13s
Val loss: 0.4242 score: 0.8182 time: 0.04s
Test loss: 0.4653 score: 0.8140 time: 0.04s
Epoch 269/1000, LR 0.000227
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.12s
Val loss: 0.4228 score: 0.8182 time: 0.04s
Test loss: 0.4639 score: 0.8140 time: 0.04s
Epoch 270/1000, LR 0.000227
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.13s
Val loss: 0.4216 score: 0.8182 time: 0.04s
Test loss: 0.4628 score: 0.8140 time: 0.04s
Epoch 271/1000, LR 0.000227
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.13s
Val loss: 0.4203 score: 0.8182 time: 0.04s
Test loss: 0.4615 score: 0.8140 time: 0.04s
Epoch 272/1000, LR 0.000226
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.13s
Val loss: 0.4189 score: 0.8182 time: 0.04s
Test loss: 0.4601 score: 0.8140 time: 0.04s
Epoch 273/1000, LR 0.000226
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.13s
Val loss: 0.4177 score: 0.8182 time: 0.04s
Test loss: 0.4590 score: 0.8140 time: 0.04s
Epoch 274/1000, LR 0.000226
Train loss: 0.6669;  Loss pred: 0.6669; Loss self: 0.0000; time: 0.13s
Val loss: 0.4164 score: 0.8182 time: 0.04s
Test loss: 0.4577 score: 0.8140 time: 0.04s
Epoch 275/1000, LR 0.000225
Train loss: 0.6675;  Loss pred: 0.6675; Loss self: 0.0000; time: 0.13s
Val loss: 0.4151 score: 0.8182 time: 0.04s
Test loss: 0.4564 score: 0.7907 time: 0.04s
Epoch 276/1000, LR 0.000225
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.13s
Val loss: 0.4138 score: 0.8182 time: 0.04s
Test loss: 0.4551 score: 0.7907 time: 0.04s
Epoch 277/1000, LR 0.000225
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.13s
Val loss: 0.4126 score: 0.8182 time: 0.04s
Test loss: 0.4540 score: 0.7907 time: 0.05s
Epoch 278/1000, LR 0.000224
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.14s
Val loss: 0.4114 score: 0.8182 time: 1.62s
Test loss: 0.4528 score: 0.7907 time: 1.47s
Epoch 279/1000, LR 0.000224
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 1.93s
Val loss: 0.4103 score: 0.8182 time: 0.09s
Test loss: 0.4517 score: 0.7907 time: 0.13s
Epoch 280/1000, LR 0.000224
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.28s
Val loss: 0.4092 score: 0.8182 time: 0.05s
Test loss: 0.4506 score: 0.7907 time: 0.04s
Epoch 281/1000, LR 0.000223
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.13s
Val loss: 0.4080 score: 0.8182 time: 0.04s
Test loss: 0.4496 score: 0.7907 time: 0.04s
Epoch 282/1000, LR 0.000223
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.12s
Val loss: 0.4070 score: 0.8182 time: 0.04s
Test loss: 0.4486 score: 0.7907 time: 0.04s
Epoch 283/1000, LR 0.000223
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.12s
Val loss: 0.4056 score: 0.8182 time: 0.04s
Test loss: 0.4472 score: 0.7907 time: 0.05s
Epoch 284/1000, LR 0.000222
Train loss: 0.6555;  Loss pred: 0.6555; Loss self: 0.0000; time: 0.14s
Val loss: 0.4043 score: 0.8182 time: 0.04s
Test loss: 0.4459 score: 0.7907 time: 0.05s
Epoch 285/1000, LR 0.000222
Train loss: 0.6561;  Loss pred: 0.6561; Loss self: 0.0000; time: 0.14s
Val loss: 0.4030 score: 0.8409 time: 0.04s
Test loss: 0.4447 score: 0.7907 time: 0.05s
Epoch 286/1000, LR 0.000222
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.14s
Val loss: 0.4017 score: 0.8409 time: 0.04s
Test loss: 0.4434 score: 0.7907 time: 0.05s
Epoch 287/1000, LR 0.000221
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.14s
Val loss: 0.4005 score: 0.8409 time: 0.04s
Test loss: 0.4423 score: 0.7907 time: 0.05s
Epoch 288/1000, LR 0.000221
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.14s
Val loss: 0.3995 score: 0.8409 time: 0.04s
Test loss: 0.4413 score: 0.7907 time: 0.05s
Epoch 289/1000, LR 0.000221
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.14s
Val loss: 0.3984 score: 0.8409 time: 0.04s
Test loss: 0.4403 score: 0.7907 time: 0.05s
Epoch 290/1000, LR 0.000220
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.14s
Val loss: 0.3974 score: 0.8409 time: 0.04s
Test loss: 0.4394 score: 0.7907 time: 0.05s
Epoch 291/1000, LR 0.000220
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.14s
Val loss: 0.3964 score: 0.8409 time: 0.05s
Test loss: 0.4385 score: 0.7907 time: 0.05s
Epoch 292/1000, LR 0.000220
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.14s
Val loss: 0.3954 score: 0.8409 time: 0.04s
Test loss: 0.4374 score: 0.7907 time: 0.05s
Epoch 293/1000, LR 0.000219
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.14s
Val loss: 0.3942 score: 0.8409 time: 0.04s
Test loss: 0.4364 score: 0.7907 time: 0.05s
Epoch 294/1000, LR 0.000219
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.14s
Val loss: 0.3932 score: 0.8636 time: 0.04s
Test loss: 0.4354 score: 0.7907 time: 0.05s
Epoch 295/1000, LR 0.000219
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.14s
Val loss: 0.3922 score: 0.8636 time: 0.04s
Test loss: 0.4345 score: 0.7907 time: 0.05s
Epoch 296/1000, LR 0.000218
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.14s
Val loss: 0.3915 score: 0.8636 time: 0.04s
Test loss: 0.4338 score: 0.7907 time: 0.05s
Epoch 297/1000, LR 0.000218
Train loss: 0.6418;  Loss pred: 0.6418; Loss self: 0.0000; time: 0.14s
Val loss: 0.3911 score: 0.8636 time: 0.04s
Test loss: 0.4335 score: 0.7907 time: 0.05s
Epoch 298/1000, LR 0.000218
Train loss: 0.6417;  Loss pred: 0.6417; Loss self: 0.0000; time: 0.14s
Val loss: 0.3905 score: 0.8636 time: 0.04s
Test loss: 0.4329 score: 0.7907 time: 0.05s
Epoch 299/1000, LR 0.000217
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.12s
Val loss: 0.3901 score: 0.8636 time: 0.04s
Test loss: 0.4325 score: 0.7907 time: 0.04s
Epoch 300/1000, LR 0.000217
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.12s
Val loss: 0.3896 score: 0.8636 time: 0.04s
Test loss: 0.4321 score: 0.7907 time: 0.04s
Epoch 301/1000, LR 0.000217
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.12s
Val loss: 0.3889 score: 0.8636 time: 0.04s
Test loss: 0.4314 score: 0.7907 time: 0.04s
Epoch 302/1000, LR 0.000216
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.12s
Val loss: 0.3882 score: 0.8636 time: 0.04s
Test loss: 0.4308 score: 0.7907 time: 0.04s
Epoch 303/1000, LR 0.000216
Train loss: 0.6329;  Loss pred: 0.6329; Loss self: 0.0000; time: 0.12s
Val loss: 0.3874 score: 0.8636 time: 0.04s
Test loss: 0.4300 score: 0.7907 time: 0.04s
Epoch 304/1000, LR 0.000216
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.13s
Val loss: 0.3866 score: 0.8636 time: 0.04s
Test loss: 0.4293 score: 0.7907 time: 0.04s
Epoch 305/1000, LR 0.000215
Train loss: 0.6326;  Loss pred: 0.6326; Loss self: 0.0000; time: 0.12s
Val loss: 0.3857 score: 0.8636 time: 0.04s
Test loss: 0.4284 score: 0.8140 time: 0.04s
Epoch 306/1000, LR 0.000215
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.12s
Val loss: 0.3847 score: 0.8636 time: 0.04s
Test loss: 0.4274 score: 0.8140 time: 0.04s
Epoch 307/1000, LR 0.000215
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 0.12s
Val loss: 0.3838 score: 0.8636 time: 0.04s
Test loss: 0.4265 score: 0.8140 time: 0.04s
Epoch 308/1000, LR 0.000214
Train loss: 0.6292;  Loss pred: 0.6292; Loss self: 0.0000; time: 0.12s
Val loss: 0.3829 score: 0.8636 time: 0.04s
Test loss: 0.4256 score: 0.8140 time: 0.04s
Epoch 309/1000, LR 0.000214
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.12s
Val loss: 0.3815 score: 0.8636 time: 0.04s
Test loss: 0.4243 score: 0.8140 time: 0.04s
Epoch 310/1000, LR 0.000214
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.13s
Val loss: 0.3803 score: 0.8636 time: 0.04s
Test loss: 0.4231 score: 0.8140 time: 0.04s
Epoch 311/1000, LR 0.000213
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.13s
Val loss: 0.3792 score: 0.8636 time: 0.04s
Test loss: 0.4221 score: 0.8140 time: 0.04s
Epoch 312/1000, LR 0.000213
Train loss: 0.6267;  Loss pred: 0.6267; Loss self: 0.0000; time: 0.13s
Val loss: 0.3783 score: 0.8636 time: 0.04s
Test loss: 0.4212 score: 0.8140 time: 0.04s
Epoch 313/1000, LR 0.000213
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.12s
Val loss: 0.3777 score: 0.8636 time: 0.04s
Test loss: 0.4205 score: 0.8140 time: 0.04s
Epoch 314/1000, LR 0.000212
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 0.13s
Val loss: 0.3773 score: 0.8636 time: 0.04s
Test loss: 0.4202 score: 0.8140 time: 0.04s
Epoch 315/1000, LR 0.000212
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.12s
Val loss: 0.3771 score: 0.8636 time: 0.04s
Test loss: 0.4200 score: 0.8140 time: 0.04s
Epoch 316/1000, LR 0.000212
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 0.13s
Val loss: 0.3772 score: 0.8636 time: 0.04s
Test loss: 0.4201 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 317/1000, LR 0.000211
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 0.14s
Val loss: 0.3773 score: 0.8636 time: 0.04s
Test loss: 0.4202 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 314,   Train_Loss: 0.6220,   Val_Loss: 0.3771,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.3771,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4200


[0.0463027679361403, 0.046259362949058414, 0.04596019291784614, 0.04561654897406697, 0.04588835802860558, 0.04604867403395474, 0.04535726201720536, 0.046053358004428446, 0.04566571593750268, 0.04678334796335548, 0.0465407760348171, 0.04644067899789661, 0.04653931793291122, 0.046527687925845385, 0.04572255304083228, 0.045986766926944256, 0.046098260092549026, 0.0463720120023936, 0.04664589499589056, 0.046330520999617875, 0.04633354092948139, 0.04547141108196229, 0.04596649203449488, 0.04645781894214451, 0.04668607795611024, 0.05057964602019638, 0.05079427093733102, 0.047762421891093254, 0.047230051015503705, 0.04673698404803872, 0.04687817906960845, 0.04727379302494228, 0.04702134197577834, 0.0471195459831506, 0.04775088396854699, 0.04753599606920034, 0.04728932003490627, 0.046841329080052674, 0.047027983935549855, 0.047536334954202175, 0.047799917054362595, 0.047053631977178156, 0.046999391983263195, 0.047840524930506945, 0.04790025600232184, 0.04737738100811839, 0.047814025077968836, 0.047541448031552136, 0.04732265800703317, 0.047160877962596714, 0.046449188026599586, 0.04721484798938036, 0.04671189503278583, 0.06361852795816958, 0.047425522934645414, 0.047313392045907676, 0.04721049196086824, 0.04685039899777621, 0.04980898497160524, 0.05127816600725055, 0.05152676301077008, 0.051272177952341735, 0.04716491897124797, 0.047243195003829896, 0.046901218011043966, 0.04702739999629557, 0.046948243980295956, 0.04762987594585866, 0.047184247989207506, 0.04701687302440405, 0.047256725025363266, 0.0490265159169212, 0.046590261976234615, 0.046888976939953864, 0.047174846986308694, 0.048528098966926336, 0.047574181109666824, 0.04730584588833153, 0.04743338201660663, 0.047136443899944425, 0.04698418499901891, 0.04718546592630446, 0.0469494319986552, 0.04671340307686478, 0.04715319792740047, 0.0470925469417125, 0.0470671399962157, 0.047144121024757624, 0.04679153405595571, 0.04695767001248896, 0.04670009994879365, 0.0470132710179314, 0.04667257994879037, 0.04706395196262747, 0.04671443591360003, 0.047278532991185784, 0.04699041193816811, 0.04676895996090025, 0.047454954008571804, 0.04698239604476839, 0.04608870402444154, 0.04710882203653455, 0.04713064501993358, 0.046796797891147435, 0.05063085397705436, 0.04685074998997152, 0.04641677695326507, 0.04634670598898083, 0.04668514896184206, 0.046939651016145945, 0.052236813004128635, 0.05494206794537604, 0.046938139013946056, 0.0467534699710086, 0.04656854702625424, 0.0465717549668625, 0.04637136100791395, 0.04656023404095322, 0.046699830098077655, 0.04668600100558251, 0.046432727947831154, 0.04646608093753457, 0.04644594504497945, 0.04642171005252749, 0.046848146012052894, 0.046576212043873966, 0.04721202305518091, 0.046979557955637574, 0.04815361101645976, 0.04746992595028132, 0.047241403022781014, 0.04717558994889259, 0.16428952803835273, 0.056028797989711165, 0.05017605097964406, 0.0468361999373883, 0.0466393840033561, 0.04617550491821021, 0.05103085096925497, 0.04634332796558738, 0.04620699991937727, 0.04657524800859392, 0.04611333308275789, 0.04628169909119606, 0.05145859997719526, 0.04609872796572745, 0.04659191099926829, 0.046515663038007915, 0.046609458047896624, 0.04640127893071622, 0.046144252992235124, 0.05090018908958882, 0.04672811599448323, 0.045862813014537096, 0.04715293901972473, 0.04668958799447864, 0.05546276597306132, 0.051583608030341566, 0.05106227193027735, 0.051681061973795295, 0.048451484995894134, 0.047402694006450474, 0.04699014697689563, 0.04775575397070497, 0.04809246794320643, 0.04821207490749657, 0.04988165001850575, 0.048276557005010545, 0.04718915105331689, 0.047230490017682314, 0.04721697105560452, 0.046438405057415366, 0.046736242016777396, 0.04710893810261041, 0.046841425937600434, 0.04760952095966786, 0.047559803002513945, 0.047111898893490434, 0.04663799598347396, 0.047316470998339355, 0.04721726605203003, 0.046757134958170354, 0.36463007901329547, 0.4745790039887652, 0.14272007998079062, 0.04897517501376569, 0.04866371292155236, 0.048898293054662645, 0.048375944956205785, 0.04876545106526464, 0.04521785001270473, 0.045973419095389545, 0.0453291010344401, 0.048166429973207414, 0.04871960496529937, 0.048597855027765036, 0.048663357039913535, 0.048332020989619195, 0.04937621089629829, 0.048104844987392426, 0.04955346800852567, 0.049893092014826834, 0.0494749469216913, 0.049541119020432234, 0.04636526992544532, 0.04635611898265779, 0.045787760987877846, 0.047118671936914325, 0.04852792399469763, 0.046123400097712874, 0.0467719939770177, 0.04891908704303205, 0.050555179943330586, 0.050426954054273665, 0.05029647995252162, 0.04841684200800955, 0.04791560291778296, 0.048152952920645475, 0.04839079990051687, 0.048848535981960595, 0.049190302030183375, 0.04834908898919821, 0.0482932860031724, 0.052170187002047896, 0.053863622015342116, 0.051177115994505584, 0.04841795901302248, 0.04851947200950235, 0.048629804980009794, 0.052083446993492544, 0.048119317973032594, 0.04863428708631545, 0.04883611900731921, 0.048260307987220585, 0.052099525928497314, 0.04861650103703141, 0.048207483952865005, 0.04860588803421706, 0.04999630304519087, 0.05330427596345544, 0.06785119906999171, 0.05733479594346136, 0.045539845013991, 0.04792778997216374, 0.04702035104855895, 0.047172547900117934, 0.05654309701640159, 0.050458219018764794, 0.05267890903633088, 0.04638269101269543, 0.051501031033694744, 0.05142601998522878, 0.05087344103958458, 0.05135481804609299, 0.05157930497080088, 0.051829651929438114, 0.05100489396136254, 0.051311254035681486, 0.051697005052119493, 0.05162557808216661, 0.0516647930489853, 0.05112609907519072, 0.05122573603875935, 0.05197352007962763, 0.04708845796994865, 0.046813593013212085, 0.04668490495532751, 0.05233603506349027, 0.05217173905111849, 0.046070221927948296, 0.04630429099779576, 0.047210165997967124, 0.04668897402007133, 0.05169049301184714, 0.052486656000837684, 0.047463444992899895, 0.05297105200588703, 0.04663898597937077, 0.04659103904850781, 0.04628975293599069, 0.046531197032891214, 0.048409126000478864, 0.04781432100571692, 0.047092381049878895, 0.04670244094450027, 0.047225925023667514, 0.0475758999818936, 0.05184516799636185, 1.4765326670603827, 0.13192861701827496, 0.04866807593498379, 0.04731182008981705, 0.0463159199571237, 0.05033052701037377, 0.051074599963612854, 0.05015300295781344, 0.050315883941948414, 0.050427659996785223, 0.05042098101694137, 0.05058637110050768, 0.05063118599355221, 0.050399707979522645, 0.05076727794948965, 0.05184581899084151, 0.05085575405973941, 0.05051971203647554, 0.05107190599665046, 0.05083005700726062, 0.05068147007841617, 0.0454188899602741, 0.04565943800844252, 0.04559330805204809, 0.04689582402352244, 0.04617736302316189, 0.04597969294991344, 0.046665312023833394, 0.046254192013293505, 0.046051869983784854, 0.046245359000749886, 0.047438922920264304, 0.046767390915192664, 0.04634408303536475, 0.045551762101240456, 0.046476967982016504, 0.046504706027917564, 0.04641953809186816, 0.050127913942560554, 0.05077736999373883]
[0.0010523356349122796, 0.0010513491579331458, 0.0010445498390419577, 0.001036739749410613, 0.0010429172279228542, 0.0010465607734989715, 0.0010308468640273945, 0.0010466672273733739, 0.0010378571803977882, 0.0010632579082580792, 0.001057744909882207, 0.0010800157906487584, 0.0010823097193700282, 0.0010820392540894276, 0.0010633151869960996, 0.0010694596959754479, 0.0010720525602918379, 0.0010784188837765954, 0.001084788255718385, 0.0010774539767352993, 0.0010775242076623579, 0.0010574746763247044, 0.001068988186848718, 0.0010804143940033608, 0.0010857227431653545, 0.0011762708376789855, 0.0011812621148216517, 0.001110753997467285, 0.0010983732794303187, 0.0010869066057683423, 0.0010901902109211267, 0.001099390535463774, 0.0010935195808320544, 0.0010958033949569907, 0.0011104856736871393, 0.0011054882806790776, 0.0010997516287187504, 0.0010893332344198295, 0.0010936740450127873, 0.0011054961617256319, 0.0011116259780084325, 0.0010942705110971664, 0.0010930091158898418, 0.0011125703472210917, 0.0011139594419144613, 0.0011017995583283346, 0.0011119540715806705, 0.0011056150705012124, 0.0011005269303961203, 0.0010967646037813189, 0.0010802136750371998, 0.0010980197206832641, 0.0010863231402973449, 0.0014795006501899903, 0.0011029191380150097, 0.0011003114429280856, 0.0010979184176946103, 0.0010895441627389816, 0.0011583484877117497, 0.0011925154885407104, 0.0011982968142039555, 0.0011923762314498079, 0.001096858580726697, 0.0010986789535774395, 0.0010907260002568364, 0.0010936604650301296, 0.001091819627448743, 0.00110767153362462, 0.0010973080927722675, 0.0010934156517303267, 0.0010989936052410061, 0.0011401515329516557, 0.0010834944645635957, 0.0010904413241849735, 0.0010970894647978765, 0.0011285604410913102, 0.0011063763048759726, 0.001100135950891431, 0.0011031019073629448, 0.0010961963697661494, 0.001092655465093463, 0.0010973364168908014, 0.001091847255782679, 0.0010863582110898786, 0.0010965859983116388, 0.0010951755102723837, 0.0010945846510747837, 0.0010963749075525028, 0.0010881752106036212, 0.0010920388374997432, 0.001086048836018457, 0.0010933318841379396, 0.0010854088360183808, 0.0010945105107587783, 0.001086382230548838, 0.0010995007672368786, 0.0010928002776318165, 0.001087650231648843, 0.001103603581594693, 0.0010926138615062417, 0.0010718303261498033, 0.0010955540008496406, 0.0010960615120914786, 0.0010882976253755218, 0.001177461720396613, 0.0010895523253481748, 0.0010794599291456992, 0.0010778303718367635, 0.0010857011386474899, 0.0010916197910731615, 0.0012148096047471775, 0.0012777225103575824, 0.0010915846282313036, 0.0010872899993257814, 0.0010829894657268426, 0.0010830640689968023, 0.0010784037443700917, 0.0010827961404872842, 0.0010860425604204106, 0.001085720953618198, 0.0010798308825077012, 0.0010806065334310366, 0.0010801382568599873, 0.00107957465238436, 0.0010894917677221603, 0.0010831677219505574, 0.001097954024539091, 0.001092547859433432, 0.0011198514189874364, 0.0011039517662856122, 0.0010986372795995585, 0.001097106742997502, 0.0038206866985663426, 0.0013029953020863062, 0.0011668849065033502, 0.001089213952032286, 0.0010846368372873511, 0.001073848951586284, 0.0011867639760291854, 0.0010777518131531948, 0.00107458139347389, 0.00108314530252544, 0.001072403094947858, 0.0010763185835161873, 0.001196711627376634, 0.001072063441063429, 0.001083532813936472, 0.001081759605535068, 0.0010839408848348053, 0.001079099510016656, 0.001073122162610119, 0.0011837253276648562, 0.0010867003719647263, 0.0010665770468496999, 0.0010965799772029007, 0.0010858043719646196, 0.0012898317668153796, 0.0011996187914032922, 0.0011874946960529617, 0.001201885162181286, 0.0011267787208347473, 0.0011023882327081506, 0.0010927941157417589, 0.0011105989295512784, 0.0011184294870513123, 0.0011212110443603854, 0.0011600383725233895, 0.0011227106280235011, 0.0010974221175189974, 0.0010983834887833097, 0.001098069094316384, 0.0010799629083119852, 0.0010868893492273813, 0.0010955567000607072, 0.0010893354869209403, 0.001107198161852741, 0.001106041930291022, 0.0010956255556625683, 0.0010846045577552082, 0.0011003830464730083, 0.0010980759546983727, 0.001087375231585357, 0.008479769279378964, 0.01103672102299454, 0.0033190716274602468, 0.0011389575584596673, 0.0011317142539895898, 0.001137169605922387, 0.0011250219757257159, 0.0011340802573317358, 0.001051577907272203, 0.001069149281288129, 0.0010541651403358162, 0.0011201495342606374, 0.0011330140689604503, 0.0011301826750643033, 0.0011317059776724079, 0.0011240004881306789, 0.0011482839743325183, 0.0011187173252881959, 0.0011524062327564108, 0.0011603044654610892, 0.0011505801609695652, 0.0011521190469867962, 0.001078262091289426, 0.0010780492786664603, 0.00106483165088088, 0.0010957830683003332, 0.0011285563719697124, 0.001072637211574718, 0.0010877207901632024, 0.001137653187047257, 0.001175701859147223, 0.0011727198617272946, 0.0011696855802912005, 0.0011259730699537105, 0.0011143163469251852, 0.0011198361144336157, 0.0011253674395469038, 0.0011360124646967581, 0.001143960512329846, 0.0011243974183534468, 0.0011230996744923812, 0.0012132601628383232, 0.0012526423724498166, 0.0011901654882443158, 0.0011259990468144763, 0.0011283598141744733, 0.0011309256972095302, 0.0012112429533370358, 0.0011190539063495953, 0.0011310299322398942, 0.001135723697844633, 0.0011223327438888509, 0.0012116168820580772, 0.001130616303186777, 0.0011211042779736048, 0.0011303694891678386, 0.001162704721981183, 0.001239634324731522, 0.0015779348620928305, 0.0013333673475223573, 0.0010590661631160697, 0.0011145997667945055, 0.0010934965360129988, 0.0010970359976771613, 0.0013149557445674789, 0.0011734469539247626, 0.0012250909078216484, 0.0010786672328533821, 0.001197698396132436, 0.0011959539531448552, 0.001183103279990339, 0.0011942980940951857, 0.0011995187202511832, 0.0012053407425450724, 0.0011861603246828497, 0.001193284977573988, 0.0012022559314446395, 0.0012005948391201537, 0.0012015068150926815, 0.0011889790482602495, 0.001191296186947892, 0.0012086865134797124, 0.0010950804179057825, 0.0010886882096095834, 0.001085695464077384, 0.0012171170944997737, 0.0012132962570027557, 0.001071400509952286, 0.0010768439766929247, 0.0010979108371620262, 0.001085790093490031, 0.001202104488647608, 0.0012206199069962251, 0.0011038010463465093, 0.0012318849303694659, 0.0010846275809155994, 0.0010835125360118096, 0.0010765058822323416, 0.0010821208612300283, 0.001125793627918113, 0.0011119609536213238, 0.001095171652322765, 0.001086103277779076, 0.0010982773261318027, 0.0011064162786486884, 0.0012057015813107406, 0.03433796900140425, 0.0030681073725180225, 0.0011318157194182277, 0.0011002748858096989, 0.0010771144176075279, 0.0011704773723342737, 0.0011877813945026245, 0.0011663489059956615, 0.0011701368358592655, 0.0011727362789950052, 0.0011725809538823573, 0.0011764272348955274, 0.0011774694417105165, 0.001172086232081922, 0.001180634370918364, 0.0012057167207172445, 0.0011826919548776607, 0.0011748770241040824, 0.0011877187441081502, 0.0011820943490060609, 0.0011786388390329341, 0.0010562532548900954, 0.0010618473955451748, 0.0010603094895825136, 0.0010906005586865683, 0.0010738921633293463, 0.0010692951848817079, 0.0010852398145077533, 0.0010756788840300816, 0.001070973720553136, 0.0010754734651337182, 0.001103230765587542, 0.001087613742213783, 0.0010777693729154593, 0.0010593433046800105, 0.0010808597205120118, 0.0010815047913469201, 0.0010795241416713527, 0.001165765440524664, 0.0011808690696218332]
[950.2671646041501, 951.1587967273466, 957.3502025687753, 964.5622255426209, 958.8488647288616, 955.510683490167, 970.0761916208587, 955.4135009171101, 963.5237091260684, 940.5055840480757, 945.4075275213213, 925.9123881876825, 923.9499397474341, 924.1808892058483, 940.45492082647, 935.0516001333795, 932.7900860828844, 927.2834656771082, 921.8388885836201, 928.1138884743958, 928.053395820644, 945.649122753029, 935.4640325333351, 925.570786126429, 921.0454568581337, 850.1443442848565, 846.5521643779977, 900.2893550508721, 910.4372973445413, 920.0422508179464, 917.2711238666113, 909.5948780186229, 914.4783664862263, 912.5724601713331, 900.506889638392, 904.577658105721, 909.296220970398, 917.9927394142089, 914.3492108641088, 904.571209400712, 899.5831509727585, 913.8508164652574, 914.9054527197414, 898.8195690256684, 897.6987512951014, 907.6060998946248, 899.3177196414911, 904.4739228696171, 908.6556379316071, 911.7726780681076, 925.7427702584515, 910.7304551668075, 920.536406622327, 675.9037245922027, 906.6847836186437, 908.8335910956788, 910.8144866535552, 917.815022280622, 863.2980580614751, 838.5635319703118, 834.5177823612203, 838.6614674330618, 911.6945589626279, 910.1839957376737, 916.8205394980289, 914.3605643388149, 915.9022011142094, 902.7947091207736, 911.3210834648764, 914.5652876081509, 909.9234019480057, 877.076398267143, 922.9396482452495, 917.0598892585331, 911.502691518632, 886.0845760578023, 903.8516059977462, 908.9785668668571, 906.5345579816661, 912.2453125924227, 915.201572633385, 911.2975607183484, 915.8790249311573, 920.5066890383785, 911.9211822325402, 913.0956550985035, 913.5885461376513, 912.0967591572797, 918.9696569592782, 915.7183477920355, 920.768907286049, 914.6353586756238, 921.3118290692316, 913.6504311016089, 920.4863370186038, 909.5036854890689, 915.0802946052303, 919.4132184241179, 906.1224670501819, 915.2364208719014, 932.98349151229, 912.7801999942174, 912.3575538126721, 918.8662886725915, 849.2845097870041, 917.8081462773642, 926.3891812931073, 927.7897766935902, 921.0637848697002, 916.0698699103917, 823.1742621166688, 782.6425471052717, 916.099378955438, 919.7178311398899, 923.3700157266583, 923.3064124509816, 927.2964835485727, 923.534876611193, 920.7742278653396, 921.0469749777507, 926.0709396250002, 925.4062131429998, 925.8074081248145, 926.2907366261234, 917.8591611488134, 923.2180573099153, 910.784948777604, 915.2917113567685, 892.9756064462504, 905.8366774162868, 910.2185212252122, 911.4883363744641, 261.7330545253123, 767.4624754201633, 856.9825476589357, 918.0932709631306, 921.9675799514373, 931.2296655156252, 842.6275318416032, 927.8574044559339, 930.5949331276021, 923.2371664895005, 932.4851865040745, 929.09294266121, 835.6232003796482, 932.7806188485021, 922.9069827308714, 924.419801666908, 922.5595362171454, 926.6985951875419, 931.8603555514438, 844.7905748309935, 920.2168562729262, 937.5787740357384, 911.926189415521, 920.9762143346614, 775.2949072335379, 833.5981456494344, 842.1090244224557, 832.0262463221635, 887.4856983979727, 907.1214390082689, 915.0854544281905, 900.4150583901928, 894.1109042434618, 891.8927484971998, 862.0404494247342, 890.7014639742661, 911.2263950545803, 910.4288349306033, 910.689505037533, 925.9577271621581, 920.0568583277158, 912.7779511042996, 917.9908412114147, 903.1806901906702, 904.1248551371644, 912.7205867293414, 921.9950191521293, 908.7744519558348, 910.6838153784062, 919.6457404515579, 117.92773683498584, 90.60662110753208, 301.2890688247062, 877.9958415240737, 883.6152734444561, 879.3762995352612, 888.871525691693, 881.7718089483368, 950.9518915189116, 935.3230811652263, 948.6179742971188, 892.7379509737126, 882.601573445163, 884.8127139651152, 883.6217354411356, 889.6793289325847, 870.8647184432633, 893.8808556865669, 867.7495587715849, 861.8427574547113, 869.1267535478146, 867.9658604858223, 927.4183040267721, 927.6013812995574, 939.115586179986, 912.5893882911496, 886.0877709233629, 932.2816598278548, 919.3535777227899, 879.0025039137529, 850.5557699171578, 852.7185670131859, 854.9306043005538, 888.1207079323049, 897.4112268561556, 892.987810547416, 888.5986610760842, 880.2720314049859, 874.15604753992, 889.3652579391255, 890.3929212266756, 824.2255293873505, 798.3124489428542, 840.2192887269481, 888.1002189380748, 886.2421254620953, 884.2313889121284, 825.5981983176448, 893.6120005711302, 884.1498986853493, 880.495847623671, 891.0013589508479, 825.3434025295023, 884.473359513197, 891.9776863286074, 884.6664825818904, 860.063592324676, 806.6895051624023, 633.7397214696748, 749.9808675067562, 944.2280707540684, 897.1830335797712, 914.4976385989326, 911.5471161542347, 760.481867265369, 852.1902048110106, 816.2659551348024, 927.0699707403877, 834.9347408572672, 836.1525938105068, 845.2347457004479, 837.3118946971206, 833.6676894801581, 829.6409178772998, 843.0563551915931, 838.0227848280243, 831.7696539025537, 832.9204552743554, 832.2882462575648, 841.0577137277824, 839.4218087459897, 827.344384873692, 913.1749446423186, 918.5366307573148, 921.068598964621, 821.6136348088946, 824.2010096283753, 933.3577786373597, 928.6396373512542, 910.8207753781587, 920.9883254559105, 831.8744414015295, 819.2558504644252, 905.9603660550222, 811.764131005386, 921.9754481587495, 922.9242549244494, 928.9312919742788, 924.1111929616782, 888.262266903448, 899.3121536716731, 913.0988716509288, 920.722753037681, 910.5168396055838, 903.8189506948877, 829.3926254230182, 29.12228151755583, 325.93383431013734, 883.5360587799724, 908.8637874926084, 928.4064753503037, 854.3522699680284, 841.9057619762964, 857.3763775654622, 854.6009059407749, 852.7066297095931, 852.8195828945111, 850.0313239422794, 849.2789405619685, 853.1795465456042, 847.0022765999465, 829.3822112752407, 845.5287075182999, 851.1529117377736, 841.9501712511013, 845.9561631783697, 848.4363206802974, 946.7426446927744, 941.7549114829055, 943.1208621868885, 916.9259927799044, 931.1921942886135, 935.1954578478965, 921.4553194895299, 929.6454684073123, 933.729727264942, 929.8230336865314, 906.4286740294403, 919.4440647325313, 927.8422871628961, 943.9810452212788, 925.1894404264525, 924.6376049380115, 926.3340775795613, 857.8054943453636, 846.8339341975012]
Elapsed: 0.05570966743568822~0.08437494358841124
Time per graph: 0.00129475752215209~0.0019623063469051054
Speed: 882.2583485157694~105.04145587572945
Total Time: 0.0519
best val loss: 0.3771126866340637 test_score: 0.8140

Testing...
Test loss: 0.4354 score: 0.7907 time: 0.05s
test Score 0.7907
Epoch Time List: [0.20359622710384429, 0.20337672717869282, 0.20284737402107567, 0.2023511049337685, 0.20266417390666902, 0.20245250896550715, 0.2015137701528147, 0.20231087296269834, 0.20180388388689607, 0.20334205694962293, 0.2040402750717476, 0.2096581329824403, 0.2099714840296656, 0.20804461010266095, 0.20766570407431573, 0.20810164418071508, 0.20810931804589927, 0.2076836300548166, 0.20820820110384375, 0.20911253313533962, 0.20840816700365394, 0.2058047541650012, 0.2085513409692794, 0.20885543711483479, 0.21053881908301264, 0.2161976199131459, 0.23405489197466522, 0.22237638290971518, 0.3164221770130098, 0.21334919112268835, 0.20956985012162477, 0.21051702881231904, 0.21148839895613492, 0.2116892390185967, 0.21267867705319077, 0.21255788602866232, 0.22992383502423763, 0.21057768096216023, 0.2110402489779517, 0.21214303793385625, 0.21338804287370294, 0.2127763129537925, 0.2114479609299451, 0.2971009351313114, 0.21822191902901977, 0.21616741688922048, 0.21325974294450134, 0.21309780806768686, 0.21743887197226286, 0.21176349488086998, 0.22785691393073648, 0.21146260190289468, 0.21104628790635616, 0.23955026594921947, 0.2365013089729473, 0.21240183792542666, 0.2113572359085083, 0.21148887590970844, 0.21766497602220625, 0.23364661808591336, 0.235454031964764, 0.23574492405168712, 0.2263159069698304, 0.2126787918386981, 0.21255663398187608, 0.21360436908435076, 0.2140819620108232, 0.21494629187509418, 0.21414038899820298, 0.21411508694291115, 0.21170845301821828, 0.21470517595298588, 0.21369695698376745, 0.21214432502165437, 0.21057835698593408, 0.22420798090752214, 0.21406974596902728, 0.21403264091350138, 0.21340467198751867, 0.21237208100501448, 0.2125616370467469, 0.21301789488643408, 0.21402289404068142, 0.2112297200364992, 0.21160783502273262, 0.21201159106567502, 0.21124601701740175, 0.2120880780275911, 0.21142675494775176, 0.2117872111266479, 0.21182618383318186, 0.21221130213234574, 0.21364209009334445, 0.21184493007604033, 0.21111911081243306, 0.212103397003375, 0.21203458099626005, 0.21123986307065934, 0.21193508803844452, 0.21126056800130755, 0.2106621318962425, 0.21209937008097768, 0.21258174208924174, 0.21228844195138663, 0.21930355404037982, 0.22825090889818966, 0.2103547250153497, 0.2107163000619039, 0.2112288409844041, 0.21120507700834423, 0.21606794313993305, 0.23471007822081447, 0.21356851293239743, 0.21028709411621094, 0.21003580885007977, 0.21081785194110125, 0.21040379290934652, 0.21057170606218278, 0.21068213693797588, 0.21353137295227498, 0.20966556412167847, 0.2101291810395196, 0.20992739393841475, 0.20978812675457448, 0.2100206819595769, 0.2099198839860037, 0.21536430914420635, 0.21246666193474084, 0.21338636300060898, 0.214204513002187, 0.21309705707244575, 0.21275412489194423, 5.127003434114158, 1.5452323151985183, 0.241386879933998, 0.21114692802075297, 0.21008727501612157, 0.20885504607576877, 0.21825050003826618, 0.22458055813331157, 0.20792489091400057, 0.20764894294552505, 0.20779922197107226, 0.20730779191944748, 0.2178076390409842, 0.22201754711568356, 0.21000314317643642, 0.20876079704612494, 0.20815361407585442, 0.20927119499538094, 0.20781506702769548, 0.21700917405541986, 0.2233183989301324, 0.2068215870531276, 0.2087684499565512, 0.2100120469694957, 0.21887176705058664, 0.21848010795656592, 0.23512165003921837, 0.2357441479107365, 0.2367474379716441, 0.21481451706495136, 0.21346107497811317, 0.2176540030632168, 0.2184490030631423, 0.21824951202142984, 0.22205006412696093, 0.2262484940001741, 0.215398233034648, 0.21375716698821634, 0.21472540602553636, 0.2141660099150613, 0.21179635007865727, 0.21581954800058156, 0.2125028430018574, 0.2132975549902767, 0.21283054701052606, 0.21468082594219595, 0.21230250783264637, 0.21333367004990578, 0.2142136157490313, 0.21240473084617406, 0.5296811461448669, 2.981146744918078, 2.1118258060887456, 0.33539960999041796, 0.22152943687979132, 0.2187063969904557, 0.2193378059891984, 0.2190395719371736, 0.20626813697163016, 0.20426946703810245, 0.2033132379874587, 0.20888091600500047, 0.21874432009644806, 0.21852767409291118, 0.21894438192248344, 0.2191854891134426, 0.2199814929626882, 0.21764020179398358, 0.21884227695409209, 0.22452199587132782, 0.22415406885556877, 0.22327521198894829, 0.21743988106027246, 0.20624816603958607, 0.20507452602032572, 0.20664430398028344, 0.2112148329615593, 0.21504357911180705, 0.21071124693844467, 0.21534804010298103, 0.22612456313800067, 0.22864222596399486, 0.22705569013487548, 0.22525552695151418, 0.21838498301804066, 0.2183153829537332, 0.21844365913420916, 0.21979904302861542, 0.2201390868285671, 0.22027178795542568, 0.22072151605971158, 0.22704323893412948, 0.2431087449658662, 0.241554235923104, 0.22279616200830787, 0.21923284092918038, 0.21951630502007902, 0.22788646793924272, 0.2314197290688753, 0.21885842306073755, 0.2202436758670956, 0.2190100859152153, 0.22745669598225504, 0.23090640211012214, 0.21909435105044395, 0.22005416091997176, 0.22593449789565057, 0.23170110897626728, 0.24933487991802394, 0.24350099102593958, 0.2258073149714619, 0.29051786090712994, 0.21230795595329255, 0.21372452203650028, 0.22276974318083376, 0.22740899305790663, 0.2292868250515312, 0.22500541002955288, 0.2916443300200626, 0.23369732801802456, 0.23268979298882186, 0.23503832786809653, 0.23488538688980043, 0.23429440008476377, 0.31700207490939647, 0.23359598603565246, 0.23321869899518788, 0.23398521298076957, 0.23547177202999592, 0.23467738192994148, 0.23298562306445092, 0.323114734957926, 0.2199867080198601, 0.2113051018677652, 0.21028613788075745, 0.22089106286875904, 0.23872679704800248, 0.2257065250305459, 0.20849499001633376, 0.2097292230464518, 0.20997620793059468, 0.22012886183802038, 0.24139239417854697, 0.23333325900603086, 0.22672194615006447, 0.21310047421138734, 0.20785360399167985, 0.20726806903257966, 0.20765986596234143, 0.21408926008734852, 0.20981329190544784, 0.21080682799220085, 0.2080618980107829, 0.20876112708356231, 0.2127843969501555, 0.21629762405063957, 3.224430836038664, 2.1488680409966037, 0.3718968409812078, 0.2138612010749057, 0.2061049350304529, 0.21431277401279658, 0.22894343396183103, 0.2272587699117139, 0.22851752501446754, 0.22811568598262966, 0.2275672028772533, 0.22839462384581566, 0.2284228369826451, 0.22827368206344545, 0.2278960649855435, 0.22981143987271935, 0.23019842396024615, 0.22842751594725996, 0.22879341198131442, 0.22918460192158818, 0.22850299696438015, 0.20323550491593778, 0.20292208704631776, 0.20172004599589854, 0.20419893506914377, 0.2048339070752263, 0.20776873093564063, 0.20634119503665715, 0.20690285204909742, 0.20633166108746082, 0.206418821006082, 0.209066248848103, 0.2083053330425173, 0.20783506287261844, 0.20700591313652694, 0.20570461393799633, 0.20757879107259214, 0.20737372594885528, 0.21449889009818435, 0.23266887897625566]
Total Epoch List: [11, 317]
Total Time List: [0.046794573077932, 0.05185602291021496]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72eb2ad70a60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.4937;  Loss pred: 2.4937; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.5236;  Loss pred: 2.5236; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.5388;  Loss pred: 2.5388; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.5059;  Loss pred: 2.5059; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.4541;  Loss pred: 2.4541; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.4239;  Loss pred: 2.4239; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.4740;  Loss pred: 2.4740; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.3579;  Loss pred: 2.3579; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 2.4216;  Loss pred: 2.4216; Loss self: 0.0000; time: 0.13s
Val loss: 0.6930 score: 0.5227 time: 0.04s
Test loss: 0.6931 score: 0.5116 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.3194;  Loss pred: 2.3194; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 11/1000, LR 0.000270
Train loss: 2.2627;  Loss pred: 2.2627; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 2.4216,   Val_Loss: 0.6930,   Val_Precision: 0.6000,   Val_Recall: 0.1364,   Val_accuracy: 0.2222,   Val_Score: 0.5227,   Val_Loss: 0.6930,   Test_Precision: 0.5000,   Test_Recall: 0.0952,   Test_accuracy: 0.1600,   Test_Score: 0.5116,   Test_loss: 0.6931


[0.0463027679361403, 0.046259362949058414, 0.04596019291784614, 0.04561654897406697, 0.04588835802860558, 0.04604867403395474, 0.04535726201720536, 0.046053358004428446, 0.04566571593750268, 0.04678334796335548, 0.0465407760348171, 0.04644067899789661, 0.04653931793291122, 0.046527687925845385, 0.04572255304083228, 0.045986766926944256, 0.046098260092549026, 0.0463720120023936, 0.04664589499589056, 0.046330520999617875, 0.04633354092948139, 0.04547141108196229, 0.04596649203449488, 0.04645781894214451, 0.04668607795611024, 0.05057964602019638, 0.05079427093733102, 0.047762421891093254, 0.047230051015503705, 0.04673698404803872, 0.04687817906960845, 0.04727379302494228, 0.04702134197577834, 0.0471195459831506, 0.04775088396854699, 0.04753599606920034, 0.04728932003490627, 0.046841329080052674, 0.047027983935549855, 0.047536334954202175, 0.047799917054362595, 0.047053631977178156, 0.046999391983263195, 0.047840524930506945, 0.04790025600232184, 0.04737738100811839, 0.047814025077968836, 0.047541448031552136, 0.04732265800703317, 0.047160877962596714, 0.046449188026599586, 0.04721484798938036, 0.04671189503278583, 0.06361852795816958, 0.047425522934645414, 0.047313392045907676, 0.04721049196086824, 0.04685039899777621, 0.04980898497160524, 0.05127816600725055, 0.05152676301077008, 0.051272177952341735, 0.04716491897124797, 0.047243195003829896, 0.046901218011043966, 0.04702739999629557, 0.046948243980295956, 0.04762987594585866, 0.047184247989207506, 0.04701687302440405, 0.047256725025363266, 0.0490265159169212, 0.046590261976234615, 0.046888976939953864, 0.047174846986308694, 0.048528098966926336, 0.047574181109666824, 0.04730584588833153, 0.04743338201660663, 0.047136443899944425, 0.04698418499901891, 0.04718546592630446, 0.0469494319986552, 0.04671340307686478, 0.04715319792740047, 0.0470925469417125, 0.0470671399962157, 0.047144121024757624, 0.04679153405595571, 0.04695767001248896, 0.04670009994879365, 0.0470132710179314, 0.04667257994879037, 0.04706395196262747, 0.04671443591360003, 0.047278532991185784, 0.04699041193816811, 0.04676895996090025, 0.047454954008571804, 0.04698239604476839, 0.04608870402444154, 0.04710882203653455, 0.04713064501993358, 0.046796797891147435, 0.05063085397705436, 0.04685074998997152, 0.04641677695326507, 0.04634670598898083, 0.04668514896184206, 0.046939651016145945, 0.052236813004128635, 0.05494206794537604, 0.046938139013946056, 0.0467534699710086, 0.04656854702625424, 0.0465717549668625, 0.04637136100791395, 0.04656023404095322, 0.046699830098077655, 0.04668600100558251, 0.046432727947831154, 0.04646608093753457, 0.04644594504497945, 0.04642171005252749, 0.046848146012052894, 0.046576212043873966, 0.04721202305518091, 0.046979557955637574, 0.04815361101645976, 0.04746992595028132, 0.047241403022781014, 0.04717558994889259, 0.16428952803835273, 0.056028797989711165, 0.05017605097964406, 0.0468361999373883, 0.0466393840033561, 0.04617550491821021, 0.05103085096925497, 0.04634332796558738, 0.04620699991937727, 0.04657524800859392, 0.04611333308275789, 0.04628169909119606, 0.05145859997719526, 0.04609872796572745, 0.04659191099926829, 0.046515663038007915, 0.046609458047896624, 0.04640127893071622, 0.046144252992235124, 0.05090018908958882, 0.04672811599448323, 0.045862813014537096, 0.04715293901972473, 0.04668958799447864, 0.05546276597306132, 0.051583608030341566, 0.05106227193027735, 0.051681061973795295, 0.048451484995894134, 0.047402694006450474, 0.04699014697689563, 0.04775575397070497, 0.04809246794320643, 0.04821207490749657, 0.04988165001850575, 0.048276557005010545, 0.04718915105331689, 0.047230490017682314, 0.04721697105560452, 0.046438405057415366, 0.046736242016777396, 0.04710893810261041, 0.046841425937600434, 0.04760952095966786, 0.047559803002513945, 0.047111898893490434, 0.04663799598347396, 0.047316470998339355, 0.04721726605203003, 0.046757134958170354, 0.36463007901329547, 0.4745790039887652, 0.14272007998079062, 0.04897517501376569, 0.04866371292155236, 0.048898293054662645, 0.048375944956205785, 0.04876545106526464, 0.04521785001270473, 0.045973419095389545, 0.0453291010344401, 0.048166429973207414, 0.04871960496529937, 0.048597855027765036, 0.048663357039913535, 0.048332020989619195, 0.04937621089629829, 0.048104844987392426, 0.04955346800852567, 0.049893092014826834, 0.0494749469216913, 0.049541119020432234, 0.04636526992544532, 0.04635611898265779, 0.045787760987877846, 0.047118671936914325, 0.04852792399469763, 0.046123400097712874, 0.0467719939770177, 0.04891908704303205, 0.050555179943330586, 0.050426954054273665, 0.05029647995252162, 0.04841684200800955, 0.04791560291778296, 0.048152952920645475, 0.04839079990051687, 0.048848535981960595, 0.049190302030183375, 0.04834908898919821, 0.0482932860031724, 0.052170187002047896, 0.053863622015342116, 0.051177115994505584, 0.04841795901302248, 0.04851947200950235, 0.048629804980009794, 0.052083446993492544, 0.048119317973032594, 0.04863428708631545, 0.04883611900731921, 0.048260307987220585, 0.052099525928497314, 0.04861650103703141, 0.048207483952865005, 0.04860588803421706, 0.04999630304519087, 0.05330427596345544, 0.06785119906999171, 0.05733479594346136, 0.045539845013991, 0.04792778997216374, 0.04702035104855895, 0.047172547900117934, 0.05654309701640159, 0.050458219018764794, 0.05267890903633088, 0.04638269101269543, 0.051501031033694744, 0.05142601998522878, 0.05087344103958458, 0.05135481804609299, 0.05157930497080088, 0.051829651929438114, 0.05100489396136254, 0.051311254035681486, 0.051697005052119493, 0.05162557808216661, 0.0516647930489853, 0.05112609907519072, 0.05122573603875935, 0.05197352007962763, 0.04708845796994865, 0.046813593013212085, 0.04668490495532751, 0.05233603506349027, 0.05217173905111849, 0.046070221927948296, 0.04630429099779576, 0.047210165997967124, 0.04668897402007133, 0.05169049301184714, 0.052486656000837684, 0.047463444992899895, 0.05297105200588703, 0.04663898597937077, 0.04659103904850781, 0.04628975293599069, 0.046531197032891214, 0.048409126000478864, 0.04781432100571692, 0.047092381049878895, 0.04670244094450027, 0.047225925023667514, 0.0475758999818936, 0.05184516799636185, 1.4765326670603827, 0.13192861701827496, 0.04866807593498379, 0.04731182008981705, 0.0463159199571237, 0.05033052701037377, 0.051074599963612854, 0.05015300295781344, 0.050315883941948414, 0.050427659996785223, 0.05042098101694137, 0.05058637110050768, 0.05063118599355221, 0.050399707979522645, 0.05076727794948965, 0.05184581899084151, 0.05085575405973941, 0.05051971203647554, 0.05107190599665046, 0.05083005700726062, 0.05068147007841617, 0.0454188899602741, 0.04565943800844252, 0.04559330805204809, 0.04689582402352244, 0.04617736302316189, 0.04597969294991344, 0.046665312023833394, 0.046254192013293505, 0.046051869983784854, 0.046245359000749886, 0.047438922920264304, 0.046767390915192664, 0.04634408303536475, 0.045551762101240456, 0.046476967982016504, 0.046504706027917564, 0.04641953809186816, 0.050127913942560554, 0.05077736999373883, 0.04312678205315024, 0.04276339500211179, 0.04343279195018113, 0.04384796996600926, 0.04733498895075172, 0.04303583700675517, 0.04326147900428623, 0.04339607595466077, 0.042956926045008004, 0.042993209906853735, 0.043256417964585125]
[0.0010523356349122796, 0.0010513491579331458, 0.0010445498390419577, 0.001036739749410613, 0.0010429172279228542, 0.0010465607734989715, 0.0010308468640273945, 0.0010466672273733739, 0.0010378571803977882, 0.0010632579082580792, 0.001057744909882207, 0.0010800157906487584, 0.0010823097193700282, 0.0010820392540894276, 0.0010633151869960996, 0.0010694596959754479, 0.0010720525602918379, 0.0010784188837765954, 0.001084788255718385, 0.0010774539767352993, 0.0010775242076623579, 0.0010574746763247044, 0.001068988186848718, 0.0010804143940033608, 0.0010857227431653545, 0.0011762708376789855, 0.0011812621148216517, 0.001110753997467285, 0.0010983732794303187, 0.0010869066057683423, 0.0010901902109211267, 0.001099390535463774, 0.0010935195808320544, 0.0010958033949569907, 0.0011104856736871393, 0.0011054882806790776, 0.0010997516287187504, 0.0010893332344198295, 0.0010936740450127873, 0.0011054961617256319, 0.0011116259780084325, 0.0010942705110971664, 0.0010930091158898418, 0.0011125703472210917, 0.0011139594419144613, 0.0011017995583283346, 0.0011119540715806705, 0.0011056150705012124, 0.0011005269303961203, 0.0010967646037813189, 0.0010802136750371998, 0.0010980197206832641, 0.0010863231402973449, 0.0014795006501899903, 0.0011029191380150097, 0.0011003114429280856, 0.0010979184176946103, 0.0010895441627389816, 0.0011583484877117497, 0.0011925154885407104, 0.0011982968142039555, 0.0011923762314498079, 0.001096858580726697, 0.0010986789535774395, 0.0010907260002568364, 0.0010936604650301296, 0.001091819627448743, 0.00110767153362462, 0.0010973080927722675, 0.0010934156517303267, 0.0010989936052410061, 0.0011401515329516557, 0.0010834944645635957, 0.0010904413241849735, 0.0010970894647978765, 0.0011285604410913102, 0.0011063763048759726, 0.001100135950891431, 0.0011031019073629448, 0.0010961963697661494, 0.001092655465093463, 0.0010973364168908014, 0.001091847255782679, 0.0010863582110898786, 0.0010965859983116388, 0.0010951755102723837, 0.0010945846510747837, 0.0010963749075525028, 0.0010881752106036212, 0.0010920388374997432, 0.001086048836018457, 0.0010933318841379396, 0.0010854088360183808, 0.0010945105107587783, 0.001086382230548838, 0.0010995007672368786, 0.0010928002776318165, 0.001087650231648843, 0.001103603581594693, 0.0010926138615062417, 0.0010718303261498033, 0.0010955540008496406, 0.0010960615120914786, 0.0010882976253755218, 0.001177461720396613, 0.0010895523253481748, 0.0010794599291456992, 0.0010778303718367635, 0.0010857011386474899, 0.0010916197910731615, 0.0012148096047471775, 0.0012777225103575824, 0.0010915846282313036, 0.0010872899993257814, 0.0010829894657268426, 0.0010830640689968023, 0.0010784037443700917, 0.0010827961404872842, 0.0010860425604204106, 0.001085720953618198, 0.0010798308825077012, 0.0010806065334310366, 0.0010801382568599873, 0.00107957465238436, 0.0010894917677221603, 0.0010831677219505574, 0.001097954024539091, 0.001092547859433432, 0.0011198514189874364, 0.0011039517662856122, 0.0010986372795995585, 0.001097106742997502, 0.0038206866985663426, 0.0013029953020863062, 0.0011668849065033502, 0.001089213952032286, 0.0010846368372873511, 0.001073848951586284, 0.0011867639760291854, 0.0010777518131531948, 0.00107458139347389, 0.00108314530252544, 0.001072403094947858, 0.0010763185835161873, 0.001196711627376634, 0.001072063441063429, 0.001083532813936472, 0.001081759605535068, 0.0010839408848348053, 0.001079099510016656, 0.001073122162610119, 0.0011837253276648562, 0.0010867003719647263, 0.0010665770468496999, 0.0010965799772029007, 0.0010858043719646196, 0.0012898317668153796, 0.0011996187914032922, 0.0011874946960529617, 0.001201885162181286, 0.0011267787208347473, 0.0011023882327081506, 0.0010927941157417589, 0.0011105989295512784, 0.0011184294870513123, 0.0011212110443603854, 0.0011600383725233895, 0.0011227106280235011, 0.0010974221175189974, 0.0010983834887833097, 0.001098069094316384, 0.0010799629083119852, 0.0010868893492273813, 0.0010955567000607072, 0.0010893354869209403, 0.001107198161852741, 0.001106041930291022, 0.0010956255556625683, 0.0010846045577552082, 0.0011003830464730083, 0.0010980759546983727, 0.001087375231585357, 0.008479769279378964, 0.01103672102299454, 0.0033190716274602468, 0.0011389575584596673, 0.0011317142539895898, 0.001137169605922387, 0.0011250219757257159, 0.0011340802573317358, 0.001051577907272203, 0.001069149281288129, 0.0010541651403358162, 0.0011201495342606374, 0.0011330140689604503, 0.0011301826750643033, 0.0011317059776724079, 0.0011240004881306789, 0.0011482839743325183, 0.0011187173252881959, 0.0011524062327564108, 0.0011603044654610892, 0.0011505801609695652, 0.0011521190469867962, 0.001078262091289426, 0.0010780492786664603, 0.00106483165088088, 0.0010957830683003332, 0.0011285563719697124, 0.001072637211574718, 0.0010877207901632024, 0.001137653187047257, 0.001175701859147223, 0.0011727198617272946, 0.0011696855802912005, 0.0011259730699537105, 0.0011143163469251852, 0.0011198361144336157, 0.0011253674395469038, 0.0011360124646967581, 0.001143960512329846, 0.0011243974183534468, 0.0011230996744923812, 0.0012132601628383232, 0.0012526423724498166, 0.0011901654882443158, 0.0011259990468144763, 0.0011283598141744733, 0.0011309256972095302, 0.0012112429533370358, 0.0011190539063495953, 0.0011310299322398942, 0.001135723697844633, 0.0011223327438888509, 0.0012116168820580772, 0.001130616303186777, 0.0011211042779736048, 0.0011303694891678386, 0.001162704721981183, 0.001239634324731522, 0.0015779348620928305, 0.0013333673475223573, 0.0010590661631160697, 0.0011145997667945055, 0.0010934965360129988, 0.0010970359976771613, 0.0013149557445674789, 0.0011734469539247626, 0.0012250909078216484, 0.0010786672328533821, 0.001197698396132436, 0.0011959539531448552, 0.001183103279990339, 0.0011942980940951857, 0.0011995187202511832, 0.0012053407425450724, 0.0011861603246828497, 0.001193284977573988, 0.0012022559314446395, 0.0012005948391201537, 0.0012015068150926815, 0.0011889790482602495, 0.001191296186947892, 0.0012086865134797124, 0.0010950804179057825, 0.0010886882096095834, 0.001085695464077384, 0.0012171170944997737, 0.0012132962570027557, 0.001071400509952286, 0.0010768439766929247, 0.0010979108371620262, 0.001085790093490031, 0.001202104488647608, 0.0012206199069962251, 0.0011038010463465093, 0.0012318849303694659, 0.0010846275809155994, 0.0010835125360118096, 0.0010765058822323416, 0.0010821208612300283, 0.001125793627918113, 0.0011119609536213238, 0.001095171652322765, 0.001086103277779076, 0.0010982773261318027, 0.0011064162786486884, 0.0012057015813107406, 0.03433796900140425, 0.0030681073725180225, 0.0011318157194182277, 0.0011002748858096989, 0.0010771144176075279, 0.0011704773723342737, 0.0011877813945026245, 0.0011663489059956615, 0.0011701368358592655, 0.0011727362789950052, 0.0011725809538823573, 0.0011764272348955274, 0.0011774694417105165, 0.001172086232081922, 0.001180634370918364, 0.0012057167207172445, 0.0011826919548776607, 0.0011748770241040824, 0.0011877187441081502, 0.0011820943490060609, 0.0011786388390329341, 0.0010562532548900954, 0.0010618473955451748, 0.0010603094895825136, 0.0010906005586865683, 0.0010738921633293463, 0.0010692951848817079, 0.0010852398145077533, 0.0010756788840300816, 0.001070973720553136, 0.0010754734651337182, 0.001103230765587542, 0.001087613742213783, 0.0010777693729154593, 0.0010593433046800105, 0.0010808597205120118, 0.0010815047913469201, 0.0010795241416713527, 0.001165765440524664, 0.0011808690696218332, 0.0010029484198407033, 0.0009944975581886463, 0.0010100649290739796, 0.0010197202317676572, 0.0011008136965291097, 0.0010008334187617482, 0.0010060809070764239, 0.0010092110687130412, 0.0009989982801164653, 0.0009998420908570635, 0.0010059632084787238]
[950.2671646041501, 951.1587967273466, 957.3502025687753, 964.5622255426209, 958.8488647288616, 955.510683490167, 970.0761916208587, 955.4135009171101, 963.5237091260684, 940.5055840480757, 945.4075275213213, 925.9123881876825, 923.9499397474341, 924.1808892058483, 940.45492082647, 935.0516001333795, 932.7900860828844, 927.2834656771082, 921.8388885836201, 928.1138884743958, 928.053395820644, 945.649122753029, 935.4640325333351, 925.570786126429, 921.0454568581337, 850.1443442848565, 846.5521643779977, 900.2893550508721, 910.4372973445413, 920.0422508179464, 917.2711238666113, 909.5948780186229, 914.4783664862263, 912.5724601713331, 900.506889638392, 904.577658105721, 909.296220970398, 917.9927394142089, 914.3492108641088, 904.571209400712, 899.5831509727585, 913.8508164652574, 914.9054527197414, 898.8195690256684, 897.6987512951014, 907.6060998946248, 899.3177196414911, 904.4739228696171, 908.6556379316071, 911.7726780681076, 925.7427702584515, 910.7304551668075, 920.536406622327, 675.9037245922027, 906.6847836186437, 908.8335910956788, 910.8144866535552, 917.815022280622, 863.2980580614751, 838.5635319703118, 834.5177823612203, 838.6614674330618, 911.6945589626279, 910.1839957376737, 916.8205394980289, 914.3605643388149, 915.9022011142094, 902.7947091207736, 911.3210834648764, 914.5652876081509, 909.9234019480057, 877.076398267143, 922.9396482452495, 917.0598892585331, 911.502691518632, 886.0845760578023, 903.8516059977462, 908.9785668668571, 906.5345579816661, 912.2453125924227, 915.201572633385, 911.2975607183484, 915.8790249311573, 920.5066890383785, 911.9211822325402, 913.0956550985035, 913.5885461376513, 912.0967591572797, 918.9696569592782, 915.7183477920355, 920.768907286049, 914.6353586756238, 921.3118290692316, 913.6504311016089, 920.4863370186038, 909.5036854890689, 915.0802946052303, 919.4132184241179, 906.1224670501819, 915.2364208719014, 932.98349151229, 912.7801999942174, 912.3575538126721, 918.8662886725915, 849.2845097870041, 917.8081462773642, 926.3891812931073, 927.7897766935902, 921.0637848697002, 916.0698699103917, 823.1742621166688, 782.6425471052717, 916.099378955438, 919.7178311398899, 923.3700157266583, 923.3064124509816, 927.2964835485727, 923.534876611193, 920.7742278653396, 921.0469749777507, 926.0709396250002, 925.4062131429998, 925.8074081248145, 926.2907366261234, 917.8591611488134, 923.2180573099153, 910.784948777604, 915.2917113567685, 892.9756064462504, 905.8366774162868, 910.2185212252122, 911.4883363744641, 261.7330545253123, 767.4624754201633, 856.9825476589357, 918.0932709631306, 921.9675799514373, 931.2296655156252, 842.6275318416032, 927.8574044559339, 930.5949331276021, 923.2371664895005, 932.4851865040745, 929.09294266121, 835.6232003796482, 932.7806188485021, 922.9069827308714, 924.419801666908, 922.5595362171454, 926.6985951875419, 931.8603555514438, 844.7905748309935, 920.2168562729262, 937.5787740357384, 911.926189415521, 920.9762143346614, 775.2949072335379, 833.5981456494344, 842.1090244224557, 832.0262463221635, 887.4856983979727, 907.1214390082689, 915.0854544281905, 900.4150583901928, 894.1109042434618, 891.8927484971998, 862.0404494247342, 890.7014639742661, 911.2263950545803, 910.4288349306033, 910.689505037533, 925.9577271621581, 920.0568583277158, 912.7779511042996, 917.9908412114147, 903.1806901906702, 904.1248551371644, 912.7205867293414, 921.9950191521293, 908.7744519558348, 910.6838153784062, 919.6457404515579, 117.92773683498584, 90.60662110753208, 301.2890688247062, 877.9958415240737, 883.6152734444561, 879.3762995352612, 888.871525691693, 881.7718089483368, 950.9518915189116, 935.3230811652263, 948.6179742971188, 892.7379509737126, 882.601573445163, 884.8127139651152, 883.6217354411356, 889.6793289325847, 870.8647184432633, 893.8808556865669, 867.7495587715849, 861.8427574547113, 869.1267535478146, 867.9658604858223, 927.4183040267721, 927.6013812995574, 939.115586179986, 912.5893882911496, 886.0877709233629, 932.2816598278548, 919.3535777227899, 879.0025039137529, 850.5557699171578, 852.7185670131859, 854.9306043005538, 888.1207079323049, 897.4112268561556, 892.987810547416, 888.5986610760842, 880.2720314049859, 874.15604753992, 889.3652579391255, 890.3929212266756, 824.2255293873505, 798.3124489428542, 840.2192887269481, 888.1002189380748, 886.2421254620953, 884.2313889121284, 825.5981983176448, 893.6120005711302, 884.1498986853493, 880.495847623671, 891.0013589508479, 825.3434025295023, 884.473359513197, 891.9776863286074, 884.6664825818904, 860.063592324676, 806.6895051624023, 633.7397214696748, 749.9808675067562, 944.2280707540684, 897.1830335797712, 914.4976385989326, 911.5471161542347, 760.481867265369, 852.1902048110106, 816.2659551348024, 927.0699707403877, 834.9347408572672, 836.1525938105068, 845.2347457004479, 837.3118946971206, 833.6676894801581, 829.6409178772998, 843.0563551915931, 838.0227848280243, 831.7696539025537, 832.9204552743554, 832.2882462575648, 841.0577137277824, 839.4218087459897, 827.344384873692, 913.1749446423186, 918.5366307573148, 921.068598964621, 821.6136348088946, 824.2010096283753, 933.3577786373597, 928.6396373512542, 910.8207753781587, 920.9883254559105, 831.8744414015295, 819.2558504644252, 905.9603660550222, 811.764131005386, 921.9754481587495, 922.9242549244494, 928.9312919742788, 924.1111929616782, 888.262266903448, 899.3121536716731, 913.0988716509288, 920.722753037681, 910.5168396055838, 903.8189506948877, 829.3926254230182, 29.12228151755583, 325.93383431013734, 883.5360587799724, 908.8637874926084, 928.4064753503037, 854.3522699680284, 841.9057619762964, 857.3763775654622, 854.6009059407749, 852.7066297095931, 852.8195828945111, 850.0313239422794, 849.2789405619685, 853.1795465456042, 847.0022765999465, 829.3822112752407, 845.5287075182999, 851.1529117377736, 841.9501712511013, 845.9561631783697, 848.4363206802974, 946.7426446927744, 941.7549114829055, 943.1208621868885, 916.9259927799044, 931.1921942886135, 935.1954578478965, 921.4553194895299, 929.6454684073123, 933.729727264942, 929.8230336865314, 906.4286740294403, 919.4440647325313, 927.8422871628961, 943.9810452212788, 925.1894404264525, 924.6376049380115, 926.3340775795613, 857.8054943453636, 846.8339341975012, 997.06024778306, 1005.5328861956943, 990.0353642778122, 980.6611351297083, 908.4189296999323, 999.1672752466845, 993.9558468571931, 990.8730007046124, 1001.0027243324363, 1000.1579340821719, 994.0721405828134]
Elapsed: 0.05531615573070822~0.08302284278484845
Time per graph: 0.0012856325695436257~0.0019308567632944041
Speed: 885.6686601712815~105.09044972485607
Total Time: 0.0443
best val loss: 0.6929851174354553 test_score: 0.5116

Testing...
Test loss: 0.6931 score: 0.5116 time: 0.04s
test Score 0.5116
Epoch Time List: [0.20359622710384429, 0.20337672717869282, 0.20284737402107567, 0.2023511049337685, 0.20266417390666902, 0.20245250896550715, 0.2015137701528147, 0.20231087296269834, 0.20180388388689607, 0.20334205694962293, 0.2040402750717476, 0.2096581329824403, 0.2099714840296656, 0.20804461010266095, 0.20766570407431573, 0.20810164418071508, 0.20810931804589927, 0.2076836300548166, 0.20820820110384375, 0.20911253313533962, 0.20840816700365394, 0.2058047541650012, 0.2085513409692794, 0.20885543711483479, 0.21053881908301264, 0.2161976199131459, 0.23405489197466522, 0.22237638290971518, 0.3164221770130098, 0.21334919112268835, 0.20956985012162477, 0.21051702881231904, 0.21148839895613492, 0.2116892390185967, 0.21267867705319077, 0.21255788602866232, 0.22992383502423763, 0.21057768096216023, 0.2110402489779517, 0.21214303793385625, 0.21338804287370294, 0.2127763129537925, 0.2114479609299451, 0.2971009351313114, 0.21822191902901977, 0.21616741688922048, 0.21325974294450134, 0.21309780806768686, 0.21743887197226286, 0.21176349488086998, 0.22785691393073648, 0.21146260190289468, 0.21104628790635616, 0.23955026594921947, 0.2365013089729473, 0.21240183792542666, 0.2113572359085083, 0.21148887590970844, 0.21766497602220625, 0.23364661808591336, 0.235454031964764, 0.23574492405168712, 0.2263159069698304, 0.2126787918386981, 0.21255663398187608, 0.21360436908435076, 0.2140819620108232, 0.21494629187509418, 0.21414038899820298, 0.21411508694291115, 0.21170845301821828, 0.21470517595298588, 0.21369695698376745, 0.21214432502165437, 0.21057835698593408, 0.22420798090752214, 0.21406974596902728, 0.21403264091350138, 0.21340467198751867, 0.21237208100501448, 0.2125616370467469, 0.21301789488643408, 0.21402289404068142, 0.2112297200364992, 0.21160783502273262, 0.21201159106567502, 0.21124601701740175, 0.2120880780275911, 0.21142675494775176, 0.2117872111266479, 0.21182618383318186, 0.21221130213234574, 0.21364209009334445, 0.21184493007604033, 0.21111911081243306, 0.212103397003375, 0.21203458099626005, 0.21123986307065934, 0.21193508803844452, 0.21126056800130755, 0.2106621318962425, 0.21209937008097768, 0.21258174208924174, 0.21228844195138663, 0.21930355404037982, 0.22825090889818966, 0.2103547250153497, 0.2107163000619039, 0.2112288409844041, 0.21120507700834423, 0.21606794313993305, 0.23471007822081447, 0.21356851293239743, 0.21028709411621094, 0.21003580885007977, 0.21081785194110125, 0.21040379290934652, 0.21057170606218278, 0.21068213693797588, 0.21353137295227498, 0.20966556412167847, 0.2101291810395196, 0.20992739393841475, 0.20978812675457448, 0.2100206819595769, 0.2099198839860037, 0.21536430914420635, 0.21246666193474084, 0.21338636300060898, 0.214204513002187, 0.21309705707244575, 0.21275412489194423, 5.127003434114158, 1.5452323151985183, 0.241386879933998, 0.21114692802075297, 0.21008727501612157, 0.20885504607576877, 0.21825050003826618, 0.22458055813331157, 0.20792489091400057, 0.20764894294552505, 0.20779922197107226, 0.20730779191944748, 0.2178076390409842, 0.22201754711568356, 0.21000314317643642, 0.20876079704612494, 0.20815361407585442, 0.20927119499538094, 0.20781506702769548, 0.21700917405541986, 0.2233183989301324, 0.2068215870531276, 0.2087684499565512, 0.2100120469694957, 0.21887176705058664, 0.21848010795656592, 0.23512165003921837, 0.2357441479107365, 0.2367474379716441, 0.21481451706495136, 0.21346107497811317, 0.2176540030632168, 0.2184490030631423, 0.21824951202142984, 0.22205006412696093, 0.2262484940001741, 0.215398233034648, 0.21375716698821634, 0.21472540602553636, 0.2141660099150613, 0.21179635007865727, 0.21581954800058156, 0.2125028430018574, 0.2132975549902767, 0.21283054701052606, 0.21468082594219595, 0.21230250783264637, 0.21333367004990578, 0.2142136157490313, 0.21240473084617406, 0.5296811461448669, 2.981146744918078, 2.1118258060887456, 0.33539960999041796, 0.22152943687979132, 0.2187063969904557, 0.2193378059891984, 0.2190395719371736, 0.20626813697163016, 0.20426946703810245, 0.2033132379874587, 0.20888091600500047, 0.21874432009644806, 0.21852767409291118, 0.21894438192248344, 0.2191854891134426, 0.2199814929626882, 0.21764020179398358, 0.21884227695409209, 0.22452199587132782, 0.22415406885556877, 0.22327521198894829, 0.21743988106027246, 0.20624816603958607, 0.20507452602032572, 0.20664430398028344, 0.2112148329615593, 0.21504357911180705, 0.21071124693844467, 0.21534804010298103, 0.22612456313800067, 0.22864222596399486, 0.22705569013487548, 0.22525552695151418, 0.21838498301804066, 0.2183153829537332, 0.21844365913420916, 0.21979904302861542, 0.2201390868285671, 0.22027178795542568, 0.22072151605971158, 0.22704323893412948, 0.2431087449658662, 0.241554235923104, 0.22279616200830787, 0.21923284092918038, 0.21951630502007902, 0.22788646793924272, 0.2314197290688753, 0.21885842306073755, 0.2202436758670956, 0.2190100859152153, 0.22745669598225504, 0.23090640211012214, 0.21909435105044395, 0.22005416091997176, 0.22593449789565057, 0.23170110897626728, 0.24933487991802394, 0.24350099102593958, 0.2258073149714619, 0.29051786090712994, 0.21230795595329255, 0.21372452203650028, 0.22276974318083376, 0.22740899305790663, 0.2292868250515312, 0.22500541002955288, 0.2916443300200626, 0.23369732801802456, 0.23268979298882186, 0.23503832786809653, 0.23488538688980043, 0.23429440008476377, 0.31700207490939647, 0.23359598603565246, 0.23321869899518788, 0.23398521298076957, 0.23547177202999592, 0.23467738192994148, 0.23298562306445092, 0.323114734957926, 0.2199867080198601, 0.2113051018677652, 0.21028613788075745, 0.22089106286875904, 0.23872679704800248, 0.2257065250305459, 0.20849499001633376, 0.2097292230464518, 0.20997620793059468, 0.22012886183802038, 0.24139239417854697, 0.23333325900603086, 0.22672194615006447, 0.21310047421138734, 0.20785360399167985, 0.20726806903257966, 0.20765986596234143, 0.21408926008734852, 0.20981329190544784, 0.21080682799220085, 0.2080618980107829, 0.20876112708356231, 0.2127843969501555, 0.21629762405063957, 3.224430836038664, 2.1488680409966037, 0.3718968409812078, 0.2138612010749057, 0.2061049350304529, 0.21431277401279658, 0.22894343396183103, 0.2272587699117139, 0.22851752501446754, 0.22811568598262966, 0.2275672028772533, 0.22839462384581566, 0.2284228369826451, 0.22827368206344545, 0.2278960649855435, 0.22981143987271935, 0.23019842396024615, 0.22842751594725996, 0.22879341198131442, 0.22918460192158818, 0.22850299696438015, 0.20323550491593778, 0.20292208704631776, 0.20172004599589854, 0.20419893506914377, 0.2048339070752263, 0.20776873093564063, 0.20634119503665715, 0.20690285204909742, 0.20633166108746082, 0.206418821006082, 0.209066248848103, 0.2083053330425173, 0.20783506287261844, 0.20700591313652694, 0.20570461393799633, 0.20757879107259214, 0.20737372594885528, 0.21449889009818435, 0.23266887897625566, 0.212917007971555, 0.20875282702036202, 0.21168190892785788, 0.2114289361052215, 0.2159775320906192, 0.2239965390181169, 0.2104737600311637, 0.2107616619905457, 0.21210036298725754, 0.2102704969001934, 0.20969912398140877]
Total Epoch List: [11, 317, 11]
Total Time List: [0.046794573077932, 0.05185602291021496, 0.04425983293913305]
T-times Epoch Time: 0.3149052295176855 ~ 0.050646685212327874
T-times Total Epoch: 150.44444444444443 ~ 52.718914543238505
T-times Total Time: 0.04819296155538824 ~ 0.0006483429291882307
T-times Inference Elapsed: 0.06447003624202553 ~ 0.008646565236776928
T-times Time Per Graph: 0.001497527008224066 ~ 0.00019952205561220383
T-times Speed: 875.2239470722039 ~ 20.743403180874004
T-times cross validation test micro f1 score:0.6004639595653716 ~ 0.05846877777253106
T-times cross validation test precision:0.6337062175297469 ~ 0.1240156573780185
T-times cross validation test recall:0.6608946608946608 ~ 0.132592648778518
T-times cross validation test f1_score:0.6004639595653716 ~ 0.1275087610457268
