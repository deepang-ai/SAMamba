Namespace(seed=35, model='SAMamba', dataset='mining/Volume', num_heads=2, num_layers=2, dim_hidden=64, dropout=0.5, epochs=1000, lr=0.0005, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Volume/seed35/khopgnn_gat_1_0.5_0.0005_0.0001_2_2_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c4424f3700>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7278;  Loss pred: 0.7192; Loss self: 0.8584; time: 0.37s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6967 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6953 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7124;  Loss pred: 0.7017; Loss self: 1.0716; time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6972 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6958 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6872;  Loss pred: 0.6761; Loss self: 1.1067; time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6973 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6959 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6736;  Loss pred: 0.6634; Loss self: 1.0168; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6973 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6959 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6547;  Loss pred: 0.6450; Loss self: 0.9721; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6971 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6958 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.6164;  Loss pred: 0.6070; Loss self: 0.9377; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6970 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6957 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5888;  Loss pred: 0.5787; Loss self: 1.0110; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6967 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6955 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000300
Train loss: 0.5625;  Loss pred: 0.5516; Loss self: 1.0903; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6964 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6953 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000350
Train loss: 0.5409;  Loss pred: 0.5286; Loss self: 1.2298; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6961 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6950 score: 0.5000 time: 0.05s
Epoch 10/1000, LR 0.000400
Train loss: 0.4857;  Loss pred: 0.4728; Loss self: 1.2965; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6955 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.5000 time: 0.05s
Epoch 11/1000, LR 0.000450
Train loss: 0.4412;  Loss pred: 0.4286; Loss self: 1.2590; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6947 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.5000 time: 0.05s
Epoch 12/1000, LR 0.000450
Train loss: 0.4113;  Loss pred: 0.3976; Loss self: 1.3632; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5000 time: 0.05s
Epoch 13/1000, LR 0.000450
Train loss: 0.3694;  Loss pred: 0.3544; Loss self: 1.4969; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6918 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6914 score: 0.5000 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.3439;  Loss pred: 0.3291; Loss self: 1.4801; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6895 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6893 score: 0.5000 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.3169;  Loss pred: 0.3008; Loss self: 1.6118; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6865 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6867 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2866;  Loss pred: 0.2691; Loss self: 1.7544; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6821 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6828 score: 0.5000 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.2605;  Loss pred: 0.2425; Loss self: 1.7916; time: 0.10s
Val loss: 0.6762 score: 0.6744 time: 0.05s
Test loss: 0.6775 score: 0.6591 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.2516;  Loss pred: 0.2336; Loss self: 1.7998; time: 0.10s
Val loss: 0.6684 score: 0.8140 time: 0.05s
Test loss: 0.6705 score: 0.7500 time: 0.06s
Epoch 19/1000, LR 0.000450
Train loss: 0.2280;  Loss pred: 0.2092; Loss self: 1.8757; time: 0.09s
Val loss: 0.6576 score: 0.8372 time: 0.05s
Test loss: 0.6606 score: 0.8409 time: 0.06s
Epoch 20/1000, LR 0.000450
Train loss: 0.2052;  Loss pred: 0.1858; Loss self: 1.9381; time: 0.08s
Val loss: 0.6432 score: 0.8372 time: 0.05s
Test loss: 0.6472 score: 0.8409 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1951;  Loss pred: 0.1752; Loss self: 1.9893; time: 0.09s
Val loss: 0.6243 score: 0.8837 time: 0.05s
Test loss: 0.6300 score: 0.8409 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1766;  Loss pred: 0.1564; Loss self: 2.0163; time: 0.09s
Val loss: 0.6002 score: 0.8605 time: 0.05s
Test loss: 0.6080 score: 0.8636 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1628;  Loss pred: 0.1420; Loss self: 2.0791; time: 0.09s
Val loss: 0.5717 score: 0.8837 time: 0.05s
Test loss: 0.5823 score: 0.8636 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1473;  Loss pred: 0.1262; Loss self: 2.1168; time: 0.09s
Val loss: 0.5391 score: 0.8837 time: 0.05s
Test loss: 0.5524 score: 0.8636 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.1370;  Loss pred: 0.1157; Loss self: 2.1335; time: 0.09s
Val loss: 0.5040 score: 0.9070 time: 0.05s
Test loss: 0.5205 score: 0.8636 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.1270;  Loss pred: 0.1051; Loss self: 2.1926; time: 0.09s
Val loss: 0.4689 score: 0.9070 time: 0.05s
Test loss: 0.4889 score: 0.8636 time: 0.05s
Epoch 27/1000, LR 0.000450
Train loss: 0.1160;  Loss pred: 0.0935; Loss self: 2.2494; time: 0.08s
Val loss: 0.4359 score: 0.9070 time: 0.05s
Test loss: 0.4598 score: 0.8636 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.1077;  Loss pred: 0.0849; Loss self: 2.2790; time: 0.09s
Val loss: 0.4061 score: 0.9070 time: 0.05s
Test loss: 0.4342 score: 0.8636 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.0977;  Loss pred: 0.0749; Loss self: 2.2831; time: 0.10s
Val loss: 0.3813 score: 0.9070 time: 0.06s
Test loss: 0.4133 score: 0.8636 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0903;  Loss pred: 0.0667; Loss self: 2.3578; time: 0.10s
Val loss: 0.3623 score: 0.9070 time: 0.05s
Test loss: 0.3980 score: 0.8636 time: 0.05s
Epoch 31/1000, LR 0.000450
Train loss: 0.0837;  Loss pred: 0.0602; Loss self: 2.3453; time: 0.10s
Val loss: 0.3486 score: 0.9070 time: 0.06s
Test loss: 0.3877 score: 0.8636 time: 0.05s
Epoch 32/1000, LR 0.000450
Train loss: 0.0776;  Loss pred: 0.0537; Loss self: 2.3863; time: 0.10s
Val loss: 0.3392 score: 0.9070 time: 0.05s
Test loss: 0.3812 score: 0.8636 time: 0.05s
Epoch 33/1000, LR 0.000449
Train loss: 0.0746;  Loss pred: 0.0511; Loss self: 2.3489; time: 0.10s
Val loss: 0.3325 score: 0.9070 time: 0.05s
Test loss: 0.3779 score: 0.8636 time: 0.05s
Epoch 34/1000, LR 0.000449
Train loss: 0.0655;  Loss pred: 0.0414; Loss self: 2.4066; time: 0.10s
Val loss: 0.3279 score: 0.9070 time: 0.05s
Test loss: 0.3769 score: 0.8636 time: 0.06s
Epoch 35/1000, LR 0.000449
Train loss: 0.0624;  Loss pred: 0.0376; Loss self: 2.4741; time: 0.10s
Val loss: 0.3252 score: 0.9070 time: 0.05s
Test loss: 0.3773 score: 0.8636 time: 0.05s
Epoch 36/1000, LR 0.000449
Train loss: 0.0570;  Loss pred: 0.0328; Loss self: 2.4147; time: 0.10s
Val loss: 0.3236 score: 0.9070 time: 0.05s
Test loss: 0.3789 score: 0.8636 time: 0.05s
Epoch 37/1000, LR 0.000449
Train loss: 0.0554;  Loss pred: 0.0310; Loss self: 2.4376; time: 0.10s
Val loss: 0.3231 score: 0.9070 time: 0.05s
Test loss: 0.3805 score: 0.8636 time: 0.05s
Epoch 38/1000, LR 0.000449
Train loss: 0.0505;  Loss pred: 0.0259; Loss self: 2.4542; time: 0.09s
Val loss: 0.3233 score: 0.8837 time: 0.05s
Test loss: 0.3823 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0481;  Loss pred: 0.0232; Loss self: 2.4975; time: 0.09s
Val loss: 0.3236 score: 0.8837 time: 0.05s
Test loss: 0.3836 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0462;  Loss pred: 0.0214; Loss self: 2.4815; time: 0.10s
Val loss: 0.3244 score: 0.8837 time: 0.05s
Test loss: 0.3851 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0440;  Loss pred: 0.0186; Loss self: 2.5323; time: 0.10s
Val loss: 0.3253 score: 0.8837 time: 0.05s
Test loss: 0.3863 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0437;  Loss pred: 0.0181; Loss self: 2.5550; time: 0.10s
Val loss: 0.3264 score: 0.8837 time: 0.05s
Test loss: 0.3877 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0412;  Loss pred: 0.0157; Loss self: 2.5494; time: 0.11s
Val loss: 0.3277 score: 0.8837 time: 0.05s
Test loss: 0.3889 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0396;  Loss pred: 0.0138; Loss self: 2.5765; time: 0.10s
Val loss: 0.3291 score: 0.8837 time: 0.05s
Test loss: 0.3900 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0392;  Loss pred: 0.0136; Loss self: 2.5589; time: 0.11s
Val loss: 0.3306 score: 0.8837 time: 0.05s
Test loss: 0.3909 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0377;  Loss pred: 0.0124; Loss self: 2.5306; time: 0.10s
Val loss: 0.3324 score: 0.8837 time: 0.05s
Test loss: 0.3919 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0357;  Loss pred: 0.0101; Loss self: 2.5626; time: 0.10s
Val loss: 0.3341 score: 0.8837 time: 0.05s
Test loss: 0.3927 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0352;  Loss pred: 0.0099; Loss self: 2.5335; time: 0.10s
Val loss: 0.3365 score: 0.8605 time: 0.06s
Test loss: 0.3944 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0352;  Loss pred: 0.0095; Loss self: 2.5702; time: 0.10s
Val loss: 0.3391 score: 0.8605 time: 0.05s
Test loss: 0.3960 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0344;  Loss pred: 0.0090; Loss self: 2.5411; time: 0.10s
Val loss: 0.3415 score: 0.8605 time: 0.05s
Test loss: 0.3969 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0329;  Loss pred: 0.0077; Loss self: 2.5202; time: 0.10s
Val loss: 0.3444 score: 0.8372 time: 0.05s
Test loss: 0.3985 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0330;  Loss pred: 0.0073; Loss self: 2.5639; time: 0.09s
Val loss: 0.3471 score: 0.8372 time: 0.05s
Test loss: 0.3998 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0326;  Loss pred: 0.0068; Loss self: 2.5800; time: 0.10s
Val loss: 0.3500 score: 0.8372 time: 0.05s
Test loss: 0.4011 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0316;  Loss pred: 0.0062; Loss self: 2.5439; time: 0.10s
Val loss: 0.3534 score: 0.8372 time: 0.05s
Test loss: 0.4028 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0315;  Loss pred: 0.0061; Loss self: 2.5424; time: 0.10s
Val loss: 0.3564 score: 0.8372 time: 0.05s
Test loss: 0.4040 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 56/1000, LR 0.000448
Train loss: 0.0314;  Loss pred: 0.0058; Loss self: 2.5641; time: 0.10s
Val loss: 0.3596 score: 0.8372 time: 0.05s
Test loss: 0.4055 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 57/1000, LR 0.000448
Train loss: 0.0307;  Loss pred: 0.0053; Loss self: 2.5427; time: 0.10s
Val loss: 0.3633 score: 0.8372 time: 0.05s
Test loss: 0.4077 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 036,   Train_Loss: 0.0554,   Val_Loss: 0.3231,   Val_Precision: 0.9091,   Val_Recall: 0.9091,   Val_accuracy: 0.9091,   Val_Score: 0.9070,   Val_Loss: 0.3231,   Test_Precision: 0.9444,   Test_Recall: 0.7727,   Test_accuracy: 0.8500,   Test_Score: 0.8636,   Test_loss: 0.3805


[0.049230017932131886, 0.04876580787822604, 0.05172514007426798, 0.0540130480658263, 0.0547037641517818, 0.059053167002275586, 0.05540980398654938, 0.05452913884073496, 0.05457239388488233, 0.0546314287930727, 0.05247370107099414, 0.05475683999247849, 0.05404204083606601, 0.058836731128394604, 0.052862936863675714, 0.055091978050768375, 0.05486768297851086, 0.06329527799971402, 0.061134123941883445, 0.054392114048823714, 0.054626944940537214, 0.05575049202889204, 0.05589108192361891, 0.05023140902630985, 0.052599001908674836, 0.05023908498696983, 0.05502135609276593, 0.050689755007624626, 0.05560959689319134, 0.0533615502063185, 0.05977504001930356, 0.05998993897810578, 0.05996654788032174, 0.06147557613439858, 0.05993029102683067, 0.05939474608749151, 0.058927883859723806, 0.06043584202416241, 0.062245048116892576, 0.05663242796435952, 0.05721319490112364, 0.056685341987758875, 0.058194548822939396, 0.05761528108268976, 0.051579179940745234, 0.05208118888549507, 0.05249822302721441, 0.05256102583371103, 0.05286954203620553, 0.053331646136939526, 0.05290554091334343, 0.06000530417077243, 0.05477499798871577, 0.0636102280113846, 0.05836409400217235, 0.060802171006798744, 0.05400713300332427]
[0.0011188640439120884, 0.0011083138154142282, 0.0011755713653242724, 0.001227569274223325, 0.00124326736708595, 0.0013421174318698997, 0.0012593137269670312, 0.0012392986100167036, 0.0012402816792018712, 0.0012416233816607432, 0.0011925841152498667, 0.001244473636192693, 0.001228228200819682, 0.001337198434736241, 0.001201430383265357, 0.0012520904102447357, 0.0012469927949661558, 0.0014385290454480457, 0.0013894119077700782, 0.0012361844102005389, 0.0012415214759213004, 0.0012670566370202737, 0.0012702518619004297, 0.001141622932416133, 0.0011954318615607917, 0.001141797386067496, 0.0012504853657446802, 0.0011520398865369234, 0.0012638544748452578, 0.0012127625046890569, 0.0013585236368023536, 0.0013634077040478587, 0.0013628760881891305, 0.001397172184872695, 0.0013620520687916062, 0.0013498805928975344, 0.0013392700877209957, 0.0013735418641855094, 0.0014146601844748313, 0.0012871006355536256, 0.0013002998841164465, 0.00128830322699452, 0.0013226033823395317, 0.0013094382064247673, 0.0011722540895623918, 0.0011836633837612515, 0.001193141432436691, 0.001194568768947978, 0.0012015805008228529, 0.0012120828667486255, 0.0012023986571214416, 0.0013637569129721007, 0.0012448863179253583, 0.0014456870002587411, 0.0013264566818675535, 0.0013818675228817897, 0.0012274348409846425]
[893.7636395066532, 902.2715282370216, 850.6501855156686, 814.6179779814818, 804.3322188564028, 745.0912835598551, 794.0832999640445, 806.9080300078136, 806.2684604383628, 805.3972039914729, 838.5152772142054, 803.5525791123799, 814.1809472642221, 747.8321646384873, 832.3411942372464, 798.6643710533158, 801.9292525480395, 695.1545421792571, 719.7289690750811, 808.9407953606014, 805.463312068707, 789.2307027029916, 787.2454510744784, 875.9459639476567, 836.5177741660407, 875.8121293692348, 799.6894864935001, 868.025501274994, 791.2303353773676, 824.5637510506581, 736.0931918371076, 733.4563220019019, 733.7424206544791, 715.7313971943381, 734.1863229114186, 740.8062648367204, 746.675378751777, 728.0447914072031, 706.8835406371694, 776.9400250275428, 769.0533639318901, 776.2147754088128, 756.0845627289385, 763.6862855333633, 853.0573780069345, 844.8347847192534, 838.123606149316, 837.1221699364123, 832.2372070079292, 825.0260996448773, 831.6709221831745, 733.2685103099878, 803.286200194192, 691.7126596704719, 753.8881696400922, 723.6583706045633, 814.7071979786762]
Elapsed: 0.05579437533998045~0.0036154594218207087
Time per graph: 0.0012680539849995555~8.216953231410702e-05
Speed: 791.8971973188734~50.85977651808771
Total Time: 0.0544
best val loss: 0.323062926530838 test_score: 0.8636

Testing...
Test loss: 0.5205 score: 0.8636 time: 0.05s
test Score 0.8636
Epoch Time List: [0.4711068111937493, 0.17783386819064617, 0.17739120917394757, 0.18486022390425205, 0.18698138114996254, 0.19153064512647688, 0.19711823901161551, 0.18995680892840028, 0.19201195216737688, 0.18975565768778324, 0.19351811916567385, 0.19647868513129652, 0.1899943680036813, 0.2038509042467922, 0.19185978826135397, 0.20034926873631775, 0.1958206903655082, 0.20490386290475726, 0.19661809783428907, 0.1822536790277809, 0.18505774205550551, 0.18483520695008337, 0.1877669550012797, 0.18444787105545402, 0.19033537292852998, 0.18047863594256341, 0.183275650953874, 0.1873333938419819, 0.21351710497401655, 0.2002280382439494, 0.20756256207823753, 0.20843703602440655, 0.20242110989056528, 0.20766091998666525, 0.21117226500064135, 0.2065688988659531, 0.20400089398026466, 0.19936714693903923, 0.20398422423750162, 0.19712390913628042, 0.20363928982988, 0.2042218279093504, 0.20922232512384653, 0.2087039367761463, 0.20315873017534614, 0.19418918807059526, 0.19707828364335, 0.20511539513245225, 0.2061792358290404, 0.20018151286058128, 0.19753739493899047, 0.20014700619503856, 0.1980231530033052, 0.20641368301585317, 0.20815901388414204, 0.20793032669462264, 0.19778333511203527]
Total Epoch List: [57]
Total Time List: [0.05442717298865318]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c442356740>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.6818;  Loss pred: 0.6724; Loss self: 0.9459; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6960 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6941 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.6925;  Loss pred: 0.6834; Loss self: 0.9117; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6951 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000050
Train loss: 0.6687;  Loss pred: 0.6586; Loss self: 1.0031; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6945 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000100
Train loss: 0.6423;  Loss pred: 0.6331; Loss self: 0.9148; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6940 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000150
Train loss: 0.6153;  Loss pred: 0.6052; Loss self: 1.0098; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5116 time: 0.05s
Epoch 6/1000, LR 0.000200
Train loss: 0.5669;  Loss pred: 0.5574; Loss self: 0.9518; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6924 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000250
Train loss: 0.5200;  Loss pred: 0.5097; Loss self: 1.0386; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6920 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000300
Train loss: 0.4764;  Loss pred: 0.4641; Loss self: 1.2217; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6917 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000350
Train loss: 0.4368;  Loss pred: 0.4237; Loss self: 1.3131; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6918 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6913 score: 0.5116 time: 0.06s
Epoch 10/1000, LR 0.000400
Train loss: 0.3992;  Loss pred: 0.3856; Loss self: 1.3582; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6908 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6905 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000450
Train loss: 0.3396;  Loss pred: 0.3248; Loss self: 1.4824; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6894 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6893 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000450
Train loss: 0.3177;  Loss pred: 0.3024; Loss self: 1.5313; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6876 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6879 score: 0.5116 time: 0.05s
Epoch 13/1000, LR 0.000450
Train loss: 0.2834;  Loss pred: 0.2672; Loss self: 1.6199; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6855 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6862 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.2548;  Loss pred: 0.2377; Loss self: 1.7128; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6822 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6836 score: 0.5116 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.2274;  Loss pred: 0.2091; Loss self: 1.8345; time: 0.10s
Val loss: 0.6776 score: 0.5227 time: 0.04s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6798 score: 0.5116 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2031;  Loss pred: 0.1835; Loss self: 1.9635; time: 0.11s
Val loss: 0.6714 score: 0.5682 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6746 score: 0.5116 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.1755;  Loss pred: 0.1558; Loss self: 1.9735; time: 0.11s
Val loss: 0.6627 score: 0.5909 time: 0.05s
Test loss: 0.6673 score: 0.6047 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.1623;  Loss pred: 0.1414; Loss self: 2.0869; time: 0.11s
Val loss: 0.6509 score: 0.5682 time: 0.05s
Test loss: 0.6575 score: 0.6744 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.1467;  Loss pred: 0.1251; Loss self: 2.1606; time: 0.10s
Val loss: 0.6348 score: 0.7273 time: 0.05s
Test loss: 0.6442 score: 0.6977 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.1346;  Loss pred: 0.1120; Loss self: 2.2641; time: 0.11s
Val loss: 0.6118 score: 0.8409 time: 0.05s
Test loss: 0.6247 score: 0.7674 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1162;  Loss pred: 0.0930; Loss self: 2.3166; time: 0.10s
Val loss: 0.5831 score: 0.8636 time: 0.05s
Test loss: 0.6003 score: 0.8372 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1095;  Loss pred: 0.0857; Loss self: 2.3866; time: 0.11s
Val loss: 0.5489 score: 0.8864 time: 0.05s
Test loss: 0.5712 score: 0.8837 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.0982;  Loss pred: 0.0740; Loss self: 2.4219; time: 0.11s
Val loss: 0.5116 score: 0.9091 time: 0.05s
Test loss: 0.5379 score: 0.8837 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.0864;  Loss pred: 0.0624; Loss self: 2.4058; time: 0.11s
Val loss: 0.4729 score: 0.9091 time: 0.05s
Test loss: 0.5030 score: 0.8837 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.0814;  Loss pred: 0.0566; Loss self: 2.4731; time: 0.11s
Val loss: 0.4358 score: 0.9091 time: 0.05s
Test loss: 0.4686 score: 0.8837 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.0744;  Loss pred: 0.0494; Loss self: 2.5013; time: 0.11s
Val loss: 0.4030 score: 0.8864 time: 0.05s
Test loss: 0.4379 score: 0.8837 time: 0.05s
Epoch 27/1000, LR 0.000450
Train loss: 0.0679;  Loss pred: 0.0424; Loss self: 2.5500; time: 0.11s
Val loss: 0.3765 score: 0.8864 time: 0.05s
Test loss: 0.4122 score: 0.8605 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.0611;  Loss pred: 0.0358; Loss self: 2.5279; time: 0.11s
Val loss: 0.3562 score: 0.8864 time: 0.05s
Test loss: 0.3923 score: 0.8605 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.0605;  Loss pred: 0.0345; Loss self: 2.5996; time: 0.11s
Val loss: 0.3432 score: 0.8636 time: 0.05s
Test loss: 0.3793 score: 0.8605 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0535;  Loss pred: 0.0278; Loss self: 2.5693; time: 0.11s
Val loss: 0.3351 score: 0.8636 time: 0.05s
Test loss: 0.3704 score: 0.8605 time: 0.05s
Epoch 31/1000, LR 0.000450
Train loss: 0.0509;  Loss pred: 0.0250; Loss self: 2.5897; time: 0.11s
Val loss: 0.3314 score: 0.8636 time: 0.05s
Test loss: 0.3660 score: 0.8605 time: 0.05s
Epoch 32/1000, LR 0.000450
Train loss: 0.0488;  Loss pred: 0.0229; Loss self: 2.5890; time: 0.11s
Val loss: 0.3305 score: 0.8636 time: 0.05s
Test loss: 0.3652 score: 0.8605 time: 0.05s
Epoch 33/1000, LR 0.000449
Train loss: 0.0462;  Loss pred: 0.0200; Loss self: 2.6138; time: 0.11s
Val loss: 0.3317 score: 0.8636 time: 0.05s
Test loss: 0.3667 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0451;  Loss pred: 0.0186; Loss self: 2.6570; time: 0.11s
Val loss: 0.3349 score: 0.8636 time: 0.05s
Test loss: 0.3702 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0426;  Loss pred: 0.0160; Loss self: 2.6660; time: 0.11s
Val loss: 0.3393 score: 0.8636 time: 0.05s
Test loss: 0.3758 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0410;  Loss pred: 0.0142; Loss self: 2.6733; time: 0.12s
Val loss: 0.3434 score: 0.8864 time: 0.05s
Test loss: 0.3818 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0392;  Loss pred: 0.0127; Loss self: 2.6538; time: 0.11s
Val loss: 0.3476 score: 0.8864 time: 0.05s
Test loss: 0.3882 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0382;  Loss pred: 0.0117; Loss self: 2.6559; time: 0.10s
Val loss: 0.3523 score: 0.8864 time: 0.05s
Test loss: 0.3959 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0375;  Loss pred: 0.0110; Loss self: 2.6512; time: 0.10s
Val loss: 0.3559 score: 0.8864 time: 0.05s
Test loss: 0.4021 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0363;  Loss pred: 0.0098; Loss self: 2.6504; time: 0.13s
Val loss: 0.3597 score: 0.9091 time: 0.05s
Test loss: 0.4088 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0353;  Loss pred: 0.0089; Loss self: 2.6422; time: 0.09s
Val loss: 0.3633 score: 0.9091 time: 0.05s
Test loss: 0.4155 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0345;  Loss pred: 0.0078; Loss self: 2.6735; time: 0.09s
Val loss: 0.3666 score: 0.9091 time: 0.05s
Test loss: 0.4217 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0341;  Loss pred: 0.0075; Loss self: 2.6636; time: 0.09s
Val loss: 0.3697 score: 0.9091 time: 0.05s
Test loss: 0.4275 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0337;  Loss pred: 0.0069; Loss self: 2.6763; time: 0.09s
Val loss: 0.3727 score: 0.9091 time: 0.05s
Test loss: 0.4330 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0334;  Loss pred: 0.0064; Loss self: 2.6976; time: 0.10s
Val loss: 0.3758 score: 0.9091 time: 0.05s
Test loss: 0.4381 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0328;  Loss pred: 0.0061; Loss self: 2.6668; time: 0.11s
Val loss: 0.3784 score: 0.9091 time: 0.05s
Test loss: 0.4425 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0324;  Loss pred: 0.0060; Loss self: 2.6491; time: 0.11s
Val loss: 0.3813 score: 0.9091 time: 0.05s
Test loss: 0.4469 score: 0.8372 time: 0.14s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0319;  Loss pred: 0.0053; Loss self: 2.6572; time: 0.10s
Val loss: 0.3840 score: 0.9091 time: 0.05s
Test loss: 0.4510 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0317;  Loss pred: 0.0049; Loss self: 2.6756; time: 0.10s
Val loss: 0.3854 score: 0.9091 time: 0.05s
Test loss: 0.4529 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0311;  Loss pred: 0.0047; Loss self: 2.6472; time: 0.11s
Val loss: 0.3865 score: 0.9091 time: 0.05s
Test loss: 0.4542 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0309;  Loss pred: 0.0042; Loss self: 2.6770; time: 0.11s
Val loss: 0.3877 score: 0.8864 time: 0.05s
Test loss: 0.4556 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0310;  Loss pred: 0.0043; Loss self: 2.6651; time: 0.10s
Val loss: 0.3895 score: 0.8864 time: 0.05s
Test loss: 0.4574 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.0488,   Val_Loss: 0.3305,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3305,   Test_Precision: 0.9444,   Test_Recall: 0.7727,   Test_accuracy: 0.8500,   Test_Score: 0.8605,   Test_loss: 0.3652


[0.049230017932131886, 0.04876580787822604, 0.05172514007426798, 0.0540130480658263, 0.0547037641517818, 0.059053167002275586, 0.05540980398654938, 0.05452913884073496, 0.05457239388488233, 0.0546314287930727, 0.05247370107099414, 0.05475683999247849, 0.05404204083606601, 0.058836731128394604, 0.052862936863675714, 0.055091978050768375, 0.05486768297851086, 0.06329527799971402, 0.061134123941883445, 0.054392114048823714, 0.054626944940537214, 0.05575049202889204, 0.05589108192361891, 0.05023140902630985, 0.052599001908674836, 0.05023908498696983, 0.05502135609276593, 0.050689755007624626, 0.05560959689319134, 0.0533615502063185, 0.05977504001930356, 0.05998993897810578, 0.05996654788032174, 0.06147557613439858, 0.05993029102683067, 0.05939474608749151, 0.058927883859723806, 0.06043584202416241, 0.062245048116892576, 0.05663242796435952, 0.05721319490112364, 0.056685341987758875, 0.058194548822939396, 0.05761528108268976, 0.051579179940745234, 0.05208118888549507, 0.05249822302721441, 0.05256102583371103, 0.05286954203620553, 0.053331646136939526, 0.05290554091334343, 0.06000530417077243, 0.05477499798871577, 0.0636102280113846, 0.05836409400217235, 0.060802171006798744, 0.05400713300332427, 0.05110812000930309, 0.05102991289459169, 0.05185443698428571, 0.05204289895482361, 0.05215533799491823, 0.05251895799301565, 0.05123193096369505, 0.052332468098029494, 0.06670803297311068, 0.05666830204427242, 0.05834147008135915, 0.05672800401225686, 0.054844834143295884, 0.05075205420143902, 0.051365809980779886, 0.0519083549734205, 0.05191950197331607, 0.056018037954345345, 0.05422501894645393, 0.0520679319743067, 0.0526689721737057, 0.05226935795508325, 0.05224256101064384, 0.05240799603052437, 0.052372195990756154, 0.05227412795647979, 0.05203060316853225, 0.052162467036396265, 0.05176380416378379, 0.051694202004000545, 0.05194024718366563, 0.051779353991150856, 0.05193474399857223, 0.051961694145575166, 0.05225494806654751, 0.05224733683280647, 0.0535465341527015, 0.0522301469463855, 0.052125888178125024, 0.05487470701336861, 0.055202336981892586, 0.05540278903208673, 0.05547012900933623, 0.05572123290039599, 0.05600650887936354, 0.056632788851857185, 0.1465982070658356, 0.05526415817439556, 0.05539505206979811, 0.05600702413357794, 0.056327929021790624, 0.05664109205827117]
[0.0011188640439120884, 0.0011083138154142282, 0.0011755713653242724, 0.001227569274223325, 0.00124326736708595, 0.0013421174318698997, 0.0012593137269670312, 0.0012392986100167036, 0.0012402816792018712, 0.0012416233816607432, 0.0011925841152498667, 0.001244473636192693, 0.001228228200819682, 0.001337198434736241, 0.001201430383265357, 0.0012520904102447357, 0.0012469927949661558, 0.0014385290454480457, 0.0013894119077700782, 0.0012361844102005389, 0.0012415214759213004, 0.0012670566370202737, 0.0012702518619004297, 0.001141622932416133, 0.0011954318615607917, 0.001141797386067496, 0.0012504853657446802, 0.0011520398865369234, 0.0012638544748452578, 0.0012127625046890569, 0.0013585236368023536, 0.0013634077040478587, 0.0013628760881891305, 0.001397172184872695, 0.0013620520687916062, 0.0013498805928975344, 0.0013392700877209957, 0.0013735418641855094, 0.0014146601844748313, 0.0012871006355536256, 0.0013002998841164465, 0.00128830322699452, 0.0013226033823395317, 0.0013094382064247673, 0.0011722540895623918, 0.0011836633837612515, 0.001193141432436691, 0.001194568768947978, 0.0012015805008228529, 0.0012120828667486255, 0.0012023986571214416, 0.0013637569129721007, 0.0012448863179253583, 0.0014456870002587411, 0.0013264566818675535, 0.0013818675228817897, 0.0012274348409846425, 0.001188560930448909, 0.0011867421603393416, 0.0012059171391694352, 0.0012102999756935723, 0.0012129148370911215, 0.001221371116116643, 0.0011914402549696524, 0.0012170341418146393, 0.0015513496040258296, 0.0013178674894016842, 0.0013567783739850965, 0.0013192559072617875, 0.001275461259146416, 0.0011802803302660239, 0.0011945537204832532, 0.0012071710458935, 0.001207430278449211, 0.0013027450687057057, 0.0012610469522431147, 0.0012108821389373652, 0.0012248598179931556, 0.0012155664640717035, 0.0012149432793172987, 0.001218790605361032, 0.0012179580462966548, 0.0012156773943367393, 0.0012100140271751687, 0.0012130806287534016, 0.0012038093991577625, 0.0012021907442790825, 0.0012079127252015264, 0.00120417102305002, 0.0012077847441528426, 0.001208411491757562, 0.001215231350384826, 0.0012150543449489877, 0.001245268236109337, 0.0012146545801485001, 0.0012122299576308145, 0.001276155977055084, 0.0012837752786486647, 0.001288436954234575, 0.0012900030002171217, 0.0012958426255906045, 0.001302476950682873, 0.001317041601205981, 0.003409260629438037, 0.0012852129807998968, 0.0012882570248790258, 0.0013024889333390218, 0.001309951837716061, 0.0013172346990295621]
[893.7636395066532, 902.2715282370216, 850.6501855156686, 814.6179779814818, 804.3322188564028, 745.0912835598551, 794.0832999640445, 806.9080300078136, 806.2684604383628, 805.3972039914729, 838.5152772142054, 803.5525791123799, 814.1809472642221, 747.8321646384873, 832.3411942372464, 798.6643710533158, 801.9292525480395, 695.1545421792571, 719.7289690750811, 808.9407953606014, 805.463312068707, 789.2307027029916, 787.2454510744784, 875.9459639476567, 836.5177741660407, 875.8121293692348, 799.6894864935001, 868.025501274994, 791.2303353773676, 824.5637510506581, 736.0931918371076, 733.4563220019019, 733.7424206544791, 715.7313971943381, 734.1863229114186, 740.8062648367204, 746.675378751777, 728.0447914072031, 706.8835406371694, 776.9400250275428, 769.0533639318901, 776.2147754088128, 756.0845627289385, 763.6862855333633, 853.0573780069345, 844.8347847192534, 838.123606149316, 837.1221699364123, 832.2372070079292, 825.0260996448773, 831.6709221831745, 733.2685103099878, 803.286200194192, 691.7126596704719, 753.8881696400922, 723.6583706045633, 814.7071979786762, 841.3535851479728, 842.6430217276987, 829.2443713742565, 826.2414443385754, 824.460192438782, 818.7519639235503, 839.3203065188287, 821.6696357498779, 644.6000291648962, 758.8016306965756, 737.040049557117, 758.003048912302, 784.0300854525645, 847.2563461043273, 837.1327156349678, 828.3830227719219, 828.205170806526, 767.6098908541738, 792.9918852119093, 825.8442071641842, 816.4199570514344, 822.6617215568497, 823.0836920731973, 820.4854842180035, 821.0463431319479, 822.5866538758743, 826.4367003534201, 824.3475135099885, 830.6962885483728, 831.8147554859689, 827.8743812663796, 830.4468226341477, 827.96210569907, 827.5326797377274, 822.8885797616488, 823.0084556769217, 803.0398359186911, 823.279322651336, 824.9259917271825, 783.6032726247507, 778.9525290225451, 776.1342118552261, 775.1919955470561, 771.6986463107211, 767.7679052022472, 759.2774587259247, 293.3187305673472, 778.0811545940155, 776.2426136150162, 767.7608418801914, 763.386844621348, 759.1661537133235]
Elapsed: 0.055628916951443216~0.00937961777678655
Time per graph: 0.0012782745616732101~0.00021797504846667962
Speed: 792.5948852649968~65.7122094270487
Total Time: 0.0572
best val loss: 0.3305438458919525 test_score: 0.8605

Testing...
Test loss: 0.5379 score: 0.8837 time: 0.05s
test Score 0.8837
Epoch Time List: [0.4711068111937493, 0.17783386819064617, 0.17739120917394757, 0.18486022390425205, 0.18698138114996254, 0.19153064512647688, 0.19711823901161551, 0.18995680892840028, 0.19201195216737688, 0.18975565768778324, 0.19351811916567385, 0.19647868513129652, 0.1899943680036813, 0.2038509042467922, 0.19185978826135397, 0.20034926873631775, 0.1958206903655082, 0.20490386290475726, 0.19661809783428907, 0.1822536790277809, 0.18505774205550551, 0.18483520695008337, 0.1877669550012797, 0.18444787105545402, 0.19033537292852998, 0.18047863594256341, 0.183275650953874, 0.1873333938419819, 0.21351710497401655, 0.2002280382439494, 0.20756256207823753, 0.20843703602440655, 0.20242110989056528, 0.20766091998666525, 0.21117226500064135, 0.2065688988659531, 0.20400089398026466, 0.19936714693903923, 0.20398422423750162, 0.19712390913628042, 0.20363928982988, 0.2042218279093504, 0.20922232512384653, 0.2087039367761463, 0.20315873017534614, 0.19418918807059526, 0.19707828364335, 0.20511539513245225, 0.2061792358290404, 0.20018151286058128, 0.19753739493899047, 0.20014700619503856, 0.1980231530033052, 0.20641368301585317, 0.20815901388414204, 0.20793032669462264, 0.19778333511203527, 0.18937340402044356, 0.18815133231692016, 0.18892964092083275, 0.19485076609998941, 0.1943234638310969, 0.19488284387625754, 0.1959059580694884, 0.1939656229224056, 0.2247717382851988, 0.2207756380084902, 0.20321447704918683, 0.19147701491601765, 0.19655899493955076, 0.19399438588880002, 0.19329873914830387, 0.19789309590123594, 0.19940075813792646, 0.20435295323841274, 0.19018180179409683, 0.19859999394975603, 0.19798283278942108, 0.20110692689195275, 0.20134542416781187, 0.20163669297471642, 0.20082075940445065, 0.1994052620138973, 0.1995548012200743, 0.19877533009275794, 0.19971981714479625, 0.2003507148474455, 0.1984347349498421, 0.20145052601583302, 0.19980896194465458, 0.20027146418578923, 0.20328134903684258, 0.2155721231829375, 0.20200208690948784, 0.1984251954127103, 0.197484971024096, 0.22764042392373085, 0.19068951485678554, 0.19161004479974508, 0.19218252110294998, 0.1953248418867588, 0.19858991517685354, 0.216038255719468, 0.3008588901720941, 0.20364028098993003, 0.2027148602064699, 0.20776056707836688, 0.21149147697724402, 0.20482467720285058]
Total Epoch List: [57, 52]
Total Time List: [0.05442717298865318, 0.0571957528591156]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c442354820>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7112;  Loss pred: 0.7021; Loss self: 0.9049; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7012;  Loss pred: 0.6906; Loss self: 1.0574; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.7091;  Loss pred: 0.6988; Loss self: 1.0363; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6868;  Loss pred: 0.6767; Loss self: 1.0093; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6497;  Loss pred: 0.6413; Loss self: 0.8383; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.6180;  Loss pred: 0.6089; Loss self: 0.9063; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5759;  Loss pred: 0.5666; Loss self: 0.9227; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6936 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6925 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.5646;  Loss pred: 0.5550; Loss self: 0.9563; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6922 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000350
Train loss: 0.5230;  Loss pred: 0.5127; Loss self: 1.0302; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6918 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000400
Train loss: 0.4833;  Loss pred: 0.4717; Loss self: 1.1558; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6911 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000450
Train loss: 0.4387;  Loss pred: 0.4275; Loss self: 1.1176; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6915 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6902 score: 0.5116 time: 0.04s
Epoch 12/1000, LR 0.000450
Train loss: 0.4066;  Loss pred: 0.3939; Loss self: 1.2719; time: 0.22s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6903 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6890 score: 0.5116 time: 0.07s
Epoch 13/1000, LR 0.000450
Train loss: 0.3888;  Loss pred: 0.3758; Loss self: 1.3023; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6886 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6872 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.3521;  Loss pred: 0.3385; Loss self: 1.3606; time: 0.10s
Val loss: 0.6861 score: 0.5455 time: 0.05s
Test loss: 0.6846 score: 0.5581 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.3210;  Loss pred: 0.3068; Loss self: 1.4194; time: 0.10s
Val loss: 0.6825 score: 0.6591 time: 0.05s
Test loss: 0.6810 score: 0.8140 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2958;  Loss pred: 0.2810; Loss self: 1.4773; time: 0.10s
Val loss: 0.6775 score: 0.7727 time: 0.05s
Test loss: 0.6759 score: 0.8140 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.2735;  Loss pred: 0.2580; Loss self: 1.5480; time: 0.10s
Val loss: 0.6704 score: 0.8636 time: 0.05s
Test loss: 0.6688 score: 0.8605 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.2550;  Loss pred: 0.2386; Loss self: 1.6360; time: 0.10s
Val loss: 0.6605 score: 0.8636 time: 0.05s
Test loss: 0.6590 score: 0.8605 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.2285;  Loss pred: 0.2118; Loss self: 1.6707; time: 0.11s
Val loss: 0.6470 score: 0.8864 time: 0.11s
Test loss: 0.6461 score: 0.8605 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.2152;  Loss pred: 0.1977; Loss self: 1.7498; time: 0.10s
Val loss: 0.6296 score: 0.8864 time: 0.05s
Test loss: 0.6293 score: 0.8605 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1977;  Loss pred: 0.1794; Loss self: 1.8305; time: 0.10s
Val loss: 0.6070 score: 0.9091 time: 0.05s
Test loss: 0.6086 score: 0.8605 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1753;  Loss pred: 0.1570; Loss self: 1.8253; time: 0.10s
Val loss: 0.5784 score: 0.9091 time: 0.05s
Test loss: 0.5823 score: 0.8605 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1623;  Loss pred: 0.1434; Loss self: 1.8946; time: 0.10s
Val loss: 0.5438 score: 0.8864 time: 0.05s
Test loss: 0.5492 score: 0.8605 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1517;  Loss pred: 0.1322; Loss self: 1.9516; time: 0.10s
Val loss: 0.5049 score: 0.8864 time: 0.05s
Test loss: 0.5113 score: 0.8605 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.1346;  Loss pred: 0.1142; Loss self: 2.0349; time: 0.10s
Val loss: 0.4630 score: 0.8864 time: 0.11s
Test loss: 0.4704 score: 0.8837 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.1275;  Loss pred: 0.1069; Loss self: 2.0680; time: 0.10s
Val loss: 0.4226 score: 0.8864 time: 0.12s
Test loss: 0.4299 score: 0.8837 time: 0.04s
Epoch 27/1000, LR 0.000450
Train loss: 0.1162;  Loss pred: 0.0952; Loss self: 2.1062; time: 0.10s
Val loss: 0.3850 score: 0.8864 time: 0.04s
Test loss: 0.3908 score: 0.8837 time: 0.04s
Epoch 28/1000, LR 0.000450
Train loss: 0.1082;  Loss pred: 0.0871; Loss self: 2.1168; time: 0.10s
Val loss: 0.3538 score: 0.8636 time: 0.04s
Test loss: 0.3572 score: 0.9302 time: 0.04s
Epoch 29/1000, LR 0.000450
Train loss: 0.0976;  Loss pred: 0.0759; Loss self: 2.1703; time: 0.10s
Val loss: 0.3303 score: 0.8636 time: 0.05s
Test loss: 0.3299 score: 0.9302 time: 0.04s
Epoch 30/1000, LR 0.000450
Train loss: 0.0908;  Loss pred: 0.0691; Loss self: 2.1737; time: 0.10s
Val loss: 0.3153 score: 0.8636 time: 0.04s
Test loss: 0.3104 score: 0.9302 time: 0.04s
Epoch 31/1000, LR 0.000450
Train loss: 0.0838;  Loss pred: 0.0615; Loss self: 2.2306; time: 0.10s
Val loss: 0.3091 score: 0.8864 time: 0.05s
Test loss: 0.2983 score: 0.9070 time: 0.04s
Epoch 32/1000, LR 0.000450
Train loss: 0.0778;  Loss pred: 0.0551; Loss self: 2.2743; time: 0.10s
Val loss: 0.3101 score: 0.8864 time: 0.04s
Test loss: 0.2923 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0704;  Loss pred: 0.0481; Loss self: 2.2319; time: 0.10s
Val loss: 0.3164 score: 0.8864 time: 0.05s
Test loss: 0.2907 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0691;  Loss pred: 0.0460; Loss self: 2.3043; time: 0.12s
Val loss: 0.3270 score: 0.8636 time: 0.04s
Test loss: 0.2935 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0619;  Loss pred: 0.0386; Loss self: 2.3254; time: 0.10s
Val loss: 0.3395 score: 0.8636 time: 0.04s
Test loss: 0.2989 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0590;  Loss pred: 0.0357; Loss self: 2.3306; time: 0.11s
Val loss: 0.3530 score: 0.8636 time: 0.05s
Test loss: 0.3055 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0547;  Loss pred: 0.0311; Loss self: 2.3556; time: 0.10s
Val loss: 0.3690 score: 0.8636 time: 0.05s
Test loss: 0.3136 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0519;  Loss pred: 0.0282; Loss self: 2.3674; time: 0.10s
Val loss: 0.3844 score: 0.8636 time: 0.04s
Test loss: 0.3218 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0487;  Loss pred: 0.0249; Loss self: 2.3781; time: 0.10s
Val loss: 0.3989 score: 0.8636 time: 0.04s
Test loss: 0.3297 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0466;  Loss pred: 0.0227; Loss self: 2.3902; time: 0.10s
Val loss: 0.4142 score: 0.8636 time: 0.04s
Test loss: 0.3381 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0433;  Loss pred: 0.0194; Loss self: 2.3851; time: 0.10s
Val loss: 0.4277 score: 0.8409 time: 0.09s
Test loss: 0.3461 score: 0.9070 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0419;  Loss pred: 0.0176; Loss self: 2.4313; time: 0.10s
Val loss: 0.4381 score: 0.8409 time: 0.04s
Test loss: 0.3525 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0403;  Loss pred: 0.0160; Loss self: 2.4255; time: 0.10s
Val loss: 0.4480 score: 0.8409 time: 0.05s
Test loss: 0.3590 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0385;  Loss pred: 0.0143; Loss self: 2.4167; time: 0.11s
Val loss: 0.4575 score: 0.8409 time: 0.05s
Test loss: 0.3658 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0375;  Loss pred: 0.0130; Loss self: 2.4476; time: 0.10s
Val loss: 0.4637 score: 0.8409 time: 0.05s
Test loss: 0.3707 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0367;  Loss pred: 0.0122; Loss self: 2.4461; time: 0.11s
Val loss: 0.4706 score: 0.8409 time: 0.05s
Test loss: 0.3763 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0362;  Loss pred: 0.0115; Loss self: 2.4714; time: 0.10s
Val loss: 0.4737 score: 0.8409 time: 0.05s
Test loss: 0.3794 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0344;  Loss pred: 0.0099; Loss self: 2.4528; time: 0.10s
Val loss: 0.4775 score: 0.8409 time: 0.05s
Test loss: 0.3828 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0336;  Loss pred: 0.0091; Loss self: 2.4506; time: 0.14s
Val loss: 0.4785 score: 0.8409 time: 0.05s
Test loss: 0.3844 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0333;  Loss pred: 0.0085; Loss self: 2.4720; time: 0.10s
Val loss: 0.4802 score: 0.8409 time: 0.04s
Test loss: 0.3864 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0325;  Loss pred: 0.0079; Loss self: 2.4616; time: 0.10s
Val loss: 0.4806 score: 0.8409 time: 0.05s
Test loss: 0.3874 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 030,   Train_Loss: 0.0838,   Val_Loss: 0.3091,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.3091,   Test_Precision: 1.0000,   Test_Recall: 0.8095,   Test_accuracy: 0.8947,   Test_Score: 0.9070,   Test_loss: 0.2983


[0.049230017932131886, 0.04876580787822604, 0.05172514007426798, 0.0540130480658263, 0.0547037641517818, 0.059053167002275586, 0.05540980398654938, 0.05452913884073496, 0.05457239388488233, 0.0546314287930727, 0.05247370107099414, 0.05475683999247849, 0.05404204083606601, 0.058836731128394604, 0.052862936863675714, 0.055091978050768375, 0.05486768297851086, 0.06329527799971402, 0.061134123941883445, 0.054392114048823714, 0.054626944940537214, 0.05575049202889204, 0.05589108192361891, 0.05023140902630985, 0.052599001908674836, 0.05023908498696983, 0.05502135609276593, 0.050689755007624626, 0.05560959689319134, 0.0533615502063185, 0.05977504001930356, 0.05998993897810578, 0.05996654788032174, 0.06147557613439858, 0.05993029102683067, 0.05939474608749151, 0.058927883859723806, 0.06043584202416241, 0.062245048116892576, 0.05663242796435952, 0.05721319490112364, 0.056685341987758875, 0.058194548822939396, 0.05761528108268976, 0.051579179940745234, 0.05208118888549507, 0.05249822302721441, 0.05256102583371103, 0.05286954203620553, 0.053331646136939526, 0.05290554091334343, 0.06000530417077243, 0.05477499798871577, 0.0636102280113846, 0.05836409400217235, 0.060802171006798744, 0.05400713300332427, 0.05110812000930309, 0.05102991289459169, 0.05185443698428571, 0.05204289895482361, 0.05215533799491823, 0.05251895799301565, 0.05123193096369505, 0.052332468098029494, 0.06670803297311068, 0.05666830204427242, 0.05834147008135915, 0.05672800401225686, 0.054844834143295884, 0.05075205420143902, 0.051365809980779886, 0.0519083549734205, 0.05191950197331607, 0.056018037954345345, 0.05422501894645393, 0.0520679319743067, 0.0526689721737057, 0.05226935795508325, 0.05224256101064384, 0.05240799603052437, 0.052372195990756154, 0.05227412795647979, 0.05203060316853225, 0.052162467036396265, 0.05176380416378379, 0.051694202004000545, 0.05194024718366563, 0.051779353991150856, 0.05193474399857223, 0.051961694145575166, 0.05225494806654751, 0.05224733683280647, 0.0535465341527015, 0.0522301469463855, 0.052125888178125024, 0.05487470701336861, 0.055202336981892586, 0.05540278903208673, 0.05547012900933623, 0.05572123290039599, 0.05600650887936354, 0.056632788851857185, 0.1465982070658356, 0.05526415817439556, 0.05539505206979811, 0.05600702413357794, 0.056327929021790624, 0.05664109205827117, 0.050911109894514084, 0.05117928981781006, 0.050377277890220284, 0.14512397604994476, 0.04918041708879173, 0.04993113689124584, 0.050022511975839734, 0.050335215870290995, 0.05043708020821214, 0.05040582804940641, 0.04989518295042217, 0.0733703940641135, 0.05226200702600181, 0.05258900602348149, 0.05346725997515023, 0.0530266750138253, 0.0533249331638217, 0.0530424399767071, 0.051433985121548176, 0.05188264208845794, 0.052388027077540755, 0.05261954013258219, 0.052835253067314625, 0.053288305178284645, 0.05309646506793797, 0.04947422491386533, 0.049074982991442084, 0.049056994961574674, 0.04975415603257716, 0.04971153102815151, 0.0499431318603456, 0.0500620671082288, 0.04910481697879732, 0.04883181699551642, 0.048705540131777525, 0.05287447781302035, 0.0500288859475404, 0.05021870182827115, 0.0497255630325526, 0.04931933991611004, 0.07951578684151173, 0.04837954812683165, 0.05896078911609948, 0.04933312605135143, 0.04978451714850962, 0.05020252987742424, 0.050037819892168045, 0.05020895600318909, 0.04905350389890373, 0.04969021491706371, 0.049609433161094785]
[0.0011188640439120884, 0.0011083138154142282, 0.0011755713653242724, 0.001227569274223325, 0.00124326736708595, 0.0013421174318698997, 0.0012593137269670312, 0.0012392986100167036, 0.0012402816792018712, 0.0012416233816607432, 0.0011925841152498667, 0.001244473636192693, 0.001228228200819682, 0.001337198434736241, 0.001201430383265357, 0.0012520904102447357, 0.0012469927949661558, 0.0014385290454480457, 0.0013894119077700782, 0.0012361844102005389, 0.0012415214759213004, 0.0012670566370202737, 0.0012702518619004297, 0.001141622932416133, 0.0011954318615607917, 0.001141797386067496, 0.0012504853657446802, 0.0011520398865369234, 0.0012638544748452578, 0.0012127625046890569, 0.0013585236368023536, 0.0013634077040478587, 0.0013628760881891305, 0.001397172184872695, 0.0013620520687916062, 0.0013498805928975344, 0.0013392700877209957, 0.0013735418641855094, 0.0014146601844748313, 0.0012871006355536256, 0.0013002998841164465, 0.00128830322699452, 0.0013226033823395317, 0.0013094382064247673, 0.0011722540895623918, 0.0011836633837612515, 0.001193141432436691, 0.001194568768947978, 0.0012015805008228529, 0.0012120828667486255, 0.0012023986571214416, 0.0013637569129721007, 0.0012448863179253583, 0.0014456870002587411, 0.0013264566818675535, 0.0013818675228817897, 0.0012274348409846425, 0.001188560930448909, 0.0011867421603393416, 0.0012059171391694352, 0.0012102999756935723, 0.0012129148370911215, 0.001221371116116643, 0.0011914402549696524, 0.0012170341418146393, 0.0015513496040258296, 0.0013178674894016842, 0.0013567783739850965, 0.0013192559072617875, 0.001275461259146416, 0.0011802803302660239, 0.0011945537204832532, 0.0012071710458935, 0.001207430278449211, 0.0013027450687057057, 0.0012610469522431147, 0.0012108821389373652, 0.0012248598179931556, 0.0012155664640717035, 0.0012149432793172987, 0.001218790605361032, 0.0012179580462966548, 0.0012156773943367393, 0.0012100140271751687, 0.0012130806287534016, 0.0012038093991577625, 0.0012021907442790825, 0.0012079127252015264, 0.00120417102305002, 0.0012077847441528426, 0.001208411491757562, 0.001215231350384826, 0.0012150543449489877, 0.001245268236109337, 0.0012146545801485001, 0.0012122299576308145, 0.001276155977055084, 0.0012837752786486647, 0.001288436954234575, 0.0012900030002171217, 0.0012958426255906045, 0.001302476950682873, 0.001317041601205981, 0.003409260629438037, 0.0012852129807998968, 0.0012882570248790258, 0.0013024889333390218, 0.001309951837716061, 0.0013172346990295621, 0.0011839792998724204, 0.0011902160422746525, 0.0011715646020981462, 0.0033749761872080178, 0.0011437306299719007, 0.001161189230028973, 0.0011633142319962728, 0.0011705864155881626, 0.0011729553536793521, 0.001172228559288521, 0.001160353091870283, 0.0017062882340491512, 0.0012153955122326003, 0.0012230001400809648, 0.001243424650584889, 0.0012331784886936115, 0.0012401147247400395, 0.0012335451157373745, 0.0011961391888732134, 0.0012065730718246034, 0.001218326211105599, 0.0012237102356414463, 0.0012287268155189448, 0.0012392629111228988, 0.0012348015132078597, 0.0011505633700898913, 0.0011412786742195833, 0.0011408603479435972, 0.0011570733961064456, 0.0011560821169337562, 0.0011614681827987348, 0.0011642341187960187, 0.0011419724878790074, 0.0011356236510585214, 0.0011326869798087796, 0.0012296390189074499, 0.0011634624638962884, 0.0011678767867039802, 0.0011564084426175024, 0.001146961393397908, 0.0018492043451514356, 0.0011251057703914337, 0.0013711811422348716, 0.0011472820011942193, 0.0011577794685699912, 0.0011675006948238195, 0.0011636702300504196, 0.0011676501396090486, 0.0011407791604396216, 0.0011555863934200863, 0.0011537077479324368]
[893.7636395066532, 902.2715282370216, 850.6501855156686, 814.6179779814818, 804.3322188564028, 745.0912835598551, 794.0832999640445, 806.9080300078136, 806.2684604383628, 805.3972039914729, 838.5152772142054, 803.5525791123799, 814.1809472642221, 747.8321646384873, 832.3411942372464, 798.6643710533158, 801.9292525480395, 695.1545421792571, 719.7289690750811, 808.9407953606014, 805.463312068707, 789.2307027029916, 787.2454510744784, 875.9459639476567, 836.5177741660407, 875.8121293692348, 799.6894864935001, 868.025501274994, 791.2303353773676, 824.5637510506581, 736.0931918371076, 733.4563220019019, 733.7424206544791, 715.7313971943381, 734.1863229114186, 740.8062648367204, 746.675378751777, 728.0447914072031, 706.8835406371694, 776.9400250275428, 769.0533639318901, 776.2147754088128, 756.0845627289385, 763.6862855333633, 853.0573780069345, 844.8347847192534, 838.123606149316, 837.1221699364123, 832.2372070079292, 825.0260996448773, 831.6709221831745, 733.2685103099878, 803.286200194192, 691.7126596704719, 753.8881696400922, 723.6583706045633, 814.7071979786762, 841.3535851479728, 842.6430217276987, 829.2443713742565, 826.2414443385754, 824.460192438782, 818.7519639235503, 839.3203065188287, 821.6696357498779, 644.6000291648962, 758.8016306965756, 737.040049557117, 758.003048912302, 784.0300854525645, 847.2563461043273, 837.1327156349678, 828.3830227719219, 828.205170806526, 767.6098908541738, 792.9918852119093, 825.8442071641842, 816.4199570514344, 822.6617215568497, 823.0836920731973, 820.4854842180035, 821.0463431319479, 822.5866538758743, 826.4367003534201, 824.3475135099885, 830.6962885483728, 831.8147554859689, 827.8743812663796, 830.4468226341477, 827.96210569907, 827.5326797377274, 822.8885797616488, 823.0084556769217, 803.0398359186911, 823.279322651336, 824.9259917271825, 783.6032726247507, 778.9525290225451, 776.1342118552261, 775.1919955470561, 771.6986463107211, 767.7679052022472, 759.2774587259247, 293.3187305673472, 778.0811545940155, 776.2426136150162, 767.7608418801914, 763.386844621348, 759.1661537133235, 844.6093610823726, 840.1836006922526, 853.559418071447, 296.29838687165966, 874.3317471742172, 861.1860790123319, 859.6129682725345, 854.2726847701788, 852.5473683744041, 853.0759569677654, 861.8066405874592, 586.06745334399, 822.777433300759, 817.6613944899452, 804.2304771178651, 810.9126206534521, 806.3770069415361, 810.6716059608663, 836.0231060918752, 828.7935669638148, 820.7982319386581, 817.1869212778289, 813.8505543867833, 806.9312742474459, 809.8467561819917, 869.1394372496596, 876.2101865119043, 876.5314718865476, 864.2494100763203, 864.9904581624978, 860.9792457597481, 858.9337692955951, 875.6778386643151, 880.5734180226823, 882.8564447424126, 813.2468022107113, 859.5034485694765, 856.2547105865787, 864.7463674136833, 871.868927547308, 540.7731182451379, 888.8053250780961, 729.2982445558667, 871.6252838962769, 863.7223470849164, 856.530539496513, 859.349989521234, 856.4209141745309, 876.593853287634, 865.3615218161135, 866.7706373577737]
Elapsed: 0.05500400227465434~0.011097752483322962
Time per graph: 0.001268657163779029~0.0002575055233801055
Speed: 803.0466801241977~78.72755003420066
Total Time: 0.0503
best val loss: 0.30910876393318176 test_score: 0.9070

Testing...
Test loss: 0.6086 score: 0.8605 time: 0.05s
test Score 0.8605
Epoch Time List: [0.4711068111937493, 0.17783386819064617, 0.17739120917394757, 0.18486022390425205, 0.18698138114996254, 0.19153064512647688, 0.19711823901161551, 0.18995680892840028, 0.19201195216737688, 0.18975565768778324, 0.19351811916567385, 0.19647868513129652, 0.1899943680036813, 0.2038509042467922, 0.19185978826135397, 0.20034926873631775, 0.1958206903655082, 0.20490386290475726, 0.19661809783428907, 0.1822536790277809, 0.18505774205550551, 0.18483520695008337, 0.1877669550012797, 0.18444787105545402, 0.19033537292852998, 0.18047863594256341, 0.183275650953874, 0.1873333938419819, 0.21351710497401655, 0.2002280382439494, 0.20756256207823753, 0.20843703602440655, 0.20242110989056528, 0.20766091998666525, 0.21117226500064135, 0.2065688988659531, 0.20400089398026466, 0.19936714693903923, 0.20398422423750162, 0.19712390913628042, 0.20363928982988, 0.2042218279093504, 0.20922232512384653, 0.2087039367761463, 0.20315873017534614, 0.19418918807059526, 0.19707828364335, 0.20511539513245225, 0.2061792358290404, 0.20018151286058128, 0.19753739493899047, 0.20014700619503856, 0.1980231530033052, 0.20641368301585317, 0.20815901388414204, 0.20793032669462264, 0.19778333511203527, 0.18937340402044356, 0.18815133231692016, 0.18892964092083275, 0.19485076609998941, 0.1943234638310969, 0.19488284387625754, 0.1959059580694884, 0.1939656229224056, 0.2247717382851988, 0.2207756380084902, 0.20321447704918683, 0.19147701491601765, 0.19655899493955076, 0.19399438588880002, 0.19329873914830387, 0.19789309590123594, 0.19940075813792646, 0.20435295323841274, 0.19018180179409683, 0.19859999394975603, 0.19798283278942108, 0.20110692689195275, 0.20134542416781187, 0.20163669297471642, 0.20082075940445065, 0.1994052620138973, 0.1995548012200743, 0.19877533009275794, 0.19971981714479625, 0.2003507148474455, 0.1984347349498421, 0.20145052601583302, 0.19980896194465458, 0.20027146418578923, 0.20328134903684258, 0.2155721231829375, 0.20200208690948784, 0.1984251954127103, 0.197484971024096, 0.22764042392373085, 0.19068951485678554, 0.19161004479974508, 0.19218252110294998, 0.1953248418867588, 0.19858991517685354, 0.216038255719468, 0.3008588901720941, 0.20364028098993003, 0.2027148602064699, 0.20776056707836688, 0.21149147697724402, 0.20482467720285058, 0.19469972606748343, 0.20267251320183277, 0.19299897295422852, 0.2856463617645204, 0.1874676081351936, 0.19956992287188768, 0.19352152128703892, 0.1920370061416179, 0.19302241411060095, 0.19200019584968686, 0.19198051490820944, 0.33242117892950773, 0.19626647722907364, 0.19650631304830313, 0.19910910492762923, 0.20035161869600415, 0.1990592151414603, 0.19890974997542799, 0.26753019518218935, 0.19063913403078914, 0.1932914440985769, 0.19380652904510498, 0.19686261285096407, 0.198636680142954, 0.2577872450929135, 0.27110927109606564, 0.1837115699891001, 0.1882441013585776, 0.18948991713114083, 0.1935883138794452, 0.1919731020461768, 0.18903894699178636, 0.18636544817127287, 0.20893822377547622, 0.18583082035183907, 0.20222365576773882, 0.18837120290845633, 0.1910485418047756, 0.18861428298987448, 0.18733343994244933, 0.2594020061660558, 0.1843417740892619, 0.2025183599907905, 0.19892084412276745, 0.18945179972797632, 0.20367991109378636, 0.19014518102630973, 0.18982277112081647, 0.2327820740174502, 0.187864585313946, 0.19172411202453077]
Total Epoch List: [57, 52, 51]
Total Time List: [0.05442717298865318, 0.0571957528591156, 0.05026594805531204]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c4424a3520>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7121;  Loss pred: 0.7016; Loss self: 1.0583; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5116 time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6941 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7020;  Loss pred: 0.6914; Loss self: 1.0553; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6942 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6820;  Loss pred: 0.6719; Loss self: 1.0120; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6944 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6806;  Loss pred: 0.6700; Loss self: 1.0601; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6332;  Loss pred: 0.6226; Loss self: 1.0549; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5953;  Loss pred: 0.5852; Loss self: 1.0075; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6949 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5599;  Loss pred: 0.5492; Loss self: 1.0713; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6948 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.5250;  Loss pred: 0.5127; Loss self: 1.2242; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000350
Train loss: 0.5002;  Loss pred: 0.4866; Loss self: 1.3585; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6944 score: 0.5000 time: 0.05s
Epoch 10/1000, LR 0.000400
Train loss: 0.4628;  Loss pred: 0.4482; Loss self: 1.4512; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6943 score: 0.5000 time: 0.05s
Epoch 11/1000, LR 0.000450
Train loss: 0.4326;  Loss pred: 0.4181; Loss self: 1.4522; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6920 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6940 score: 0.5000 time: 0.05s
Epoch 12/1000, LR 0.000450
Train loss: 0.3963;  Loss pred: 0.3807; Loss self: 1.5611; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6912 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5000 time: 0.05s
Epoch 13/1000, LR 0.000450
Train loss: 0.3733;  Loss pred: 0.3572; Loss self: 1.6152; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6900 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6925 score: 0.5000 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.3375;  Loss pred: 0.3212; Loss self: 1.6260; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6884 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6913 score: 0.5000 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.3129;  Loss pred: 0.2967; Loss self: 1.6176; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6858 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6892 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.3022;  Loss pred: 0.2856; Loss self: 1.6658; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6820 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6862 score: 0.5000 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.2681;  Loss pred: 0.2509; Loss self: 1.7181; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6767 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6820 score: 0.5000 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.2498;  Loss pred: 0.2323; Loss self: 1.7437; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6688 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6754 score: 0.5000 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.2323;  Loss pred: 0.2143; Loss self: 1.8040; time: 0.09s
Val loss: 0.6584 score: 0.5814 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6665 score: 0.5000 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.2181;  Loss pred: 0.1997; Loss self: 1.8426; time: 0.11s
Val loss: 0.6445 score: 0.7209 time: 0.05s
Test loss: 0.6545 score: 0.6364 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.2018;  Loss pred: 0.1831; Loss self: 1.8736; time: 0.09s
Val loss: 0.6267 score: 0.7907 time: 0.05s
Test loss: 0.6387 score: 0.7045 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1857;  Loss pred: 0.1663; Loss self: 1.9430; time: 0.09s
Val loss: 0.6045 score: 0.8837 time: 0.05s
Test loss: 0.6190 score: 0.8636 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1704;  Loss pred: 0.1507; Loss self: 1.9670; time: 0.09s
Val loss: 0.5768 score: 0.8837 time: 0.05s
Test loss: 0.5930 score: 0.8636 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1569;  Loss pred: 0.1367; Loss self: 2.0212; time: 0.09s
Val loss: 0.5441 score: 0.8605 time: 0.05s
Test loss: 0.5617 score: 0.8636 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.1433;  Loss pred: 0.1232; Loss self: 2.0118; time: 0.09s
Val loss: 0.5082 score: 0.8605 time: 0.05s
Test loss: 0.5252 score: 0.8636 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.1337;  Loss pred: 0.1128; Loss self: 2.0834; time: 0.09s
Val loss: 0.4720 score: 0.8605 time: 0.05s
Test loss: 0.4863 score: 0.8636 time: 0.05s
Epoch 27/1000, LR 0.000450
Train loss: 0.1223;  Loss pred: 0.1012; Loss self: 2.1052; time: 0.09s
Val loss: 0.4389 score: 0.8605 time: 0.05s
Test loss: 0.4497 score: 0.8636 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.1162;  Loss pred: 0.0949; Loss self: 2.1274; time: 0.09s
Val loss: 0.4123 score: 0.8605 time: 0.05s
Test loss: 0.4171 score: 0.8636 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.1075;  Loss pred: 0.0857; Loss self: 2.1811; time: 0.09s
Val loss: 0.3925 score: 0.8605 time: 0.05s
Test loss: 0.3907 score: 0.8636 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0992;  Loss pred: 0.0767; Loss self: 2.2524; time: 0.10s
Val loss: 0.3796 score: 0.8372 time: 0.05s
Test loss: 0.3702 score: 0.8636 time: 0.05s
Epoch 31/1000, LR 0.000450
Train loss: 0.0896;  Loss pred: 0.0673; Loss self: 2.2301; time: 0.09s
Val loss: 0.3735 score: 0.8372 time: 0.05s
Test loss: 0.3564 score: 0.8636 time: 0.05s
Epoch 32/1000, LR 0.000450
Train loss: 0.0860;  Loss pred: 0.0630; Loss self: 2.2982; time: 0.09s
Val loss: 0.3705 score: 0.8372 time: 0.05s
Test loss: 0.3467 score: 0.8636 time: 0.05s
Epoch 33/1000, LR 0.000449
Train loss: 0.0807;  Loss pred: 0.0575; Loss self: 2.3155; time: 0.09s
Val loss: 0.3724 score: 0.8372 time: 0.05s
Test loss: 0.3422 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0738;  Loss pred: 0.0507; Loss self: 2.3117; time: 0.09s
Val loss: 0.3770 score: 0.8372 time: 0.05s
Test loss: 0.3412 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0677;  Loss pred: 0.0441; Loss self: 2.3562; time: 0.10s
Val loss: 0.3837 score: 0.8372 time: 0.05s
Test loss: 0.3422 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0653;  Loss pred: 0.0418; Loss self: 2.3495; time: 0.09s
Val loss: 0.3931 score: 0.8372 time: 0.05s
Test loss: 0.3451 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0602;  Loss pred: 0.0364; Loss self: 2.3800; time: 0.09s
Val loss: 0.3999 score: 0.8372 time: 0.05s
Test loss: 0.3468 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0576;  Loss pred: 0.0338; Loss self: 2.3774; time: 0.09s
Val loss: 0.4093 score: 0.8372 time: 0.05s
Test loss: 0.3497 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0535;  Loss pred: 0.0294; Loss self: 2.4137; time: 0.09s
Val loss: 0.4186 score: 0.8372 time: 0.05s
Test loss: 0.3526 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0509;  Loss pred: 0.0264; Loss self: 2.4454; time: 0.09s
Val loss: 0.4252 score: 0.8140 time: 0.05s
Test loss: 0.3537 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0484;  Loss pred: 0.0242; Loss self: 2.4167; time: 0.09s
Val loss: 0.4353 score: 0.8140 time: 0.05s
Test loss: 0.3571 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0462;  Loss pred: 0.0214; Loss self: 2.4768; time: 0.09s
Val loss: 0.4443 score: 0.8140 time: 0.05s
Test loss: 0.3598 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0446;  Loss pred: 0.0201; Loss self: 2.4504; time: 0.09s
Val loss: 0.4527 score: 0.8140 time: 0.05s
Test loss: 0.3628 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0433;  Loss pred: 0.0184; Loss self: 2.4902; time: 0.11s
Val loss: 0.4584 score: 0.8140 time: 0.05s
Test loss: 0.3640 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0415;  Loss pred: 0.0169; Loss self: 2.4690; time: 0.09s
Val loss: 0.4651 score: 0.8140 time: 0.05s
Test loss: 0.3658 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0403;  Loss pred: 0.0151; Loss self: 2.5167; time: 0.09s
Val loss: 0.4724 score: 0.8140 time: 0.05s
Test loss: 0.3679 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0391;  Loss pred: 0.0142; Loss self: 2.4908; time: 0.09s
Val loss: 0.4768 score: 0.8140 time: 0.05s
Test loss: 0.3690 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0380;  Loss pred: 0.0129; Loss self: 2.5094; time: 0.09s
Val loss: 0.4811 score: 0.8140 time: 0.05s
Test loss: 0.3700 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0372;  Loss pred: 0.0121; Loss self: 2.5092; time: 0.09s
Val loss: 0.4859 score: 0.8140 time: 0.05s
Test loss: 0.3713 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0360;  Loss pred: 0.0110; Loss self: 2.5071; time: 0.10s
Val loss: 0.4899 score: 0.8140 time: 0.05s
Test loss: 0.3724 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0359;  Loss pred: 0.0105; Loss self: 2.5377; time: 0.10s
Val loss: 0.4873 score: 0.8140 time: 0.05s
Test loss: 0.3694 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0350;  Loss pred: 0.0098; Loss self: 2.5166; time: 0.09s
Val loss: 0.4875 score: 0.8140 time: 0.05s
Test loss: 0.3679 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.0860,   Val_Loss: 0.3705,   Val_Precision: 1.0000,   Val_Recall: 0.6818,   Val_accuracy: 0.8108,   Val_Score: 0.8372,   Val_Loss: 0.3705,   Test_Precision: 0.9444,   Test_Recall: 0.7727,   Test_accuracy: 0.8500,   Test_Score: 0.8636,   Test_loss: 0.3467


[0.07393697486259043, 0.060057081980630755, 0.05296889808960259, 0.05596289713867009, 0.053757603047415614, 0.054823487997055054, 0.054723114939406514, 0.05546205607242882, 0.0546168009750545, 0.05740484595298767, 0.05501262494362891, 0.05595058994367719, 0.05553290294483304, 0.05520360101945698, 0.055743518052622676, 0.05529092508368194, 0.05578906089067459, 0.054942982038483024, 0.058414262952283025, 0.058252014918252826, 0.05488253012299538, 0.05416415701620281, 0.054844867903739214, 0.05427024210803211, 0.05410075606778264, 0.05528801400214434, 0.05498779215849936, 0.054460581159219146, 0.05403559608384967, 0.05430413084104657, 0.05402600206434727, 0.05408818880096078, 0.05396673013456166, 0.05390595202334225, 0.057899039005860686, 0.05384577508084476, 0.053428603103384376, 0.0539412060752511, 0.05390874180011451, 0.05402652104385197, 0.053707255981862545, 0.05348135204985738, 0.054020036943256855, 0.0563101340085268, 0.05455622193403542, 0.05901927105151117, 0.05412322701886296, 0.05450830003246665, 0.05394319910556078, 0.057702602818608284, 0.05657501891255379, 0.05451028188690543]
[0.0016803857923316007, 0.0013649336813779717, 0.0012038385929455135, 0.0012718840258788657, 0.0012217637056230822, 0.001245988363569433, 0.0012437071577137845, 0.0012605012743733823, 0.0012412909312512386, 0.0013046555898406289, 0.0012502869305370207, 0.0012716043169017542, 0.0012621114305643873, 0.0012546272958967495, 0.0012668981375596063, 0.0012566119337200441, 0.0012679332020607862, 0.0012487041372382505, 0.0013275968852791596, 0.0013239094299602914, 0.0012473302300680768, 0.0012310035685500638, 0.0012464742705395276, 0.001233414593364366, 0.001229562637904151, 0.0012565457727760077, 0.0012497225490568037, 0.0012377404808913443, 0.0012280817291784015, 0.0012341847918419676, 0.0012278636832806196, 0.001229277018203654, 0.0012265165939673104, 0.0012251352732577784, 0.0013158872501331973, 0.0012237676154737446, 0.0012142864341678267, 0.0012259365017102523, 0.0012251986772753298, 0.001227875478269363, 0.0012206194541332397, 0.001215485273860395, 0.0012277281123467467, 0.0012797757729210637, 0.0012399141348644414, 0.0013413470693525267, 0.0012300733413377945, 0.0012388250007378783, 0.0012259817978536541, 0.0013114227913320065, 0.0012857958843762224, 0.0012388700428842144]
[595.1014371601303, 732.6363277887962, 830.676143679056, 786.2352067115592, 818.4888742377677, 802.575713576698, 804.0477967805755, 793.3351757197689, 805.6129105784943, 766.485812644359, 799.8164065991493, 786.4081512686947, 792.3230673482, 797.0494530690456, 789.3294420072908, 795.7906280896312, 788.685080866002, 800.8302128410437, 753.2406945875936, 755.3386790439232, 801.7123099352943, 812.3453298984738, 802.2628494105675, 810.757392834404, 813.2973214805456, 795.8325288785643, 800.177608025657, 807.9238058691122, 814.2780535209246, 810.251436097785, 814.4226542544112, 813.48628925098, 815.317138731392, 816.2363959539608, 759.9435285194666, 817.1486051401027, 823.5289235404484, 815.7029329047158, 816.1941557297955, 814.4148308991856, 819.2561544171838, 822.7166725138419, 814.5125862505056, 781.3868813264992, 806.5074603809795, 745.5192044238811, 812.9596556514483, 807.2165151691097, 815.6727952655708, 762.5305939546043, 777.7284187568616, 807.187166841083]
Elapsed: 0.055474587926605284~0.0030018509693797613
Time per graph: 0.0012607860892410291~6.822388566772183e-05
Speed: 795.0468348158679~35.193036522161535
Total Time: 0.0549
best val loss: 0.37052181363105774 test_score: 0.8636

Testing...
Test loss: 0.6190 score: 0.8636 time: 0.05s
test Score 0.8636
Epoch Time List: [0.28064270014874637, 0.21783867431804538, 0.18090482498519123, 0.18511765589937568, 0.18535543885082006, 0.187170818913728, 0.19141687895171344, 0.1955487560480833, 0.19760408089496195, 0.21455286093987525, 0.19219358381815255, 0.19284596294164658, 0.1940754281822592, 0.192196199670434, 0.19290860509499907, 0.1920457947999239, 0.1935288798995316, 0.19085870846174657, 0.1988603111822158, 0.21975971665233374, 0.1953399188350886, 0.19002460199408233, 0.18900242913514376, 0.1882572090253234, 0.1867821749765426, 0.18794098193757236, 0.19118930399417877, 0.18825569795444608, 0.18693836010061204, 0.19707926525734365, 0.18877440900541842, 0.18813318200409412, 0.18609528173692524, 0.1869562140200287, 0.19696339895017445, 0.18689232598990202, 0.18726184288971126, 0.18663350422866642, 0.18618176458403468, 0.1871318907942623, 0.18823203584179282, 0.1874851672910154, 0.18622285104356706, 0.21050697285681963, 0.18928383593447506, 0.19203894515521824, 0.18518254323862493, 0.1866869197692722, 0.1854751817882061, 0.2085506059229374, 0.1967226890847087, 0.18997077690437436]
Total Epoch List: [52]
Total Time List: [0.0548607730306685]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c4387c6a40>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7261;  Loss pred: 0.7167; Loss self: 0.9332; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6948 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7438;  Loss pred: 0.7344; Loss self: 0.9416; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6947 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000050
Train loss: 0.7250;  Loss pred: 0.7150; Loss self: 0.9986; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6950 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6935 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6915;  Loss pred: 0.6820; Loss self: 0.9527; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6956 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6938 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6764;  Loss pred: 0.6668; Loss self: 0.9524; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6962 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6941 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.6423;  Loss pred: 0.6327; Loss self: 0.9683; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6965 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6943 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.6057;  Loss pred: 0.5970; Loss self: 0.8700; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6967 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6942 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.5571;  Loss pred: 0.5475; Loss self: 0.9569; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6969 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6942 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000350
Train loss: 0.5162;  Loss pred: 0.5077; Loss self: 0.8457; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6970 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6939 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 10/1000, LR 0.000400
Train loss: 0.4739;  Loss pred: 0.4639; Loss self: 1.0009; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6969 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 11/1000, LR 0.000450
Train loss: 0.4506;  Loss pred: 0.4405; Loss self: 1.0047; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6967 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 12/1000, LR 0.000450
Train loss: 0.4060;  Loss pred: 0.3949; Loss self: 1.1150; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6960 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6914 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 13/1000, LR 0.000450
Train loss: 0.3754;  Loss pred: 0.3642; Loss self: 1.1233; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6950 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6898 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 14/1000, LR 0.000450
Train loss: 0.3476;  Loss pred: 0.3357; Loss self: 1.1832; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6876 score: 0.5116 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.3292;  Loss pred: 0.3163; Loss self: 1.2807; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6918 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6845 score: 0.5116 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2921;  Loss pred: 0.2795; Loss self: 1.2608; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6890 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6802 score: 0.5116 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.2713;  Loss pred: 0.2577; Loss self: 1.3591; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6847 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6740 score: 0.5116 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.2516;  Loss pred: 0.2378; Loss self: 1.3808; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6791 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6659 score: 0.5116 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.2293;  Loss pred: 0.2151; Loss self: 1.4151; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6710 score: 0.5000 time: 0.05s
Test loss: 0.6546 score: 0.5581 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.2073;  Loss pred: 0.1922; Loss self: 1.5099; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6595 score: 0.5000 time: 0.05s
Test loss: 0.6388 score: 0.5814 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1950;  Loss pred: 0.1787; Loss self: 1.6309; time: 0.09s
Val loss: 0.6434 score: 0.5682 time: 0.05s
Test loss: 0.6175 score: 0.6279 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1763;  Loss pred: 0.1598; Loss self: 1.6424; time: 0.09s
Val loss: 0.6214 score: 0.6364 time: 0.05s
Test loss: 0.5891 score: 0.7442 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1562;  Loss pred: 0.1385; Loss self: 1.7755; time: 0.09s
Val loss: 0.5937 score: 0.7273 time: 0.05s
Test loss: 0.5541 score: 0.7907 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1441;  Loss pred: 0.1263; Loss self: 1.7754; time: 0.09s
Val loss: 0.5608 score: 0.7500 time: 0.05s
Test loss: 0.5122 score: 0.8372 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.1323;  Loss pred: 0.1138; Loss self: 1.8528; time: 0.11s
Val loss: 0.5233 score: 0.7955 time: 0.05s
Test loss: 0.4647 score: 0.9070 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.1206;  Loss pred: 0.1013; Loss self: 1.9288; time: 0.09s
Val loss: 0.4840 score: 0.8409 time: 0.05s
Test loss: 0.4162 score: 0.9070 time: 0.05s
Epoch 27/1000, LR 0.000450
Train loss: 0.1104;  Loss pred: 0.0908; Loss self: 1.9577; time: 0.09s
Val loss: 0.4461 score: 0.8636 time: 0.05s
Test loss: 0.3698 score: 0.9070 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.1007;  Loss pred: 0.0808; Loss self: 1.9847; time: 0.09s
Val loss: 0.4120 score: 0.8636 time: 0.05s
Test loss: 0.3297 score: 0.9070 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.0934;  Loss pred: 0.0728; Loss self: 2.0629; time: 0.10s
Val loss: 0.3829 score: 0.8409 time: 0.05s
Test loss: 0.2968 score: 0.9070 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0835;  Loss pred: 0.0627; Loss self: 2.0736; time: 0.09s
Val loss: 0.3600 score: 0.8636 time: 0.05s
Test loss: 0.2716 score: 0.9070 time: 0.05s
Epoch 31/1000, LR 0.000450
Train loss: 0.0777;  Loss pred: 0.0567; Loss self: 2.1017; time: 0.09s
Val loss: 0.3439 score: 0.8636 time: 0.05s
Test loss: 0.2531 score: 0.9070 time: 0.05s
Epoch 32/1000, LR 0.000450
Train loss: 0.0709;  Loss pred: 0.0496; Loss self: 2.1287; time: 0.09s
Val loss: 0.3334 score: 0.8636 time: 0.05s
Test loss: 0.2417 score: 0.9070 time: 0.05s
Epoch 33/1000, LR 0.000449
Train loss: 0.0656;  Loss pred: 0.0439; Loss self: 2.1679; time: 0.09s
Val loss: 0.3268 score: 0.8636 time: 0.05s
Test loss: 0.2344 score: 0.9070 time: 0.05s
Epoch 34/1000, LR 0.000449
Train loss: 0.0611;  Loss pred: 0.0398; Loss self: 2.1355; time: 0.11s
Val loss: 0.3233 score: 0.8636 time: 0.05s
Test loss: 0.2300 score: 0.9070 time: 0.04s
Epoch 35/1000, LR 0.000449
Train loss: 0.0583;  Loss pred: 0.0362; Loss self: 2.2084; time: 0.09s
Val loss: 0.3214 score: 0.8864 time: 0.05s
Test loss: 0.2279 score: 0.9070 time: 0.05s
Epoch 36/1000, LR 0.000449
Train loss: 0.0535;  Loss pred: 0.0311; Loss self: 2.2359; time: 0.09s
Val loss: 0.3213 score: 0.8864 time: 0.05s
Test loss: 0.2284 score: 0.9070 time: 0.05s
Epoch 37/1000, LR 0.000449
Train loss: 0.0509;  Loss pred: 0.0287; Loss self: 2.2184; time: 0.10s
Val loss: 0.3224 score: 0.8864 time: 0.05s
Test loss: 0.2297 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0489;  Loss pred: 0.0263; Loss self: 2.2617; time: 0.09s
Val loss: 0.3239 score: 0.8864 time: 0.05s
Test loss: 0.2316 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0450;  Loss pred: 0.0221; Loss self: 2.2862; time: 0.09s
Val loss: 0.3266 score: 0.8864 time: 0.05s
Test loss: 0.2354 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0429;  Loss pred: 0.0201; Loss self: 2.2852; time: 0.09s
Val loss: 0.3288 score: 0.8864 time: 0.05s
Test loss: 0.2384 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0404;  Loss pred: 0.0175; Loss self: 2.2964; time: 0.09s
Val loss: 0.3313 score: 0.8864 time: 0.05s
Test loss: 0.2425 score: 0.9070 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0389;  Loss pred: 0.0159; Loss self: 2.2992; time: 0.09s
Val loss: 0.3334 score: 0.8864 time: 0.05s
Test loss: 0.2461 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0375;  Loss pred: 0.0142; Loss self: 2.3294; time: 0.09s
Val loss: 0.3356 score: 0.8864 time: 0.05s
Test loss: 0.2495 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0356;  Loss pred: 0.0128; Loss self: 2.2731; time: 0.09s
Val loss: 0.3380 score: 0.8864 time: 0.05s
Test loss: 0.2538 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0345;  Loss pred: 0.0114; Loss self: 2.3136; time: 0.09s
Val loss: 0.3400 score: 0.8864 time: 0.05s
Test loss: 0.2581 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0340;  Loss pred: 0.0104; Loss self: 2.3608; time: 0.09s
Val loss: 0.3420 score: 0.8864 time: 0.05s
Test loss: 0.2620 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0337;  Loss pred: 0.0099; Loss self: 2.3752; time: 0.10s
Val loss: 0.3439 score: 0.8864 time: 0.05s
Test loss: 0.2660 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0323;  Loss pred: 0.0088; Loss self: 2.3545; time: 0.09s
Val loss: 0.3453 score: 0.8864 time: 0.05s
Test loss: 0.2695 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0320;  Loss pred: 0.0082; Loss self: 2.3819; time: 0.09s
Val loss: 0.3468 score: 0.8864 time: 0.05s
Test loss: 0.2730 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0314;  Loss pred: 0.0076; Loss self: 2.3794; time: 0.09s
Val loss: 0.3485 score: 0.8864 time: 0.05s
Test loss: 0.2770 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0305;  Loss pred: 0.0069; Loss self: 2.3629; time: 0.09s
Val loss: 0.3499 score: 0.8864 time: 0.05s
Test loss: 0.2802 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0301;  Loss pred: 0.0063; Loss self: 2.3831; time: 0.09s
Val loss: 0.3517 score: 0.8864 time: 0.05s
Test loss: 0.2839 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0297;  Loss pred: 0.0059; Loss self: 2.3762; time: 0.09s
Val loss: 0.3533 score: 0.8864 time: 0.05s
Test loss: 0.2874 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0294;  Loss pred: 0.0056; Loss self: 2.3805; time: 0.09s
Val loss: 0.3537 score: 0.8864 time: 0.05s
Test loss: 0.2897 score: 0.8837 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0291;  Loss pred: 0.0053; Loss self: 2.3767; time: 0.09s
Val loss: 0.3539 score: 0.8864 time: 0.05s
Test loss: 0.2919 score: 0.8837 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 56/1000, LR 0.000448
Train loss: 0.0287;  Loss pred: 0.0050; Loss self: 2.3638; time: 0.09s
Val loss: 0.3548 score: 0.8864 time: 0.05s
Test loss: 0.2956 score: 0.8837 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 035,   Train_Loss: 0.0535,   Val_Loss: 0.3213,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.3213,   Test_Precision: 0.9500,   Test_Recall: 0.8636,   Test_accuracy: 0.9048,   Test_Score: 0.9070,   Test_loss: 0.2284


[0.07393697486259043, 0.060057081980630755, 0.05296889808960259, 0.05596289713867009, 0.053757603047415614, 0.054823487997055054, 0.054723114939406514, 0.05546205607242882, 0.0546168009750545, 0.05740484595298767, 0.05501262494362891, 0.05595058994367719, 0.05553290294483304, 0.05520360101945698, 0.055743518052622676, 0.05529092508368194, 0.05578906089067459, 0.054942982038483024, 0.058414262952283025, 0.058252014918252826, 0.05488253012299538, 0.05416415701620281, 0.054844867903739214, 0.05427024210803211, 0.05410075606778264, 0.05528801400214434, 0.05498779215849936, 0.054460581159219146, 0.05403559608384967, 0.05430413084104657, 0.05402600206434727, 0.05408818880096078, 0.05396673013456166, 0.05390595202334225, 0.057899039005860686, 0.05384577508084476, 0.053428603103384376, 0.0539412060752511, 0.05390874180011451, 0.05402652104385197, 0.053707255981862545, 0.05348135204985738, 0.054020036943256855, 0.0563101340085268, 0.05455622193403542, 0.05901927105151117, 0.05412322701886296, 0.05450830003246665, 0.05394319910556078, 0.057702602818608284, 0.05657501891255379, 0.05451028188690543, 0.04928577598184347, 0.051019777078181505, 0.050430546049028635, 0.05009354301728308, 0.05060830409638584, 0.05065367300994694, 0.05082302610389888, 0.05167575506493449, 0.05089482408948243, 0.05065886699594557, 0.05005830596201122, 0.05002222605980933, 0.05590909416787326, 0.05033516604453325, 0.05017567798495293, 0.05035396991297603, 0.05052982806228101, 0.050594909116625786, 0.05386624904349446, 0.050357962027192116, 0.05025965301319957, 0.050756730139255524, 0.050527850864455104, 0.05032126582227647, 0.0508293102029711, 0.0506778780836612, 0.050850218860432506, 0.050821693148463964, 0.05048375600017607, 0.05031802994199097, 0.05048733693547547, 0.05262675788253546, 0.05096663488075137, 0.04960577585734427, 0.05029090400785208, 0.05044405208900571, 0.05024193902499974, 0.050466018030419946, 0.050444212974980474, 0.05003659799695015, 0.05001972196623683, 0.050545898033306, 0.05051519814878702, 0.05345365894027054, 0.05028198496438563, 0.05048503400757909, 0.05029981699772179, 0.05405315011739731, 0.05036340095102787, 0.05045707896351814, 0.05046781408600509, 0.05066436994820833, 0.05140785709954798, 0.08844150300137699, 0.04899672092869878, 0.0495283561758697]
[0.0016803857923316007, 0.0013649336813779717, 0.0012038385929455135, 0.0012718840258788657, 0.0012217637056230822, 0.001245988363569433, 0.0012437071577137845, 0.0012605012743733823, 0.0012412909312512386, 0.0013046555898406289, 0.0012502869305370207, 0.0012716043169017542, 0.0012621114305643873, 0.0012546272958967495, 0.0012668981375596063, 0.0012566119337200441, 0.0012679332020607862, 0.0012487041372382505, 0.0013275968852791596, 0.0013239094299602914, 0.0012473302300680768, 0.0012310035685500638, 0.0012464742705395276, 0.001233414593364366, 0.001229562637904151, 0.0012565457727760077, 0.0012497225490568037, 0.0012377404808913443, 0.0012280817291784015, 0.0012341847918419676, 0.0012278636832806196, 0.001229277018203654, 0.0012265165939673104, 0.0012251352732577784, 0.0013158872501331973, 0.0012237676154737446, 0.0012142864341678267, 0.0012259365017102523, 0.0012251986772753298, 0.001227875478269363, 0.0012206194541332397, 0.001215485273860395, 0.0012277281123467467, 0.0012797757729210637, 0.0012399141348644414, 0.0013413470693525267, 0.0012300733413377945, 0.0012388250007378783, 0.0012259817978536541, 0.0013114227913320065, 0.0012857958843762224, 0.0012388700428842144, 0.0011461808367870575, 0.0011865064436786395, 0.001172803396489038, 0.0011649661166810018, 0.0011769373045671125, 0.0011779923955801614, 0.0011819308396255554, 0.001201761745696151, 0.0011836005602205216, 0.0011781131859522225, 0.0011641466502793306, 0.0011633075827862634, 0.0013002114922761223, 0.0011705852568496106, 0.0011668762322082076, 0.0011710225561157215, 0.001175112280518163, 0.001176625793409902, 0.0012527034661277783, 0.001171115395981212, 0.0011688291398418505, 0.0011803890730059424, 0.0011750662991733744, 0.0011702619958668947, 0.0011820769814644442, 0.0011785553042711907, 0.001182563229312384, 0.0011818998406619527, 0.001174040837213397, 0.0011701867428369993, 0.0011741241147784993, 0.0012238780902915222, 0.0011852705786221248, 0.0011536226943568435, 0.0011695559071593505, 0.0011731174904419932, 0.0011684171866279008, 0.001173628326288836, 0.00117312123197629, 0.0011636418138825616, 0.0011632493480520194, 0.0011754860007745583, 0.0011747720499717912, 0.0012431083474481522, 0.0011693484875438518, 0.0011740705583157928, 0.0011697631859935301, 0.00125705000273017, 0.0011712418825820434, 0.00117342044101205, 0.0011736700950233743, 0.0011782411615862403, 0.0011955315604546043, 0.0020567791395669065, 0.0011394586262488088, 0.0011518222366481326]
[595.1014371601303, 732.6363277887962, 830.676143679056, 786.2352067115592, 818.4888742377677, 802.575713576698, 804.0477967805755, 793.3351757197689, 805.6129105784943, 766.485812644359, 799.8164065991493, 786.4081512686947, 792.3230673482, 797.0494530690456, 789.3294420072908, 795.7906280896312, 788.685080866002, 800.8302128410437, 753.2406945875936, 755.3386790439232, 801.7123099352943, 812.3453298984738, 802.2628494105675, 810.757392834404, 813.2973214805456, 795.8325288785643, 800.177608025657, 807.9238058691122, 814.2780535209246, 810.251436097785, 814.4226542544112, 813.48628925098, 815.317138731392, 816.2363959539608, 759.9435285194666, 817.1486051401027, 823.5289235404484, 815.7029329047158, 816.1941557297955, 814.4148308991856, 819.2561544171838, 822.7166725138419, 814.5125862505056, 781.3868813264992, 806.5074603809795, 745.5192044238811, 812.9596556514483, 807.2165151691097, 815.6727952655708, 762.5305939546043, 777.7284187568616, 807.187166841083, 872.4626759623485, 842.810424947718, 852.657830795553, 858.3940645836192, 849.6629311684605, 848.9019146065878, 846.0731935185036, 832.1116923394201, 844.8796271384713, 848.814877826649, 858.9983055485797, 859.617881630994, 769.1056462279236, 854.2735303973453, 856.988918274212, 853.9545158865233, 850.9825116958631, 849.8878790528344, 798.2735156717432, 853.8868188665183, 855.5570407282163, 847.1782930465722, 851.0158113661088, 854.5095060181205, 845.9685923002461, 848.4964569553163, 845.6207458619041, 846.0953843939304, 851.7591282203731, 854.5644582979988, 851.6987151640709, 817.0748442451523, 843.6892115912456, 866.8345420835452, 855.0253937230135, 852.4295376614255, 855.8586876713448, 852.0585074511016, 852.4268189361446, 859.3709748736505, 859.6609159288186, 850.7119602794709, 851.228968227506, 804.4351098219202, 855.1770585520161, 851.7375662962731, 854.8738855639889, 795.5133032322608, 853.7946045743047, 852.2094596694784, 852.0281842744613, 848.722683099716, 836.448014488005, 486.1970742325638, 877.6097498968278, 868.1895245485589]
Elapsed: 0.05337466881610453~0.004693832829425071
Time per graph: 0.0012271540386517563~0.0001031794544902858
Speed: 818.9902303688951~49.873202579242154
Total Time: 0.0501
best val loss: 0.32133910059928894 test_score: 0.9070

Testing...
Test loss: 0.2279 score: 0.9070 time: 0.04s
test Score 0.9070
Epoch Time List: [0.28064270014874637, 0.21783867431804538, 0.18090482498519123, 0.18511765589937568, 0.18535543885082006, 0.187170818913728, 0.19141687895171344, 0.1955487560480833, 0.19760408089496195, 0.21455286093987525, 0.19219358381815255, 0.19284596294164658, 0.1940754281822592, 0.192196199670434, 0.19290860509499907, 0.1920457947999239, 0.1935288798995316, 0.19085870846174657, 0.1988603111822158, 0.21975971665233374, 0.1953399188350886, 0.19002460199408233, 0.18900242913514376, 0.1882572090253234, 0.1867821749765426, 0.18794098193757236, 0.19118930399417877, 0.18825569795444608, 0.18693836010061204, 0.19707926525734365, 0.18877440900541842, 0.18813318200409412, 0.18609528173692524, 0.1869562140200287, 0.19696339895017445, 0.18689232598990202, 0.18726184288971126, 0.18663350422866642, 0.18618176458403468, 0.1871318907942623, 0.18823203584179282, 0.1874851672910154, 0.18622285104356706, 0.21050697285681963, 0.18928383593447506, 0.19203894515521824, 0.18518254323862493, 0.1866869197692722, 0.1854751817882061, 0.2085506059229374, 0.1967226890847087, 0.18997077690437436, 0.18680403428152204, 0.18738347780890763, 0.18826967640779912, 0.1865815008059144, 0.18821727996692061, 0.18807748099789023, 0.1893025068566203, 0.19160742382518947, 0.1903936699964106, 0.1891042513307184, 0.18755201203748584, 0.19141745241358876, 0.19352041301317513, 0.18736762017942965, 0.1875786290038377, 0.18731119111180305, 0.18767825793474913, 0.18918673298321664, 0.19309096597135067, 0.18882247526198626, 0.18814892694354057, 0.1889967613387853, 0.19047556282021105, 0.18894666177220643, 0.20126513321883976, 0.18950185785070062, 0.18998097302392125, 0.189632261171937, 0.19362672325223684, 0.18917532125487924, 0.19008876709267497, 0.19090703199617565, 0.18993337731808424, 0.20678376290015876, 0.18503208714537323, 0.1874958979897201, 0.19266372406855226, 0.187741887755692, 0.18889861786738038, 0.18693813402205706, 0.1872216009069234, 0.18871275009587407, 0.18818011623807251, 0.1912099609617144, 0.18763916194438934, 0.18845727597363293, 0.19224023586139083, 0.1926214226987213, 0.18895865511149168, 0.18694578390568495, 0.18858053209260106, 0.18813073821365833, 0.18767782417126, 0.2289594877511263, 0.18135696090757847, 0.18217786285094917]
Total Epoch List: [52, 56]
Total Time List: [0.0548607730306685, 0.050073337042704225]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c43876d450>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7392;  Loss pred: 0.7290; Loss self: 1.0234; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7355;  Loss pred: 0.7262; Loss self: 0.9317; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.4884 time: 0.05s
Epoch 3/1000, LR 0.000050
Train loss: 0.7416;  Loss pred: 0.7322; Loss self: 0.9422; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.4884 time: 0.05s
Epoch 4/1000, LR 0.000100
Train loss: 0.6781;  Loss pred: 0.6683; Loss self: 0.9828; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000150
Train loss: 0.6521;  Loss pred: 0.6418; Loss self: 1.0274; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5116 time: 0.05s
Epoch 6/1000, LR 0.000200
Train loss: 0.6119;  Loss pred: 0.6038; Loss self: 0.8147; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000250
Train loss: 0.5714;  Loss pred: 0.5622; Loss self: 0.9178; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6930 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000300
Train loss: 0.5437;  Loss pred: 0.5338; Loss self: 0.9947; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000350
Train loss: 0.5180;  Loss pred: 0.5074; Loss self: 1.0616; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000400
Train loss: 0.4686;  Loss pred: 0.4571; Loss self: 1.1517; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6922 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000450
Train loss: 0.4432;  Loss pred: 0.4310; Loss self: 1.2176; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000450
Train loss: 0.4048;  Loss pred: 0.3920; Loss self: 1.2776; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6907 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.5116 time: 0.05s
Epoch 13/1000, LR 0.000450
Train loss: 0.3731;  Loss pred: 0.3597; Loss self: 1.3423; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6895 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6912 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.3485;  Loss pred: 0.3343; Loss self: 1.4218; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6876 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6902 score: 0.5116 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.3185;  Loss pred: 0.3036; Loss self: 1.4882; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6851 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6886 score: 0.5116 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2892;  Loss pred: 0.2736; Loss self: 1.5592; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6813 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6860 score: 0.5116 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.2617;  Loss pred: 0.2450; Loss self: 1.6616; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6759 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6820 score: 0.5116 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.2518;  Loss pred: 0.2342; Loss self: 1.7578; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6687 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6767 score: 0.5116 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.2276;  Loss pred: 0.2102; Loss self: 1.7438; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6584 score: 0.5000 time: 0.05s
Test loss: 0.6691 score: 0.5349 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.2126;  Loss pred: 0.1939; Loss self: 1.8665; time: 0.09s
Val loss: 0.6440 score: 0.5455 time: 0.05s
Test loss: 0.6576 score: 0.6279 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1931;  Loss pred: 0.1743; Loss self: 1.8834; time: 0.09s
Val loss: 0.6254 score: 0.6818 time: 0.05s
Test loss: 0.6419 score: 0.7209 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1792;  Loss pred: 0.1596; Loss self: 1.9582; time: 0.10s
Val loss: 0.6016 score: 0.7273 time: 0.05s
Test loss: 0.6217 score: 0.7209 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1707;  Loss pred: 0.1508; Loss self: 1.9959; time: 0.09s
Val loss: 0.5722 score: 0.7955 time: 0.05s
Test loss: 0.5969 score: 0.7674 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1565;  Loss pred: 0.1361; Loss self: 2.0431; time: 0.09s
Val loss: 0.5385 score: 0.8409 time: 0.05s
Test loss: 0.5691 score: 0.7907 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.1456;  Loss pred: 0.1245; Loss self: 2.1079; time: 0.09s
Val loss: 0.5033 score: 0.8409 time: 0.05s
Test loss: 0.5416 score: 0.8140 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.1306;  Loss pred: 0.1096; Loss self: 2.0989; time: 0.09s
Val loss: 0.4680 score: 0.8636 time: 0.05s
Test loss: 0.5159 score: 0.8140 time: 0.05s
Epoch 27/1000, LR 0.000450
Train loss: 0.1234;  Loss pred: 0.1018; Loss self: 2.1638; time: 0.09s
Val loss: 0.4370 score: 0.8409 time: 0.05s
Test loss: 0.4963 score: 0.8140 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.1133;  Loss pred: 0.0915; Loss self: 2.1835; time: 0.09s
Val loss: 0.4127 score: 0.8409 time: 0.05s
Test loss: 0.4855 score: 0.8140 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.1065;  Loss pred: 0.0845; Loss self: 2.2027; time: 0.09s
Val loss: 0.3941 score: 0.8409 time: 0.05s
Test loss: 0.4821 score: 0.8140 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0980;  Loss pred: 0.0755; Loss self: 2.2449; time: 0.09s
Val loss: 0.3824 score: 0.8409 time: 0.05s
Test loss: 0.4868 score: 0.8140 time: 0.05s
Epoch 31/1000, LR 0.000450
Train loss: 0.0916;  Loss pred: 0.0689; Loss self: 2.2658; time: 0.09s
Val loss: 0.3766 score: 0.8409 time: 0.05s
Test loss: 0.4979 score: 0.8140 time: 0.05s
Epoch 32/1000, LR 0.000450
Train loss: 0.0850;  Loss pred: 0.0621; Loss self: 2.2893; time: 0.09s
Val loss: 0.3753 score: 0.8409 time: 0.05s
Test loss: 0.5125 score: 0.8140 time: 0.05s
Epoch 33/1000, LR 0.000449
Train loss: 0.0799;  Loss pred: 0.0566; Loss self: 2.3280; time: 0.11s
Val loss: 0.3785 score: 0.8409 time: 0.05s
Test loss: 0.5315 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0762;  Loss pred: 0.0522; Loss self: 2.3990; time: 0.09s
Val loss: 0.3832 score: 0.8409 time: 0.05s
Test loss: 0.5496 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0716;  Loss pred: 0.0477; Loss self: 2.3877; time: 0.09s
Val loss: 0.3900 score: 0.8409 time: 0.05s
Test loss: 0.5690 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0667;  Loss pred: 0.0429; Loss self: 2.3723; time: 0.09s
Val loss: 0.3964 score: 0.8409 time: 0.05s
Test loss: 0.5864 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0628;  Loss pred: 0.0389; Loss self: 2.3903; time: 0.09s
Val loss: 0.4041 score: 0.8409 time: 0.05s
Test loss: 0.6046 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0582;  Loss pred: 0.0340; Loss self: 2.4112; time: 0.11s
Val loss: 0.4103 score: 0.8409 time: 0.05s
Test loss: 0.6195 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0568;  Loss pred: 0.0324; Loss self: 2.4414; time: 0.09s
Val loss: 0.4158 score: 0.8409 time: 0.05s
Test loss: 0.6329 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0523;  Loss pred: 0.0279; Loss self: 2.4450; time: 0.10s
Val loss: 0.4192 score: 0.8409 time: 0.05s
Test loss: 0.6435 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0506;  Loss pred: 0.0258; Loss self: 2.4847; time: 0.10s
Val loss: 0.4228 score: 0.8409 time: 0.06s
Test loss: 0.6537 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0480;  Loss pred: 0.0231; Loss self: 2.4875; time: 0.09s
Val loss: 0.4277 score: 0.8409 time: 0.05s
Test loss: 0.6652 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0453;  Loss pred: 0.0201; Loss self: 2.5168; time: 0.09s
Val loss: 0.4309 score: 0.8409 time: 0.05s
Test loss: 0.6752 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0431;  Loss pred: 0.0183; Loss self: 2.4788; time: 0.09s
Val loss: 0.4341 score: 0.8409 time: 0.05s
Test loss: 0.6847 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0415;  Loss pred: 0.0168; Loss self: 2.4741; time: 0.10s
Val loss: 0.4360 score: 0.8409 time: 0.05s
Test loss: 0.6907 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0403;  Loss pred: 0.0151; Loss self: 2.5192; time: 0.10s
Val loss: 0.4365 score: 0.8409 time: 0.05s
Test loss: 0.6948 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0396;  Loss pred: 0.0143; Loss self: 2.5292; time: 0.10s
Val loss: 0.4341 score: 0.8409 time: 0.05s
Test loss: 0.6950 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0386;  Loss pred: 0.0133; Loss self: 2.5394; time: 0.09s
Val loss: 0.4321 score: 0.8409 time: 0.05s
Test loss: 0.6941 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0380;  Loss pred: 0.0128; Loss self: 2.5193; time: 0.09s
Val loss: 0.4297 score: 0.8409 time: 0.05s
Test loss: 0.6930 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0363;  Loss pred: 0.0113; Loss self: 2.5031; time: 0.09s
Val loss: 0.4260 score: 0.8409 time: 0.05s
Test loss: 0.6904 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0355;  Loss pred: 0.0106; Loss self: 2.4872; time: 0.09s
Val loss: 0.4258 score: 0.8409 time: 0.05s
Test loss: 0.6934 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0349;  Loss pred: 0.0100; Loss self: 2.4871; time: 0.10s
Val loss: 0.4219 score: 0.8409 time: 0.05s
Test loss: 0.6910 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.0850,   Val_Loss: 0.3753,   Val_Precision: 0.8947,   Val_Recall: 0.7727,   Val_accuracy: 0.8293,   Val_Score: 0.8409,   Val_Loss: 0.3753,   Test_Precision: 1.0000,   Test_Recall: 0.6190,   Test_accuracy: 0.7647,   Test_Score: 0.8140,   Test_loss: 0.5125


[0.07393697486259043, 0.060057081980630755, 0.05296889808960259, 0.05596289713867009, 0.053757603047415614, 0.054823487997055054, 0.054723114939406514, 0.05546205607242882, 0.0546168009750545, 0.05740484595298767, 0.05501262494362891, 0.05595058994367719, 0.05553290294483304, 0.05520360101945698, 0.055743518052622676, 0.05529092508368194, 0.05578906089067459, 0.054942982038483024, 0.058414262952283025, 0.058252014918252826, 0.05488253012299538, 0.05416415701620281, 0.054844867903739214, 0.05427024210803211, 0.05410075606778264, 0.05528801400214434, 0.05498779215849936, 0.054460581159219146, 0.05403559608384967, 0.05430413084104657, 0.05402600206434727, 0.05408818880096078, 0.05396673013456166, 0.05390595202334225, 0.057899039005860686, 0.05384577508084476, 0.053428603103384376, 0.0539412060752511, 0.05390874180011451, 0.05402652104385197, 0.053707255981862545, 0.05348135204985738, 0.054020036943256855, 0.0563101340085268, 0.05455622193403542, 0.05901927105151117, 0.05412322701886296, 0.05450830003246665, 0.05394319910556078, 0.057702602818608284, 0.05657501891255379, 0.05451028188690543, 0.04928577598184347, 0.051019777078181505, 0.050430546049028635, 0.05009354301728308, 0.05060830409638584, 0.05065367300994694, 0.05082302610389888, 0.05167575506493449, 0.05089482408948243, 0.05065886699594557, 0.05005830596201122, 0.05002222605980933, 0.05590909416787326, 0.05033516604453325, 0.05017567798495293, 0.05035396991297603, 0.05052982806228101, 0.050594909116625786, 0.05386624904349446, 0.050357962027192116, 0.05025965301319957, 0.050756730139255524, 0.050527850864455104, 0.05032126582227647, 0.0508293102029711, 0.0506778780836612, 0.050850218860432506, 0.050821693148463964, 0.05048375600017607, 0.05031802994199097, 0.05048733693547547, 0.05262675788253546, 0.05096663488075137, 0.04960577585734427, 0.05029090400785208, 0.05044405208900571, 0.05024193902499974, 0.050466018030419946, 0.050444212974980474, 0.05003659799695015, 0.05001972196623683, 0.050545898033306, 0.05051519814878702, 0.05345365894027054, 0.05028198496438563, 0.05048503400757909, 0.05029981699772179, 0.05405315011739731, 0.05036340095102787, 0.05045707896351814, 0.05046781408600509, 0.05066436994820833, 0.05140785709954798, 0.08844150300137699, 0.04899672092869878, 0.0495283561758697, 0.0504250661469996, 0.05097176996059716, 0.05125639587640762, 0.051476279040798545, 0.05384868895635009, 0.051456779008731246, 0.051339102210476995, 0.05148348701186478, 0.05173072009347379, 0.05167268915101886, 0.05442809988744557, 0.05200324207544327, 0.05173961911350489, 0.05193712003529072, 0.05168703501112759, 0.05171469505876303, 0.05494276201352477, 0.05205080006271601, 0.05179068096913397, 0.0517403450794518, 0.05171749391593039, 0.051720859948545694, 0.051497353008016944, 0.0514034319203347, 0.05160008114762604, 0.05134193995036185, 0.052087634801864624, 0.05205583688803017, 0.05287201702594757, 0.05264833802357316, 0.052840729942545295, 0.05849095596931875, 0.05321781220845878, 0.05262936791405082, 0.052719641011208296, 0.05341957090422511, 0.05408225394785404, 0.05380072700791061, 0.05352536006830633, 0.05382313905283809, 0.06040249881334603, 0.056455360958352685, 0.05553625617176294, 0.05557198519818485, 0.055178887909278274, 0.05484112398698926, 0.05566709814593196, 0.05585399689152837, 0.05567488702945411, 0.05468646693043411, 0.05494900280609727, 0.055147167993709445]
[0.0016803857923316007, 0.0013649336813779717, 0.0012038385929455135, 0.0012718840258788657, 0.0012217637056230822, 0.001245988363569433, 0.0012437071577137845, 0.0012605012743733823, 0.0012412909312512386, 0.0013046555898406289, 0.0012502869305370207, 0.0012716043169017542, 0.0012621114305643873, 0.0012546272958967495, 0.0012668981375596063, 0.0012566119337200441, 0.0012679332020607862, 0.0012487041372382505, 0.0013275968852791596, 0.0013239094299602914, 0.0012473302300680768, 0.0012310035685500638, 0.0012464742705395276, 0.001233414593364366, 0.001229562637904151, 0.0012565457727760077, 0.0012497225490568037, 0.0012377404808913443, 0.0012280817291784015, 0.0012341847918419676, 0.0012278636832806196, 0.001229277018203654, 0.0012265165939673104, 0.0012251352732577784, 0.0013158872501331973, 0.0012237676154737446, 0.0012142864341678267, 0.0012259365017102523, 0.0012251986772753298, 0.001227875478269363, 0.0012206194541332397, 0.001215485273860395, 0.0012277281123467467, 0.0012797757729210637, 0.0012399141348644414, 0.0013413470693525267, 0.0012300733413377945, 0.0012388250007378783, 0.0012259817978536541, 0.0013114227913320065, 0.0012857958843762224, 0.0012388700428842144, 0.0011461808367870575, 0.0011865064436786395, 0.001172803396489038, 0.0011649661166810018, 0.0011769373045671125, 0.0011779923955801614, 0.0011819308396255554, 0.001201761745696151, 0.0011836005602205216, 0.0011781131859522225, 0.0011641466502793306, 0.0011633075827862634, 0.0013002114922761223, 0.0011705852568496106, 0.0011668762322082076, 0.0011710225561157215, 0.001175112280518163, 0.001176625793409902, 0.0012527034661277783, 0.001171115395981212, 0.0011688291398418505, 0.0011803890730059424, 0.0011750662991733744, 0.0011702619958668947, 0.0011820769814644442, 0.0011785553042711907, 0.001182563229312384, 0.0011818998406619527, 0.001174040837213397, 0.0011701867428369993, 0.0011741241147784993, 0.0012238780902915222, 0.0011852705786221248, 0.0011536226943568435, 0.0011695559071593505, 0.0011731174904419932, 0.0011684171866279008, 0.001173628326288836, 0.00117312123197629, 0.0011636418138825616, 0.0011632493480520194, 0.0011754860007745583, 0.0011747720499717912, 0.0012431083474481522, 0.0011693484875438518, 0.0011740705583157928, 0.0011697631859935301, 0.00125705000273017, 0.0011712418825820434, 0.00117342044101205, 0.0011736700950233743, 0.0011782411615862403, 0.0011955315604546043, 0.0020567791395669065, 0.0011394586262488088, 0.0011518222366481326, 0.0011726759569069675, 0.0011853899990836548, 0.0011920092064280842, 0.001197122768390664, 0.0012522950920081416, 0.0011966692792728197, 0.0011939326095459766, 0.0011972903956247623, 0.0012030400021738091, 0.0012016904453725317, 0.0012657697648243156, 0.0012093777226847271, 0.0012032469561280206, 0.0012078400008207143, 0.001202024070026223, 0.0012026673269479773, 0.0012777386514773203, 0.0012104837223887444, 0.0012044344411426505, 0.0012032638390570185, 0.001202732416649544, 0.0012028106964778068, 0.0011976128606515568, 0.0011954286493101093, 0.001200001887154094, 0.0011939986034967872, 0.00121134034422941, 0.0012106008578611667, 0.0012295817913011062, 0.001224379954036585, 0.0012288541847103557, 0.0013602547899841568, 0.0012376235397315997, 0.0012239387886988562, 0.0012260381630513557, 0.00124231560242384, 0.0012577268359966057, 0.0012511796978583864, 0.0012447758155420077, 0.001251700908205537, 0.0014047092747289775, 0.001312915371124481, 0.0012915408412037894, 0.0012923717487949966, 0.0012832299513785646, 0.0012753749764416106, 0.001294583677812371, 0.0012989301602681015, 0.0012947648146384677, 0.00127177830070777, 0.0012778837861883085, 0.0012824922789234756]
[595.1014371601303, 732.6363277887962, 830.676143679056, 786.2352067115592, 818.4888742377677, 802.575713576698, 804.0477967805755, 793.3351757197689, 805.6129105784943, 766.485812644359, 799.8164065991493, 786.4081512686947, 792.3230673482, 797.0494530690456, 789.3294420072908, 795.7906280896312, 788.685080866002, 800.8302128410437, 753.2406945875936, 755.3386790439232, 801.7123099352943, 812.3453298984738, 802.2628494105675, 810.757392834404, 813.2973214805456, 795.8325288785643, 800.177608025657, 807.9238058691122, 814.2780535209246, 810.251436097785, 814.4226542544112, 813.48628925098, 815.317138731392, 816.2363959539608, 759.9435285194666, 817.1486051401027, 823.5289235404484, 815.7029329047158, 816.1941557297955, 814.4148308991856, 819.2561544171838, 822.7166725138419, 814.5125862505056, 781.3868813264992, 806.5074603809795, 745.5192044238811, 812.9596556514483, 807.2165151691097, 815.6727952655708, 762.5305939546043, 777.7284187568616, 807.187166841083, 872.4626759623485, 842.810424947718, 852.657830795553, 858.3940645836192, 849.6629311684605, 848.9019146065878, 846.0731935185036, 832.1116923394201, 844.8796271384713, 848.814877826649, 858.9983055485797, 859.617881630994, 769.1056462279236, 854.2735303973453, 856.988918274212, 853.9545158865233, 850.9825116958631, 849.8878790528344, 798.2735156717432, 853.8868188665183, 855.5570407282163, 847.1782930465722, 851.0158113661088, 854.5095060181205, 845.9685923002461, 848.4964569553163, 845.6207458619041, 846.0953843939304, 851.7591282203731, 854.5644582979988, 851.6987151640709, 817.0748442451523, 843.6892115912456, 866.8345420835452, 855.0253937230135, 852.4295376614255, 855.8586876713448, 852.0585074511016, 852.4268189361446, 859.3709748736505, 859.6609159288186, 850.7119602794709, 851.228968227506, 804.4351098219202, 855.1770585520161, 851.7375662962731, 854.8738855639889, 795.5133032322608, 853.7946045743047, 852.2094596694784, 852.0281842744613, 848.722683099716, 836.448014488005, 486.1970742325638, 877.6097498968278, 868.1895245485589, 852.7504926745365, 843.60421529879, 838.9196950890594, 835.3362131307024, 798.5338331051278, 835.6527716727802, 837.5682111407239, 835.2192614709704, 831.2275553539948, 832.1610643163546, 790.033091159194, 826.8715234642123, 831.0845873385314, 827.9242278120535, 831.9300960239376, 831.4851310858435, 782.6326603204817, 826.1160241185399, 830.265198204808, 831.0729264362222, 831.4401326154521, 831.3860218638745, 834.9943732701348, 836.5200219829995, 833.3320228117179, 837.5219175896556, 825.5318208163426, 826.0360906787671, 813.2846526149596, 816.7399316716676, 813.7661997999403, 735.1563893494153, 808.0001453566952, 817.0343233120989, 815.635295977416, 804.9484350425399, 795.0852056103372, 799.2457052425608, 803.3575102554303, 798.9128979970301, 711.8910780972373, 761.6637157226087, 774.2689724530455, 773.771169891633, 779.2835562524918, 784.0831272933337, 772.4491024711758, 769.8643318849399, 772.3410373019955, 786.3005678296918, 782.5437733918006, 779.7317897612611]
Elapsed: 0.053322618039965165~0.0040249936651389
Time per graph: 0.0012305316874642285~8.90248827617536e-05
Speed: 815.7965935954237~44.50933702929189
Total Time: 0.0558
best val loss: 0.37531349062919617 test_score: 0.8140

Testing...
Test loss: 0.5159 score: 0.8140 time: 0.05s
test Score 0.8140
Epoch Time List: [0.28064270014874637, 0.21783867431804538, 0.18090482498519123, 0.18511765589937568, 0.18535543885082006, 0.187170818913728, 0.19141687895171344, 0.1955487560480833, 0.19760408089496195, 0.21455286093987525, 0.19219358381815255, 0.19284596294164658, 0.1940754281822592, 0.192196199670434, 0.19290860509499907, 0.1920457947999239, 0.1935288798995316, 0.19085870846174657, 0.1988603111822158, 0.21975971665233374, 0.1953399188350886, 0.19002460199408233, 0.18900242913514376, 0.1882572090253234, 0.1867821749765426, 0.18794098193757236, 0.19118930399417877, 0.18825569795444608, 0.18693836010061204, 0.19707926525734365, 0.18877440900541842, 0.18813318200409412, 0.18609528173692524, 0.1869562140200287, 0.19696339895017445, 0.18689232598990202, 0.18726184288971126, 0.18663350422866642, 0.18618176458403468, 0.1871318907942623, 0.18823203584179282, 0.1874851672910154, 0.18622285104356706, 0.21050697285681963, 0.18928383593447506, 0.19203894515521824, 0.18518254323862493, 0.1866869197692722, 0.1854751817882061, 0.2085506059229374, 0.1967226890847087, 0.18997077690437436, 0.18680403428152204, 0.18738347780890763, 0.18826967640779912, 0.1865815008059144, 0.18821727996692061, 0.18807748099789023, 0.1893025068566203, 0.19160742382518947, 0.1903936699964106, 0.1891042513307184, 0.18755201203748584, 0.19141745241358876, 0.19352041301317513, 0.18736762017942965, 0.1875786290038377, 0.18731119111180305, 0.18767825793474913, 0.18918673298321664, 0.19309096597135067, 0.18882247526198626, 0.18814892694354057, 0.1889967613387853, 0.19047556282021105, 0.18894666177220643, 0.20126513321883976, 0.18950185785070062, 0.18998097302392125, 0.189632261171937, 0.19362672325223684, 0.18917532125487924, 0.19008876709267497, 0.19090703199617565, 0.18993337731808424, 0.20678376290015876, 0.18503208714537323, 0.1874958979897201, 0.19266372406855226, 0.187741887755692, 0.18889861786738038, 0.18693813402205706, 0.1872216009069234, 0.18871275009587407, 0.18818011623807251, 0.1912099609617144, 0.18763916194438934, 0.18845727597363293, 0.19224023586139083, 0.1926214226987213, 0.18895865511149168, 0.18694578390568495, 0.18858053209260106, 0.18813073821365833, 0.18767782417126, 0.2289594877511263, 0.18135696090757847, 0.18217786285094917, 0.18107628403231502, 0.1801416021771729, 0.1819506953470409, 0.18246619030833244, 0.18673384189605713, 0.1971398601308465, 0.18403282691724598, 0.1875953080598265, 0.18477411102503538, 0.1874892229679972, 0.18995807599276304, 0.1876800290774554, 0.1876406327355653, 0.185667566023767, 0.18697136733680964, 0.18562631402164698, 0.1891383291222155, 0.18626343202777207, 0.18485963228158653, 0.18531343596987426, 0.18512827064841986, 0.19617342902347445, 0.1856487230397761, 0.18299467395991087, 0.18580471514724195, 0.1856638598255813, 0.18480998207814991, 0.18825630121864378, 0.18718371610157192, 0.18932175054214895, 0.18852971005253494, 0.19648954016156495, 0.2071234241593629, 0.1896300120279193, 0.19073144812136889, 0.19135074876248837, 0.1922136868815869, 0.21108646388165653, 0.19224227499216795, 0.19896372128278017, 0.22185658593662083, 0.1970204298850149, 0.1958689650055021, 0.19637060817331076, 0.1966300669591874, 0.19676199438981712, 0.20784904179163277, 0.19691642792895436, 0.19585509598255157, 0.19446318689733744, 0.1953516157809645, 0.19662966043688357]
Total Epoch List: [52, 56, 52]
Total Time List: [0.0548607730306685, 0.050073337042704225, 0.055797613924369216]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c43876f730>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7211;  Loss pred: 0.7114; Loss self: 0.9663; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6937 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7132;  Loss pred: 0.7048; Loss self: 0.8365; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6928 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000050
Train loss: 0.7055;  Loss pred: 0.6959; Loss self: 0.9612; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6927 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000100
Train loss: 0.6781;  Loss pred: 0.6692; Loss self: 0.8944; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.05s
Epoch 5/1000, LR 0.000150
Train loss: 0.6721;  Loss pred: 0.6634; Loss self: 0.8678; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.6406;  Loss pred: 0.6322; Loss self: 0.8440; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6926 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5912;  Loss pred: 0.5817; Loss self: 0.9492; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6925 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5000 time: 0.06s
Epoch 8/1000, LR 0.000300
Train loss: 0.5524;  Loss pred: 0.5434; Loss self: 0.8972; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6923 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6926 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000350
Train loss: 0.5325;  Loss pred: 0.5228; Loss self: 0.9685; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6919 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6922 score: 0.5000 time: 0.05s
Epoch 10/1000, LR 0.000400
Train loss: 0.4766;  Loss pred: 0.4654; Loss self: 1.1260; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6915 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6918 score: 0.5000 time: 0.05s
Epoch 11/1000, LR 0.000450
Train loss: 0.4412;  Loss pred: 0.4295; Loss self: 1.1647; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6908 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6911 score: 0.5000 time: 0.05s
Epoch 12/1000, LR 0.000450
Train loss: 0.4066;  Loss pred: 0.3933; Loss self: 1.3324; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6898 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6902 score: 0.5000 time: 0.05s
Epoch 13/1000, LR 0.000450
Train loss: 0.3684;  Loss pred: 0.3546; Loss self: 1.3842; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6883 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6888 score: 0.5000 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.3385;  Loss pred: 0.3228; Loss self: 1.5622; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6861 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6869 score: 0.5000 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.3049;  Loss pred: 0.2894; Loss self: 1.5476; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6829 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6840 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2799;  Loss pred: 0.2638; Loss self: 1.6152; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6781 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6798 score: 0.5000 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.2584;  Loss pred: 0.2412; Loss self: 1.7181; time: 0.11s
Val loss: 0.6713 score: 0.5349 time: 0.05s
Test loss: 0.6737 score: 0.5227 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.2251;  Loss pred: 0.2071; Loss self: 1.8049; time: 0.11s
Val loss: 0.6616 score: 0.6279 time: 0.05s
Test loss: 0.6655 score: 0.6136 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.2079;  Loss pred: 0.1889; Loss self: 1.9007; time: 0.11s
Val loss: 0.6477 score: 0.7442 time: 0.05s
Test loss: 0.6538 score: 0.6591 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.1897;  Loss pred: 0.1706; Loss self: 1.9113; time: 0.11s
Val loss: 0.6283 score: 0.8372 time: 0.05s
Test loss: 0.6373 score: 0.7727 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1745;  Loss pred: 0.1540; Loss self: 2.0469; time: 0.11s
Val loss: 0.6011 score: 0.8837 time: 0.05s
Test loss: 0.6151 score: 0.8409 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1568;  Loss pred: 0.1357; Loss self: 2.1071; time: 0.11s
Val loss: 0.5660 score: 0.9070 time: 0.05s
Test loss: 0.5861 score: 0.8409 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1468;  Loss pred: 0.1248; Loss self: 2.1935; time: 0.11s
Val loss: 0.5246 score: 0.9070 time: 0.05s
Test loss: 0.5514 score: 0.8182 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1356;  Loss pred: 0.1139; Loss self: 2.1717; time: 0.11s
Val loss: 0.4786 score: 0.9070 time: 0.05s
Test loss: 0.5129 score: 0.8409 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.1193;  Loss pred: 0.0969; Loss self: 2.2351; time: 0.11s
Val loss: 0.4306 score: 0.8837 time: 0.05s
Test loss: 0.4721 score: 0.8864 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.1096;  Loss pred: 0.0870; Loss self: 2.2613; time: 0.11s
Val loss: 0.3845 score: 0.8837 time: 0.05s
Test loss: 0.4328 score: 0.8864 time: 0.06s
Epoch 27/1000, LR 0.000450
Train loss: 0.1040;  Loss pred: 0.0809; Loss self: 2.3076; time: 0.11s
Val loss: 0.3430 score: 0.8837 time: 0.05s
Test loss: 0.3967 score: 0.8864 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.0959;  Loss pred: 0.0720; Loss self: 2.3870; time: 0.11s
Val loss: 0.3082 score: 0.8837 time: 0.05s
Test loss: 0.3656 score: 0.8864 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.0865;  Loss pred: 0.0628; Loss self: 2.3759; time: 0.11s
Val loss: 0.2811 score: 0.8837 time: 0.05s
Test loss: 0.3400 score: 0.8864 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0800;  Loss pred: 0.0559; Loss self: 2.4046; time: 0.11s
Val loss: 0.2614 score: 0.8837 time: 0.05s
Test loss: 0.3208 score: 0.8864 time: 0.05s
Epoch 31/1000, LR 0.000450
Train loss: 0.0753;  Loss pred: 0.0511; Loss self: 2.4235; time: 0.11s
Val loss: 0.2475 score: 0.8837 time: 0.05s
Test loss: 0.3070 score: 0.8864 time: 0.05s
Epoch 32/1000, LR 0.000450
Train loss: 0.0687;  Loss pred: 0.0442; Loss self: 2.4445; time: 0.11s
Val loss: 0.2388 score: 0.8837 time: 0.05s
Test loss: 0.2978 score: 0.8864 time: 0.05s
Epoch 33/1000, LR 0.000449
Train loss: 0.0645;  Loss pred: 0.0398; Loss self: 2.4607; time: 0.11s
Val loss: 0.2340 score: 0.8837 time: 0.05s
Test loss: 0.2915 score: 0.8864 time: 0.05s
Epoch 34/1000, LR 0.000449
Train loss: 0.0602;  Loss pred: 0.0349; Loss self: 2.5362; time: 0.11s
Val loss: 0.2313 score: 0.8837 time: 0.05s
Test loss: 0.2873 score: 0.8864 time: 0.05s
Epoch 35/1000, LR 0.000449
Train loss: 0.0568;  Loss pred: 0.0313; Loss self: 2.5423; time: 0.11s
Val loss: 0.2307 score: 0.8837 time: 0.05s
Test loss: 0.2844 score: 0.8864 time: 0.05s
Epoch 36/1000, LR 0.000449
Train loss: 0.0527;  Loss pred: 0.0274; Loss self: 2.5374; time: 0.11s
Val loss: 0.2308 score: 0.8837 time: 0.05s
Test loss: 0.2818 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0496;  Loss pred: 0.0243; Loss self: 2.5281; time: 0.11s
Val loss: 0.2317 score: 0.8837 time: 0.06s
Test loss: 0.2793 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0474;  Loss pred: 0.0218; Loss self: 2.5633; time: 0.11s
Val loss: 0.2331 score: 0.8837 time: 0.05s
Test loss: 0.2765 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0451;  Loss pred: 0.0195; Loss self: 2.5650; time: 0.11s
Val loss: 0.2346 score: 0.8837 time: 0.05s
Test loss: 0.2738 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0429;  Loss pred: 0.0170; Loss self: 2.5888; time: 0.10s
Val loss: 0.2365 score: 0.8837 time: 0.13s
Test loss: 0.2707 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0410;  Loss pred: 0.0153; Loss self: 2.5719; time: 0.10s
Val loss: 0.2391 score: 0.8837 time: 0.05s
Test loss: 0.2679 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0392;  Loss pred: 0.0133; Loss self: 2.5928; time: 0.10s
Val loss: 0.2415 score: 0.8837 time: 0.05s
Test loss: 0.2650 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0380;  Loss pred: 0.0122; Loss self: 2.5822; time: 0.10s
Val loss: 0.2444 score: 0.8837 time: 0.05s
Test loss: 0.2620 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0375;  Loss pred: 0.0113; Loss self: 2.6244; time: 0.11s
Val loss: 0.2472 score: 0.8837 time: 0.05s
Test loss: 0.2584 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0363;  Loss pred: 0.0102; Loss self: 2.6143; time: 0.09s
Val loss: 0.2498 score: 0.8837 time: 0.05s
Test loss: 0.2546 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0352;  Loss pred: 0.0092; Loss self: 2.5968; time: 0.10s
Val loss: 0.2525 score: 0.8837 time: 0.05s
Test loss: 0.2507 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0344;  Loss pred: 0.0082; Loss self: 2.6200; time: 0.10s
Val loss: 0.2545 score: 0.8837 time: 0.05s
Test loss: 0.2467 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0345;  Loss pred: 0.0080; Loss self: 2.6414; time: 0.12s
Val loss: 0.2562 score: 0.8837 time: 0.05s
Test loss: 0.2430 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0331;  Loss pred: 0.0069; Loss self: 2.6210; time: 0.10s
Val loss: 0.2577 score: 0.8837 time: 0.05s
Test loss: 0.2389 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0329;  Loss pred: 0.0067; Loss self: 2.6231; time: 0.11s
Val loss: 0.2602 score: 0.8837 time: 0.05s
Test loss: 0.2357 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0322;  Loss pred: 0.0060; Loss self: 2.6268; time: 0.11s
Val loss: 0.2628 score: 0.8837 time: 0.05s
Test loss: 0.2329 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 52/1000, LR 0.000448
Train loss: 0.0317;  Loss pred: 0.0055; Loss self: 2.6186; time: 0.11s
Val loss: 0.2647 score: 0.8837 time: 0.05s
Test loss: 0.2300 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 53/1000, LR 0.000448
Train loss: 0.0316;  Loss pred: 0.0053; Loss self: 2.6276; time: 0.10s
Val loss: 0.2664 score: 0.8837 time: 0.05s
Test loss: 0.2273 score: 0.8864 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 54/1000, LR 0.000448
Train loss: 0.0309;  Loss pred: 0.0047; Loss self: 2.6201; time: 0.10s
Val loss: 0.2693 score: 0.8837 time: 0.05s
Test loss: 0.2250 score: 0.9091 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 55/1000, LR 0.000448
Train loss: 0.0313;  Loss pred: 0.0052; Loss self: 2.6066; time: 0.11s
Val loss: 0.2707 score: 0.8837 time: 0.05s
Test loss: 0.2223 score: 0.9318 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 034,   Train_Loss: 0.0568,   Val_Loss: 0.2307,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8837,   Val_Loss: 0.2307,   Test_Precision: 0.8696,   Test_Recall: 0.9091,   Test_accuracy: 0.8889,   Test_Score: 0.8864,   Test_loss: 0.2844


[0.05006738705560565, 0.04988181800581515, 0.05036134994588792, 0.05814485717564821, 0.0509454149287194, 0.0512829280924052, 0.06389820296317339, 0.05150386015884578, 0.05143662285991013, 0.05199134792201221, 0.051666269078850746, 0.056366337928920984, 0.05141389206983149, 0.05142602603882551, 0.05131084518507123, 0.0527197509072721, 0.05721406685188413, 0.05160485813394189, 0.05230263085104525, 0.052116221049800515, 0.05224407999776304, 0.05437243287451565, 0.05181463994085789, 0.05163630400784314, 0.05149108706973493, 0.06444668117910624, 0.05430006003007293, 0.053746509831398726, 0.053408721927553415, 0.05125955608673394, 0.05111938901245594, 0.051354585913941264, 0.05141197913326323, 0.051430467050522566, 0.05135799199342728, 0.05112447100691497, 0.052274893037974834, 0.050993934040889144, 0.05119234696030617, 0.052090574987232685, 0.05063384212553501, 0.05095842108130455, 0.051155711989849806, 0.05138216097839177, 0.05220359191298485, 0.05371546884998679, 0.053863049019128084, 0.05063412990421057, 0.05157917691394687, 0.05158593109808862, 0.06657919799908996, 0.052014298969879746, 0.054661905160173774, 0.05257646995596588, 0.1551974629983306]
[0.0011378951603546739, 0.0011336776819503443, 0.0011445761351338165, 0.0013214740267192776, 0.0011578503392890773, 0.0011655210930092092, 0.0014522318855266678, 0.0011705422763374042, 0.0011690141559070485, 0.0011816215436820958, 0.0011742333881556988, 0.0012810531347482042, 0.0011684975470416248, 0.0011687733190642161, 0.0011661555723879826, 0.0011981761569834568, 0.0013003197011791847, 0.0011728376848623157, 0.0011886961557055738, 0.001184459569313648, 0.0011873654544946146, 0.0012357371107844467, 0.0011776054532013156, 0.0011735523638146167, 0.0011702519788576121, 0.0014646972995251417, 0.0012340922734107483, 0.0012215115870772438, 0.0012138345892625775, 0.001164989911062135, 0.001161804295737635, 0.0011671496798623014, 0.001168454071210528, 0.00116887425114824, 0.0011672270907597108, 0.0011619197956117039, 0.0011880657508630645, 0.0011589530463838441, 0.0011634624309160492, 0.0011838767042552884, 0.0011507691392167048, 0.0011581459336660125, 0.001162629817951132, 0.0011677763858725402, 0.0011864452707496557, 0.0012208061102269726, 0.0012241602049801838, 0.0011507756796411493, 0.0011722540207715197, 0.0011724075249565597, 0.0015131635908884082, 0.0011821431584063578, 0.0012423160263675857, 0.0011949197717264972, 0.003527215068143877]
[878.8155841073329, 882.0849311240129, 873.6858731403538, 756.7307262804276, 863.6694796099488, 857.9853303367876, 688.5952649616552, 854.3048980075914, 855.4216345003035, 846.2946578342352, 851.6194566487696, 780.6077459828033, 855.7998281911476, 855.5979022524699, 857.5185195507498, 834.6018189158533, 769.0416434459602, 852.6329030068591, 841.2578733431088, 844.2668926044172, 842.2006857405465, 809.2336074338655, 849.1808502427567, 852.1136600581784, 854.5168203656359, 682.7349243589118, 810.3121796850969, 818.6578093726783, 823.835478775996, 858.376532281115, 860.7301622732384, 856.7881371633315, 855.8316707853068, 855.524021525543, 856.7313146828454, 860.6446019568333, 841.7042569180662, 862.8477254710118, 859.5034729335028, 844.6825555445362, 868.9840263536012, 863.4490446593246, 860.1190031082037, 856.328328006752, 842.8538801188442, 819.1308936142856, 816.8865446955022, 868.9790874897824, 853.0574280665289, 852.9457366260527, 660.8670774406357, 845.9212345720426, 804.9481603516822, 836.8762687348753, 283.5097890773723]
Elapsed: 0.054717567495324394~0.014073590731826296
Time per graph: 0.0012435810794391908~0.0003198543348142339
Speed: 825.3007260787142~86.50969694300204
Total Time: 0.1556
best val loss: 0.2306615263223648 test_score: 0.8864

Testing...
Test loss: 0.5861 score: 0.8409 time: 0.05s
test Score 0.8409
Epoch Time List: [0.20598831516690552, 0.1889060470275581, 0.19434524211101234, 0.20407130289822817, 0.1948706021066755, 0.19952097814530134, 0.21452533057890832, 0.1995694967918098, 0.1971678843256086, 0.19781315210275352, 0.19818984693847597, 0.2030402917880565, 0.19184703286737204, 0.18939315318129957, 0.18801192264072597, 0.18970888410694897, 0.2094854200258851, 0.2024341062642634, 0.2032255621161312, 0.20367946196347475, 0.20513171795755625, 0.20893152616918087, 0.20500754029490054, 0.20378793007694185, 0.2006254573352635, 0.2158158551901579, 0.20656220288947225, 0.20896726078353822, 0.2054621148854494, 0.20114277000539005, 0.19944584695622325, 0.20120167802087963, 0.20097359106875956, 0.20074720005504787, 0.20142537192441523, 0.2010243369732052, 0.21251934301108122, 0.20238774688914418, 0.206558144884184, 0.2719440208747983, 0.18938195891678333, 0.1952411187812686, 0.19714630022644997, 0.20706995809450746, 0.19427080010063946, 0.20124458987265825, 0.20334387430921197, 0.21064515528269112, 0.19467431167140603, 0.20025135297328234, 0.21675460506230593, 0.20157025614753366, 0.20368955773301423, 0.1968414280563593, 0.3120222573634237]
Total Epoch List: [55]
Total Time List: [0.155637081945315]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c442357cd0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.6769;  Loss pred: 0.6661; Loss self: 1.0873; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.6796;  Loss pred: 0.6685; Loss self: 1.1119; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6935 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5116 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6868;  Loss pred: 0.6759; Loss self: 1.0885; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6930 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000100
Train loss: 0.6437;  Loss pred: 0.6331; Loss self: 1.0626; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6944 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6932 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000150
Train loss: 0.6261;  Loss pred: 0.6160; Loss self: 1.0133; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6947 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000200
Train loss: 0.5930;  Loss pred: 0.5823; Loss self: 1.0701; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6949 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6934 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000250
Train loss: 0.5449;  Loss pred: 0.5334; Loss self: 1.1553; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6949 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6933 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000300
Train loss: 0.4955;  Loss pred: 0.4828; Loss self: 1.2756; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6948 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000350
Train loss: 0.4731;  Loss pred: 0.4608; Loss self: 1.2232; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6947 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000400
Train loss: 0.4321;  Loss pred: 0.4188; Loss self: 1.3238; time: 0.24s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6945 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000450
Train loss: 0.3829;  Loss pred: 0.3690; Loss self: 1.3976; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6939 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6919 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000450
Train loss: 0.3512;  Loss pred: 0.3356; Loss self: 1.5624; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6908 score: 0.5116 time: 0.05s
Epoch 13/1000, LR 0.000450
Train loss: 0.3252;  Loss pred: 0.3089; Loss self: 1.6283; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6896 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.2900;  Loss pred: 0.2733; Loss self: 1.6731; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6900 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6879 score: 0.5116 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.2618;  Loss pred: 0.2443; Loss self: 1.7447; time: 0.12s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6874 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6854 score: 0.5116 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2465;  Loss pred: 0.2294; Loss self: 1.7067; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6834 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6815 score: 0.5116 time: 0.09s
Epoch 17/1000, LR 0.000450
Train loss: 0.2152;  Loss pred: 0.1966; Loss self: 1.8641; time: 0.11s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6773 score: 0.5000 time: 0.05s
Test loss: 0.6753 score: 0.5349 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.1954;  Loss pred: 0.1765; Loss self: 1.8823; time: 0.10s
Val loss: 0.6690 score: 0.6136 time: 0.05s
Test loss: 0.6671 score: 0.5581 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.1760;  Loss pred: 0.1562; Loss self: 1.9834; time: 0.10s
Val loss: 0.6574 score: 0.7727 time: 0.05s
Test loss: 0.6554 score: 0.7907 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.1604;  Loss pred: 0.1403; Loss self: 2.0032; time: 0.10s
Val loss: 0.6419 score: 0.8409 time: 0.05s
Test loss: 0.6399 score: 0.8605 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1464;  Loss pred: 0.1265; Loss self: 1.9913; time: 0.10s
Val loss: 0.6211 score: 0.8636 time: 0.06s
Test loss: 0.6188 score: 0.8837 time: 0.06s
Epoch 22/1000, LR 0.000450
Train loss: 0.1297;  Loss pred: 0.1090; Loss self: 2.0743; time: 0.09s
Val loss: 0.5964 score: 0.8409 time: 0.05s
Test loss: 0.5923 score: 0.8605 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1183;  Loss pred: 0.0970; Loss self: 2.1263; time: 0.10s
Val loss: 0.5668 score: 0.8636 time: 0.05s
Test loss: 0.5593 score: 0.8605 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1048;  Loss pred: 0.0829; Loss self: 2.1866; time: 0.10s
Val loss: 0.5343 score: 0.8636 time: 0.11s
Test loss: 0.5225 score: 0.8605 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.0922;  Loss pred: 0.0694; Loss self: 2.2804; time: 0.10s
Val loss: 0.5011 score: 0.8636 time: 0.05s
Test loss: 0.4827 score: 0.8372 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.0857;  Loss pred: 0.0630; Loss self: 2.2724; time: 0.10s
Val loss: 0.4704 score: 0.8636 time: 0.05s
Test loss: 0.4436 score: 0.8605 time: 0.05s
Epoch 27/1000, LR 0.000450
Train loss: 0.0806;  Loss pred: 0.0576; Loss self: 2.3014; time: 0.10s
Val loss: 0.4452 score: 0.8636 time: 0.05s
Test loss: 0.4082 score: 0.8605 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.0739;  Loss pred: 0.0504; Loss self: 2.3475; time: 0.11s
Val loss: 0.4293 score: 0.8636 time: 0.05s
Test loss: 0.3802 score: 0.8605 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.0699;  Loss pred: 0.0461; Loss self: 2.3852; time: 0.10s
Val loss: 0.4217 score: 0.8636 time: 0.05s
Test loss: 0.3602 score: 0.8605 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0631;  Loss pred: 0.0389; Loss self: 2.4205; time: 0.10s
Val loss: 0.4219 score: 0.8636 time: 0.05s
Test loss: 0.3479 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 31/1000, LR 0.000450
Train loss: 0.0597;  Loss pred: 0.0351; Loss self: 2.4647; time: 0.10s
Val loss: 0.4292 score: 0.8636 time: 0.05s
Test loss: 0.3426 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 32/1000, LR 0.000450
Train loss: 0.0541;  Loss pred: 0.0302; Loss self: 2.3887; time: 0.11s
Val loss: 0.4401 score: 0.8636 time: 0.05s
Test loss: 0.3421 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0519;  Loss pred: 0.0269; Loss self: 2.4976; time: 0.10s
Val loss: 0.4534 score: 0.8409 time: 0.05s
Test loss: 0.3449 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0499;  Loss pred: 0.0248; Loss self: 2.5066; time: 0.10s
Val loss: 0.4674 score: 0.8182 time: 0.05s
Test loss: 0.3496 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0469;  Loss pred: 0.0218; Loss self: 2.5080; time: 0.10s
Val loss: 0.4812 score: 0.8182 time: 0.05s
Test loss: 0.3560 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0447;  Loss pred: 0.0195; Loss self: 2.5283; time: 0.11s
Val loss: 0.4948 score: 0.8182 time: 0.08s
Test loss: 0.3634 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0437;  Loss pred: 0.0182; Loss self: 2.5482; time: 0.10s
Val loss: 0.5070 score: 0.8182 time: 0.05s
Test loss: 0.3715 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0412;  Loss pred: 0.0157; Loss self: 2.5577; time: 0.10s
Val loss: 0.5178 score: 0.8182 time: 0.05s
Test loss: 0.3800 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0396;  Loss pred: 0.0142; Loss self: 2.5480; time: 0.11s
Val loss: 0.5268 score: 0.8182 time: 0.09s
Test loss: 0.3877 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0382;  Loss pred: 0.0126; Loss self: 2.5685; time: 0.09s
Val loss: 0.5320 score: 0.8182 time: 0.05s
Test loss: 0.3930 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0370;  Loss pred: 0.0113; Loss self: 2.5729; time: 0.09s
Val loss: 0.5379 score: 0.8182 time: 0.05s
Test loss: 0.3991 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0362;  Loss pred: 0.0106; Loss self: 2.5690; time: 0.10s
Val loss: 0.5411 score: 0.8409 time: 0.05s
Test loss: 0.4030 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0360;  Loss pred: 0.0102; Loss self: 2.5760; time: 0.10s
Val loss: 0.5442 score: 0.8409 time: 0.05s
Test loss: 0.4066 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0349;  Loss pred: 0.0090; Loss self: 2.5842; time: 0.10s
Val loss: 0.5481 score: 0.8409 time: 0.05s
Test loss: 0.4105 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0079; Loss self: 2.5652; time: 0.09s
Val loss: 0.5505 score: 0.8409 time: 0.05s
Test loss: 0.4133 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0076; Loss self: 2.5920; time: 0.08s
Val loss: 0.5550 score: 0.8409 time: 0.05s
Test loss: 0.4172 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0322;  Loss pred: 0.0070; Loss self: 2.5159; time: 0.10s
Val loss: 0.5589 score: 0.8409 time: 0.09s
Test loss: 0.4203 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0322;  Loss pred: 0.0063; Loss self: 2.5845; time: 0.09s
Val loss: 0.5627 score: 0.8409 time: 0.05s
Test loss: 0.4234 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0322;  Loss pred: 0.0062; Loss self: 2.5922; time: 0.09s
Val loss: 0.5663 score: 0.8409 time: 0.05s
Test loss: 0.4260 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 028,   Train_Loss: 0.0699,   Val_Loss: 0.4217,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.4217,   Test_Precision: 0.9000,   Test_Recall: 0.8182,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.3602


[0.05006738705560565, 0.04988181800581515, 0.05036134994588792, 0.05814485717564821, 0.0509454149287194, 0.0512829280924052, 0.06389820296317339, 0.05150386015884578, 0.05143662285991013, 0.05199134792201221, 0.051666269078850746, 0.056366337928920984, 0.05141389206983149, 0.05142602603882551, 0.05131084518507123, 0.0527197509072721, 0.05721406685188413, 0.05160485813394189, 0.05230263085104525, 0.052116221049800515, 0.05224407999776304, 0.05437243287451565, 0.05181463994085789, 0.05163630400784314, 0.05149108706973493, 0.06444668117910624, 0.05430006003007293, 0.053746509831398726, 0.053408721927553415, 0.05125955608673394, 0.05111938901245594, 0.051354585913941264, 0.05141197913326323, 0.051430467050522566, 0.05135799199342728, 0.05112447100691497, 0.052274893037974834, 0.050993934040889144, 0.05119234696030617, 0.052090574987232685, 0.05063384212553501, 0.05095842108130455, 0.051155711989849806, 0.05138216097839177, 0.05220359191298485, 0.05371546884998679, 0.053863049019128084, 0.05063412990421057, 0.05157917691394687, 0.05158593109808862, 0.06657919799908996, 0.052014298969879746, 0.054661905160173774, 0.05257646995596588, 0.1551974629983306, 0.051176650915294886, 0.07717586308717728, 0.05633824598044157, 0.05193333304487169, 0.05197237990796566, 0.05270265298895538, 0.0537515920586884, 0.05262195994146168, 0.059298183070495725, 0.05222792294807732, 0.052619192050769925, 0.053316516103222966, 0.053962920093908906, 0.05424968898296356, 0.05438511702232063, 0.10103111295029521, 0.05523778684437275, 0.055449035950005054, 0.05547906900756061, 0.056201465893536806, 0.06673699291422963, 0.05586067819967866, 0.05595686612650752, 0.05549790197983384, 0.0553837469778955, 0.0554701869841665, 0.0560542291495949, 0.056203399086371064, 0.05586409009993076, 0.05589008913375437, 0.05578393000178039, 0.052833949914202094, 0.05286312801763415, 0.05326846498064697, 0.05417562904767692, 0.05385227198712528, 0.05233057588338852, 0.05344440205954015, 0.049869796028360724, 0.050344658084213734, 0.050648623844608665, 0.05077068693935871, 0.05114308116026223, 0.05141018680296838, 0.09823096985928714, 0.05113668320700526, 0.04998790891841054, 0.05420976807363331, 0.05140981706790626]
[0.0011378951603546739, 0.0011336776819503443, 0.0011445761351338165, 0.0013214740267192776, 0.0011578503392890773, 0.0011655210930092092, 0.0014522318855266678, 0.0011705422763374042, 0.0011690141559070485, 0.0011816215436820958, 0.0011742333881556988, 0.0012810531347482042, 0.0011684975470416248, 0.0011687733190642161, 0.0011661555723879826, 0.0011981761569834568, 0.0013003197011791847, 0.0011728376848623157, 0.0011886961557055738, 0.001184459569313648, 0.0011873654544946146, 0.0012357371107844467, 0.0011776054532013156, 0.0011735523638146167, 0.0011702519788576121, 0.0014646972995251417, 0.0012340922734107483, 0.0012215115870772438, 0.0012138345892625775, 0.001164989911062135, 0.001161804295737635, 0.0011671496798623014, 0.001168454071210528, 0.00116887425114824, 0.0011672270907597108, 0.0011619197956117039, 0.0011880657508630645, 0.0011589530463838441, 0.0011634624309160492, 0.0011838767042552884, 0.0011507691392167048, 0.0011581459336660125, 0.001162629817951132, 0.0011677763858725402, 0.0011864452707496557, 0.0012208061102269726, 0.0012241602049801838, 0.0011507756796411493, 0.0011722540207715197, 0.0011724075249565597, 0.0015131635908884082, 0.0011821431584063578, 0.0012423160263675857, 0.0011949197717264972, 0.003527215068143877, 0.0011901546724487184, 0.0017947875136552856, 0.0013101917669870132, 0.0012077519312760857, 0.0012086599978596666, 0.0012256430927664042, 0.0012500370246206606, 0.0012237665102665508, 0.0013790275132673424, 0.001214602859257612, 0.0012237021407155797, 0.0012399189791447202, 0.0012549516300909048, 0.0012616206740224084, 0.001264770163309782, 0.002349560766285935, 0.0012845996940551803, 0.0012895124639536058, 0.0012902109071525724, 0.001307010834733414, 0.0015520230910285962, 0.0012990855395274108, 0.0013013224680583145, 0.0012906488832519498, 0.0012879941157650115, 0.0012900043484689885, 0.0013035867244091837, 0.001307055792706304, 0.0012991648860449015, 0.0012997695147384737, 0.001297300697715823, 0.0012286965096326068, 0.0012293750701775385, 0.0012388015111778365, 0.001259898349945975, 0.001252378418305239, 0.0012169901368229887, 0.0012428930711520965, 0.0011597626983339702, 0.001170806001958459, 0.001177874973130434, 0.0011807136497525282, 0.0011893739804712145, 0.001195585739603916, 0.0022844411595183056, 0.0011892251908605875, 0.0011625095097304776, 0.00126069228078217, 0.001195577141114099]
[878.8155841073329, 882.0849311240129, 873.6858731403538, 756.7307262804276, 863.6694796099488, 857.9853303367876, 688.5952649616552, 854.3048980075914, 855.4216345003035, 846.2946578342352, 851.6194566487696, 780.6077459828033, 855.7998281911476, 855.5979022524699, 857.5185195507498, 834.6018189158533, 769.0416434459602, 852.6329030068591, 841.2578733431088, 844.2668926044172, 842.2006857405465, 809.2336074338655, 849.1808502427567, 852.1136600581784, 854.5168203656359, 682.7349243589118, 810.3121796850969, 818.6578093726783, 823.835478775996, 858.376532281115, 860.7301622732384, 856.7881371633315, 855.8316707853068, 855.524021525543, 856.7313146828454, 860.6446019568333, 841.7042569180662, 862.8477254710118, 859.5034729335028, 844.6825555445362, 868.9840263536012, 863.4490446593246, 860.1190031082037, 856.328328006752, 842.8538801188442, 819.1308936142856, 816.8865446955022, 868.9790874897824, 853.0574280665289, 852.9457366260527, 660.8670774406357, 845.9212345720426, 804.9481603516822, 836.8762687348753, 283.5097890773723, 840.226924406826, 557.16901994899, 763.2470491702549, 827.9846002344377, 827.362535180143, 815.8982055231885, 799.9763049446176, 817.1493431228056, 725.1486938289512, 823.3143799869026, 817.1923270602712, 806.5043094104317, 796.8434607535935, 792.6312722917843, 790.6574878261645, 425.61146506576574, 778.45262195512, 775.4868820220864, 775.0670796970298, 765.104598542954, 644.320310554951, 769.7722509972563, 768.4490389934529, 774.8040640459674, 776.401062520417, 775.1911853528453, 767.1142865107219, 765.078281723128, 769.7252371439465, 769.3671752266091, 770.8313128642535, 813.8706280682856, 813.4214075575702, 807.2318212214748, 793.7148263134725, 798.4807031034868, 821.6993463977843, 804.5744426534236, 862.2453553960015, 854.1124646843763, 848.9865417059534, 846.9454047639706, 840.7784401032659, 836.41011001962, 437.7438201169772, 840.8836338863173, 860.2080169063264, 793.2149781860892, 836.4161254104855]
Elapsed: 0.055454130900146156~0.01231668369326763
Time per graph: 0.001274336461146253~0.00028325138292829063
Speed: 803.6015458435579~90.16044782592937
Total Time: 0.0520
best val loss: 0.42165911197662354 test_score: 0.8605

Testing...
Test loss: 0.6188 score: 0.8837 time: 0.05s
test Score 0.8837
Epoch Time List: [0.20598831516690552, 0.1889060470275581, 0.19434524211101234, 0.20407130289822817, 0.1948706021066755, 0.19952097814530134, 0.21452533057890832, 0.1995694967918098, 0.1971678843256086, 0.19781315210275352, 0.19818984693847597, 0.2030402917880565, 0.19184703286737204, 0.18939315318129957, 0.18801192264072597, 0.18970888410694897, 0.2094854200258851, 0.2024341062642634, 0.2032255621161312, 0.20367946196347475, 0.20513171795755625, 0.20893152616918087, 0.20500754029490054, 0.20378793007694185, 0.2006254573352635, 0.2158158551901579, 0.20656220288947225, 0.20896726078353822, 0.2054621148854494, 0.20114277000539005, 0.19944584695622325, 0.20120167802087963, 0.20097359106875956, 0.20074720005504787, 0.20142537192441523, 0.2010243369732052, 0.21251934301108122, 0.20238774688914418, 0.206558144884184, 0.2719440208747983, 0.18938195891678333, 0.1952411187812686, 0.19714630022644997, 0.20706995809450746, 0.19427080010063946, 0.20124458987265825, 0.20334387430921197, 0.21064515528269112, 0.19467431167140603, 0.20025135297328234, 0.21675460506230593, 0.20157025614753366, 0.20368955773301423, 0.1968414280563593, 0.3120222573634237, 0.19295505690388381, 0.24988579005002975, 0.19446129095740616, 0.19278544327244163, 0.19503771513700485, 0.2004048121161759, 0.20364685612730682, 0.20021329494193196, 0.20761268120259047, 0.3447776618413627, 0.18341297889128327, 0.2086682510562241, 0.21293834992684424, 0.19771525007672608, 0.214487228076905, 0.2578970792237669, 0.21293100202456117, 0.20291357510723174, 0.20513226022012532, 0.20685277902521193, 0.22558554774150252, 0.19688654108904302, 0.20402577496133745, 0.2596892360597849, 0.19893386587500572, 0.20318420114926994, 0.20592661201953888, 0.20910387486219406, 0.20239709899760783, 0.20274683716706932, 0.199489772785455, 0.2116155819967389, 0.19682189193554223, 0.19766914588399231, 0.20258366991765797, 0.237935746088624, 0.1946332121733576, 0.19724903674796224, 0.25280143367126584, 0.18360784417018294, 0.18373366678133607, 0.1915550099220127, 0.19167902693152428, 0.19414973189122975, 0.23558701411820948, 0.17709712218493223, 0.2358982590958476, 0.18258590577170253, 0.1810337568167597]
Total Epoch List: [55, 49]
Total Time List: [0.155637081945315, 0.05195684917271137]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SAMamba
GraphEncoder(
  (subgraph_conv): GATConv(64, 64, heads=1)
  (embedding): Linear(in_features=14887, out_features=64, bias=False)
  (encoder): GraphMambaEncoder(
    (layers): ModuleList(
      (0-1): 2 x MambaEncoderLayer(
        (self_selen): Selection(
          (khop_structure_extractor): KHopStructureExtractor(
            (structure_extractor): StructureExtractor(
              (gcn): ModuleList(
                (0): GATConv(64, 64, heads=1)
              )
              (relu): ReLU()
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (out_proj): Linear(in_features=64, out_features=64, bias=True)
            )
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=True)
          )
          (out_proj): Mamba(
            (in_proj): Linear(in_features=64, out_features=256, bias=False)
            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)
            (act): SiLU()
            (x_proj): Linear(in_features=128, out_features=36, bias=False)
            (dt_proj): Linear(in_features=4, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=64, bias=False)
          )
        )
        (norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (linear1): Linear(in_features=64, out_features=128, bias=True)
        (dropout): Dropout(p=0.5, inplace=False)
        (linear2): Linear(in_features=128, out_features=64, bias=True)
        (dropout1): Dropout(p=0.5, inplace=False)
        (dropout2): Dropout(p=0.5, inplace=False)
        (activation): ReLU()
      )
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x79c442356da0>
Training...
Epoch 1/1000, LR 0.000500
Train loss: 0.7194;  Loss pred: 0.7087; Loss self: 1.0694; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6968 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6946 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7255;  Loss pred: 0.7133; Loss self: 1.2270; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6970 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6947 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000050
Train loss: 0.6936;  Loss pred: 0.6838; Loss self: 0.9783; time: 0.09s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6965 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6942 score: 0.5116 time: 0.06s
Epoch 4/1000, LR 0.000100
Train loss: 0.6609;  Loss pred: 0.6507; Loss self: 1.0199; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6961 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6939 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000150
Train loss: 0.6417;  Loss pred: 0.6315; Loss self: 1.0205; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6958 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6936 score: 0.5116 time: 0.05s
Epoch 6/1000, LR 0.000200
Train loss: 0.5973;  Loss pred: 0.5871; Loss self: 1.0258; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6952 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6931 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000250
Train loss: 0.5461;  Loss pred: 0.5362; Loss self: 0.9952; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6946 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6927 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000300
Train loss: 0.5332;  Loss pred: 0.5234; Loss self: 0.9772; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6941 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6923 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000350
Train loss: 0.4982;  Loss pred: 0.4870; Loss self: 1.1181; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6916 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000400
Train loss: 0.4548;  Loss pred: 0.4436; Loss self: 1.1223; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6909 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000450
Train loss: 0.4275;  Loss pred: 0.4153; Loss self: 1.2260; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6912 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6899 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000450
Train loss: 0.3881;  Loss pred: 0.3755; Loss self: 1.2596; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6898 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6886 score: 0.5116 time: 0.05s
Epoch 13/1000, LR 0.000450
Train loss: 0.3609;  Loss pred: 0.3478; Loss self: 1.3166; time: 0.10s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Val loss: 0.6877 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMZX/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 0.6868 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000450
Train loss: 0.3374;  Loss pred: 0.3227; Loss self: 1.4677; time: 0.10s
Val loss: 0.6852 score: 0.5909 time: 0.05s
Test loss: 0.6843 score: 0.5814 time: 0.05s
Epoch 15/1000, LR 0.000450
Train loss: 0.2965;  Loss pred: 0.2821; Loss self: 1.4413; time: 0.10s
Val loss: 0.6820 score: 0.7500 time: 0.05s
Test loss: 0.6811 score: 0.7442 time: 0.05s
Epoch 16/1000, LR 0.000450
Train loss: 0.2767;  Loss pred: 0.2612; Loss self: 1.5468; time: 0.10s
Val loss: 0.6775 score: 0.8409 time: 0.05s
Test loss: 0.6763 score: 0.8837 time: 0.05s
Epoch 17/1000, LR 0.000450
Train loss: 0.2509;  Loss pred: 0.2348; Loss self: 1.6169; time: 0.10s
Val loss: 0.6713 score: 0.8182 time: 0.05s
Test loss: 0.6699 score: 0.9302 time: 0.05s
Epoch 18/1000, LR 0.000450
Train loss: 0.2291;  Loss pred: 0.2122; Loss self: 1.6906; time: 0.10s
Val loss: 0.6635 score: 0.8409 time: 0.05s
Test loss: 0.6615 score: 0.8837 time: 0.05s
Epoch 19/1000, LR 0.000450
Train loss: 0.2004;  Loss pred: 0.1824; Loss self: 1.7923; time: 0.11s
Val loss: 0.6528 score: 0.7727 time: 0.05s
Test loss: 0.6500 score: 0.8605 time: 0.05s
Epoch 20/1000, LR 0.000450
Train loss: 0.1903;  Loss pred: 0.1726; Loss self: 1.7710; time: 0.10s
Val loss: 0.6380 score: 0.7727 time: 0.05s
Test loss: 0.6338 score: 0.8837 time: 0.05s
Epoch 21/1000, LR 0.000450
Train loss: 0.1666;  Loss pred: 0.1480; Loss self: 1.8617; time: 0.10s
Val loss: 0.6183 score: 0.7955 time: 0.05s
Test loss: 0.6124 score: 0.9070 time: 0.05s
Epoch 22/1000, LR 0.000450
Train loss: 0.1556;  Loss pred: 0.1358; Loss self: 1.9810; time: 0.10s
Val loss: 0.5929 score: 0.8636 time: 0.05s
Test loss: 0.5845 score: 0.9070 time: 0.05s
Epoch 23/1000, LR 0.000450
Train loss: 0.1341;  Loss pred: 0.1140; Loss self: 2.0118; time: 0.10s
Val loss: 0.5631 score: 0.8636 time: 0.05s
Test loss: 0.5500 score: 0.8837 time: 0.05s
Epoch 24/1000, LR 0.000450
Train loss: 0.1199;  Loss pred: 0.0994; Loss self: 2.0464; time: 0.10s
Val loss: 0.5289 score: 0.8636 time: 0.05s
Test loss: 0.5104 score: 0.8837 time: 0.05s
Epoch 25/1000, LR 0.000450
Train loss: 0.1062;  Loss pred: 0.0853; Loss self: 2.0865; time: 0.10s
Val loss: 0.4920 score: 0.8864 time: 0.05s
Test loss: 0.4674 score: 0.8837 time: 0.05s
Epoch 26/1000, LR 0.000450
Train loss: 0.0993;  Loss pred: 0.0781; Loss self: 2.1198; time: 0.10s
Val loss: 0.4549 score: 0.8636 time: 0.05s
Test loss: 0.4247 score: 0.9070 time: 0.05s
Epoch 27/1000, LR 0.000450
Train loss: 0.0893;  Loss pred: 0.0678; Loss self: 2.1495; time: 0.10s
Val loss: 0.4209 score: 0.8864 time: 0.05s
Test loss: 0.3850 score: 0.9070 time: 0.05s
Epoch 28/1000, LR 0.000450
Train loss: 0.0825;  Loss pred: 0.0603; Loss self: 2.2208; time: 0.10s
Val loss: 0.3937 score: 0.8864 time: 0.05s
Test loss: 0.3537 score: 0.9070 time: 0.05s
Epoch 29/1000, LR 0.000450
Train loss: 0.0771;  Loss pred: 0.0549; Loss self: 2.2243; time: 0.11s
Val loss: 0.3755 score: 0.8864 time: 0.05s
Test loss: 0.3324 score: 0.9070 time: 0.05s
Epoch 30/1000, LR 0.000450
Train loss: 0.0692;  Loss pred: 0.0463; Loss self: 2.2925; time: 0.10s
Val loss: 0.3654 score: 0.8864 time: 0.05s
Test loss: 0.3198 score: 0.9070 time: 0.05s
Epoch 31/1000, LR 0.000450
Train loss: 0.0654;  Loss pred: 0.0417; Loss self: 2.3670; time: 0.10s
Val loss: 0.3653 score: 0.8864 time: 0.05s
Test loss: 0.3184 score: 0.9070 time: 0.06s
Epoch 32/1000, LR 0.000450
Train loss: 0.0594;  Loss pred: 0.0362; Loss self: 2.3199; time: 0.10s
Val loss: 0.3723 score: 0.8864 time: 0.05s
Test loss: 0.3245 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000449
Train loss: 0.0558;  Loss pred: 0.0321; Loss self: 2.3680; time: 0.10s
Val loss: 0.3848 score: 0.8864 time: 0.06s
Test loss: 0.3366 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000449
Train loss: 0.0517;  Loss pred: 0.0277; Loss self: 2.3949; time: 0.09s
Val loss: 0.4006 score: 0.8864 time: 0.05s
Test loss: 0.3523 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000449
Train loss: 0.0493;  Loss pred: 0.0255; Loss self: 2.3851; time: 0.09s
Val loss: 0.4190 score: 0.8864 time: 0.05s
Test loss: 0.3703 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000449
Train loss: 0.0456;  Loss pred: 0.0216; Loss self: 2.3995; time: 0.09s
Val loss: 0.4387 score: 0.8864 time: 0.05s
Test loss: 0.3897 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000449
Train loss: 0.0442;  Loss pred: 0.0199; Loss self: 2.4285; time: 0.09s
Val loss: 0.4585 score: 0.8864 time: 0.05s
Test loss: 0.4089 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000449
Train loss: 0.0427;  Loss pred: 0.0182; Loss self: 2.4463; time: 0.10s
Val loss: 0.4774 score: 0.8864 time: 0.05s
Test loss: 0.4265 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 39/1000, LR 0.000449
Train loss: 0.0399;  Loss pred: 0.0154; Loss self: 2.4521; time: 0.10s
Val loss: 0.4937 score: 0.8864 time: 0.05s
Test loss: 0.4412 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 40/1000, LR 0.000449
Train loss: 0.0382;  Loss pred: 0.0133; Loss self: 2.4939; time: 0.09s
Val loss: 0.5086 score: 0.8864 time: 0.05s
Test loss: 0.4547 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 41/1000, LR 0.000449
Train loss: 0.0367;  Loss pred: 0.0116; Loss self: 2.5116; time: 0.09s
Val loss: 0.5245 score: 0.8864 time: 0.05s
Test loss: 0.4697 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 42/1000, LR 0.000449
Train loss: 0.0362;  Loss pred: 0.0114; Loss self: 2.4762; time: 0.10s
Val loss: 0.5359 score: 0.8864 time: 0.05s
Test loss: 0.4796 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 43/1000, LR 0.000449
Train loss: 0.0350;  Loss pred: 0.0096; Loss self: 2.5425; time: 0.09s
Val loss: 0.5477 score: 0.8864 time: 0.05s
Test loss: 0.4899 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 44/1000, LR 0.000449
Train loss: 0.0335;  Loss pred: 0.0085; Loss self: 2.5066; time: 0.09s
Val loss: 0.5558 score: 0.8864 time: 0.05s
Test loss: 0.4965 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 45/1000, LR 0.000449
Train loss: 0.0336;  Loss pred: 0.0085; Loss self: 2.5057; time: 0.10s
Val loss: 0.5648 score: 0.8864 time: 0.05s
Test loss: 0.5043 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 46/1000, LR 0.000449
Train loss: 0.0325;  Loss pred: 0.0072; Loss self: 2.5233; time: 0.09s
Val loss: 0.5732 score: 0.8864 time: 0.05s
Test loss: 0.5126 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 47/1000, LR 0.000449
Train loss: 0.0315;  Loss pred: 0.0065; Loss self: 2.5070; time: 0.09s
Val loss: 0.5782 score: 0.8864 time: 0.05s
Test loss: 0.5164 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 48/1000, LR 0.000448
Train loss: 0.0313;  Loss pred: 0.0063; Loss self: 2.4988; time: 0.09s
Val loss: 0.5826 score: 0.8864 time: 0.05s
Test loss: 0.5198 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 49/1000, LR 0.000448
Train loss: 0.0310;  Loss pred: 0.0056; Loss self: 2.5412; time: 0.09s
Val loss: 0.5856 score: 0.8864 time: 0.05s
Test loss: 0.5219 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 50/1000, LR 0.000448
Train loss: 0.0304;  Loss pred: 0.0052; Loss self: 2.5205; time: 0.10s
Val loss: 0.5849 score: 0.8864 time: 0.05s
Test loss: 0.5204 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 51/1000, LR 0.000448
Train loss: 0.0302;  Loss pred: 0.0048; Loss self: 2.5362; time: 0.09s
Val loss: 0.5852 score: 0.8864 time: 0.05s
Test loss: 0.5202 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 030,   Train_Loss: 0.0654,   Val_Loss: 0.3653,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.3653,   Test_Precision: 1.0000,   Test_Recall: 0.8095,   Test_accuracy: 0.8947,   Test_Score: 0.9070,   Test_loss: 0.3184


[0.05006738705560565, 0.04988181800581515, 0.05036134994588792, 0.05814485717564821, 0.0509454149287194, 0.0512829280924052, 0.06389820296317339, 0.05150386015884578, 0.05143662285991013, 0.05199134792201221, 0.051666269078850746, 0.056366337928920984, 0.05141389206983149, 0.05142602603882551, 0.05131084518507123, 0.0527197509072721, 0.05721406685188413, 0.05160485813394189, 0.05230263085104525, 0.052116221049800515, 0.05224407999776304, 0.05437243287451565, 0.05181463994085789, 0.05163630400784314, 0.05149108706973493, 0.06444668117910624, 0.05430006003007293, 0.053746509831398726, 0.053408721927553415, 0.05125955608673394, 0.05111938901245594, 0.051354585913941264, 0.05141197913326323, 0.051430467050522566, 0.05135799199342728, 0.05112447100691497, 0.052274893037974834, 0.050993934040889144, 0.05119234696030617, 0.052090574987232685, 0.05063384212553501, 0.05095842108130455, 0.051155711989849806, 0.05138216097839177, 0.05220359191298485, 0.05371546884998679, 0.053863049019128084, 0.05063412990421057, 0.05157917691394687, 0.05158593109808862, 0.06657919799908996, 0.052014298969879746, 0.054661905160173774, 0.05257646995596588, 0.1551974629983306, 0.051176650915294886, 0.07717586308717728, 0.05633824598044157, 0.05193333304487169, 0.05197237990796566, 0.05270265298895538, 0.0537515920586884, 0.05262195994146168, 0.059298183070495725, 0.05222792294807732, 0.052619192050769925, 0.053316516103222966, 0.053962920093908906, 0.05424968898296356, 0.05438511702232063, 0.10103111295029521, 0.05523778684437275, 0.055449035950005054, 0.05547906900756061, 0.056201465893536806, 0.06673699291422963, 0.05586067819967866, 0.05595686612650752, 0.05549790197983384, 0.0553837469778955, 0.0554701869841665, 0.0560542291495949, 0.056203399086371064, 0.05586409009993076, 0.05589008913375437, 0.05578393000178039, 0.052833949914202094, 0.05286312801763415, 0.05326846498064697, 0.05417562904767692, 0.05385227198712528, 0.05233057588338852, 0.05344440205954015, 0.049869796028360724, 0.050344658084213734, 0.050648623844608665, 0.05077068693935871, 0.05114308116026223, 0.05141018680296838, 0.09823096985928714, 0.05113668320700526, 0.04998790891841054, 0.05420976807363331, 0.05140981706790626, 0.05285520595498383, 0.05320506892167032, 0.0664949850179255, 0.0577839189209044, 0.05444178800098598, 0.05468149296939373, 0.054692697012797, 0.05402638181112707, 0.05361834494397044, 0.05344782187603414, 0.05268490011803806, 0.052799447905272245, 0.05293326498940587, 0.05406237905845046, 0.05340426601469517, 0.05284039885737002, 0.05284911999478936, 0.05345394788309932, 0.05743369203992188, 0.05350071098655462, 0.053694973001256585, 0.053171472856774926, 0.05335905798710883, 0.05312030902132392, 0.053226036950945854, 0.05535277002491057, 0.05648558400571346, 0.052967390045523643, 0.05328579409979284, 0.05343655892647803, 0.06014656904153526, 0.06799520505592227, 0.05561846401542425, 0.051942668156698346, 0.05194635596126318, 0.05250495416112244, 0.054028116865083575, 0.0521178871858865, 0.052016713889315724, 0.051973267924040556, 0.051752666011452675, 0.05181068810634315, 0.05175490817055106, 0.05185218504630029, 0.05150556797161698, 0.05188793898560107, 0.05174570297822356, 0.052129452116787434, 0.05301064089871943, 0.05246939416974783, 0.05216358997859061]
[0.0011378951603546739, 0.0011336776819503443, 0.0011445761351338165, 0.0013214740267192776, 0.0011578503392890773, 0.0011655210930092092, 0.0014522318855266678, 0.0011705422763374042, 0.0011690141559070485, 0.0011816215436820958, 0.0011742333881556988, 0.0012810531347482042, 0.0011684975470416248, 0.0011687733190642161, 0.0011661555723879826, 0.0011981761569834568, 0.0013003197011791847, 0.0011728376848623157, 0.0011886961557055738, 0.001184459569313648, 0.0011873654544946146, 0.0012357371107844467, 0.0011776054532013156, 0.0011735523638146167, 0.0011702519788576121, 0.0014646972995251417, 0.0012340922734107483, 0.0012215115870772438, 0.0012138345892625775, 0.001164989911062135, 0.001161804295737635, 0.0011671496798623014, 0.001168454071210528, 0.00116887425114824, 0.0011672270907597108, 0.0011619197956117039, 0.0011880657508630645, 0.0011589530463838441, 0.0011634624309160492, 0.0011838767042552884, 0.0011507691392167048, 0.0011581459336660125, 0.001162629817951132, 0.0011677763858725402, 0.0011864452707496557, 0.0012208061102269726, 0.0012241602049801838, 0.0011507756796411493, 0.0011722540207715197, 0.0011724075249565597, 0.0015131635908884082, 0.0011821431584063578, 0.0012423160263675857, 0.0011949197717264972, 0.003527215068143877, 0.0011901546724487184, 0.0017947875136552856, 0.0013101917669870132, 0.0012077519312760857, 0.0012086599978596666, 0.0012256430927664042, 0.0012500370246206606, 0.0012237665102665508, 0.0013790275132673424, 0.001214602859257612, 0.0012237021407155797, 0.0012399189791447202, 0.0012549516300909048, 0.0012616206740224084, 0.001264770163309782, 0.002349560766285935, 0.0012845996940551803, 0.0012895124639536058, 0.0012902109071525724, 0.001307010834733414, 0.0015520230910285962, 0.0012990855395274108, 0.0013013224680583145, 0.0012906488832519498, 0.0012879941157650115, 0.0012900043484689885, 0.0013035867244091837, 0.001307055792706304, 0.0012991648860449015, 0.0012997695147384737, 0.001297300697715823, 0.0012286965096326068, 0.0012293750701775385, 0.0012388015111778365, 0.001259898349945975, 0.001252378418305239, 0.0012169901368229887, 0.0012428930711520965, 0.0011597626983339702, 0.001170806001958459, 0.001177874973130434, 0.0011807136497525282, 0.0011893739804712145, 0.001195585739603916, 0.0022844411595183056, 0.0011892251908605875, 0.0011625095097304776, 0.00126069228078217, 0.001195577141114099, 0.0012291908361624147, 0.001237327184224891, 0.0015463950004168722, 0.0013438120679280094, 0.0012660880930461856, 0.0012716626271952031, 0.0012719231863441162, 0.0012564274839796993, 0.0012469382545109405, 0.001242972601768236, 0.0012252302353032106, 0.0012278941373319128, 0.0012310061625443226, 0.0012572646292662898, 0.0012419596747603528, 0.0012288464850551168, 0.0012290493022044037, 0.0012431150670488213, 0.0013356672567423694, 0.0012442025810826657, 0.0012487203023548042, 0.0012365458803901145, 0.0012409083252816007, 0.001235356023751719, 0.0012378148128126944, 0.001287273721509548, 0.0013136182326910107, 0.0012317997685005499, 0.0012392045139486706, 0.0012427106727087914, 0.0013987574195705873, 0.0015812838385098201, 0.0012934526515214941, 0.0012079690268999616, 0.001208054789796818, 0.0012210454456074986, 0.001256467834071711, 0.001212043888043872, 0.001209691020681761, 0.001208680649396292, 0.0012035503723593645, 0.0012048997234033292, 0.0012036025155942108, 0.0012058647685186114, 0.001197803906316674, 0.0012066962554790946, 0.0012033884413540363, 0.0012123128399252892, 0.0012328056022958007, 0.001220218469063903, 0.0012131067436881536]
[878.8155841073329, 882.0849311240129, 873.6858731403538, 756.7307262804276, 863.6694796099488, 857.9853303367876, 688.5952649616552, 854.3048980075914, 855.4216345003035, 846.2946578342352, 851.6194566487696, 780.6077459828033, 855.7998281911476, 855.5979022524699, 857.5185195507498, 834.6018189158533, 769.0416434459602, 852.6329030068591, 841.2578733431088, 844.2668926044172, 842.2006857405465, 809.2336074338655, 849.1808502427567, 852.1136600581784, 854.5168203656359, 682.7349243589118, 810.3121796850969, 818.6578093726783, 823.835478775996, 858.376532281115, 860.7301622732384, 856.7881371633315, 855.8316707853068, 855.524021525543, 856.7313146828454, 860.6446019568333, 841.7042569180662, 862.8477254710118, 859.5034729335028, 844.6825555445362, 868.9840263536012, 863.4490446593246, 860.1190031082037, 856.328328006752, 842.8538801188442, 819.1308936142856, 816.8865446955022, 868.9790874897824, 853.0574280665289, 852.9457366260527, 660.8670774406357, 845.9212345720426, 804.9481603516822, 836.8762687348753, 283.5097890773723, 840.226924406826, 557.16901994899, 763.2470491702549, 827.9846002344377, 827.362535180143, 815.8982055231885, 799.9763049446176, 817.1493431228056, 725.1486938289512, 823.3143799869026, 817.1923270602712, 806.5043094104317, 796.8434607535935, 792.6312722917843, 790.6574878261645, 425.61146506576574, 778.45262195512, 775.4868820220864, 775.0670796970298, 765.104598542954, 644.320310554951, 769.7722509972563, 768.4490389934529, 774.8040640459674, 776.401062520417, 775.1911853528453, 767.1142865107219, 765.078281723128, 769.7252371439465, 769.3671752266091, 770.8313128642535, 813.8706280682856, 813.4214075575702, 807.2318212214748, 793.7148263134725, 798.4807031034868, 821.6993463977843, 804.5744426534236, 862.2453553960015, 854.1124646843763, 848.9865417059534, 846.9454047639706, 840.7784401032659, 836.41011001962, 437.7438201169772, 840.8836338863173, 860.2080169063264, 793.2149781860892, 836.4161254104855, 813.5433250723231, 808.193671608725, 646.6653084951926, 744.1516740818345, 789.8344558268593, 786.3720916337803, 786.2109997965333, 795.9074540717047, 801.9643285322161, 804.5229625958076, 816.1731331683368, 814.4024550625323, 812.3436181124681, 795.3775018578043, 805.1791216111417, 813.7712986623774, 813.6370104978014, 804.4307614853539, 748.6894621037231, 803.7276366440517, 800.8198458167341, 808.7043237607267, 805.8613030685156, 809.4832427036267, 807.8752892992885, 776.8355581960683, 761.2561816772682, 811.820253235868, 806.9693006633296, 804.6925337981172, 714.9202470768632, 632.397533982505, 773.12455065417, 827.8357952325341, 827.7770250537968, 818.9703369332635, 795.881894373212, 825.0526320576629, 826.6573719266075, 827.348398850827, 830.8750700975339, 829.9445842475799, 830.8390743984999, 829.2803854187451, 834.8611944964063, 828.7089608999988, 830.9868747574254, 824.8695939421269, 811.1578971881237, 819.5253762772131, 824.329767518846]
Elapsed: 0.05496072471292029~0.01027487436347832
Time per graph: 0.0012678942792398332~0.00023600167494932362
Speed: 802.1891705564778~77.37929470907561
Total Time: 0.0528
best val loss: 0.3652537167072296 test_score: 0.9070

Testing...
Test loss: 0.4674 score: 0.8837 time: 0.05s
test Score 0.8837
Epoch Time List: [0.20598831516690552, 0.1889060470275581, 0.19434524211101234, 0.20407130289822817, 0.1948706021066755, 0.19952097814530134, 0.21452533057890832, 0.1995694967918098, 0.1971678843256086, 0.19781315210275352, 0.19818984693847597, 0.2030402917880565, 0.19184703286737204, 0.18939315318129957, 0.18801192264072597, 0.18970888410694897, 0.2094854200258851, 0.2024341062642634, 0.2032255621161312, 0.20367946196347475, 0.20513171795755625, 0.20893152616918087, 0.20500754029490054, 0.20378793007694185, 0.2006254573352635, 0.2158158551901579, 0.20656220288947225, 0.20896726078353822, 0.2054621148854494, 0.20114277000539005, 0.19944584695622325, 0.20120167802087963, 0.20097359106875956, 0.20074720005504787, 0.20142537192441523, 0.2010243369732052, 0.21251934301108122, 0.20238774688914418, 0.206558144884184, 0.2719440208747983, 0.18938195891678333, 0.1952411187812686, 0.19714630022644997, 0.20706995809450746, 0.19427080010063946, 0.20124458987265825, 0.20334387430921197, 0.21064515528269112, 0.19467431167140603, 0.20025135297328234, 0.21675460506230593, 0.20157025614753366, 0.20368955773301423, 0.1968414280563593, 0.3120222573634237, 0.19295505690388381, 0.24988579005002975, 0.19446129095740616, 0.19278544327244163, 0.19503771513700485, 0.2004048121161759, 0.20364685612730682, 0.20021329494193196, 0.20761268120259047, 0.3447776618413627, 0.18341297889128327, 0.2086682510562241, 0.21293834992684424, 0.19771525007672608, 0.214487228076905, 0.2578970792237669, 0.21293100202456117, 0.20291357510723174, 0.20513226022012532, 0.20685277902521193, 0.22558554774150252, 0.19688654108904302, 0.20402577496133745, 0.2596892360597849, 0.19893386587500572, 0.20318420114926994, 0.20592661201953888, 0.20910387486219406, 0.20239709899760783, 0.20274683716706932, 0.199489772785455, 0.2116155819967389, 0.19682189193554223, 0.19766914588399231, 0.20258366991765797, 0.237935746088624, 0.1946332121733576, 0.19724903674796224, 0.25280143367126584, 0.18360784417018294, 0.18373366678133607, 0.1915550099220127, 0.19167902693152428, 0.19414973189122975, 0.23558701411820948, 0.17709712218493223, 0.2358982590958476, 0.18258590577170253, 0.1810337568167597, 0.1875889201182872, 0.18824110901914537, 0.20293209701776505, 0.20844787638634443, 0.19356762920506299, 0.19689704896882176, 0.19813842000439763, 0.19512183428741992, 0.19434909312985837, 0.19291515299119055, 0.1924566759262234, 0.19159049890004098, 0.19121197005733848, 0.19239137298427522, 0.1955970369745046, 0.19294731691479683, 0.19194650719873607, 0.19760910794138908, 0.21309327683411539, 0.19293956202454865, 0.19242275902070105, 0.19159382907673717, 0.1913814831059426, 0.19329236820340157, 0.1928612261544913, 0.19441577466204762, 0.19480119924992323, 0.19615711993537843, 0.20461722323670983, 0.19913022476248443, 0.19905507774092257, 0.20983907696790993, 0.20888865971937776, 0.18741549900732934, 0.18805176089517772, 0.18803122895769775, 0.18956768955104053, 0.19666781113483012, 0.1940653643105179, 0.18788539990782738, 0.187809290131554, 0.18934959592297673, 0.18855217285454273, 0.1882902579382062, 0.18817465496249497, 0.1877612709067762, 0.1876574601046741, 0.1877205539494753, 0.19012027303688228, 0.19039149722084403, 0.18862030329182744]
Total Epoch List: [55, 49, 51]
Total Time List: [0.155637081945315, 0.05195684917271137, 0.052841312950477004]
T-times Epoch Time: 0.19899091802697666 ~ 0.005158613927074532
T-times Total Epoch: 52.77777777777778 ~ 0.7856742013183884
T-times Total Time: 0.06478398244103624 ~ 0.015576778373070636
T-times Inference Elapsed: 0.054429115009179935 ~ 0.0007826109693406664
T-times Time Per Graph: 0.0012556943768276968 ~ 1.7795433875142233e-05
T-times Speed: 807.0108147586998 ~ 6.22233948450006
T-times cross validation test micro f1 score:0.8616636908174575 ~ 0.009605737622602238
T-times cross validation test precision:0.9503220611916263 ~ 0.019201280862170416
T-times cross validation test recall:0.7941317941317941 ~ 0.038833156117247525
T-times cross validation test f1_score:0.8616636908174575 ~ 0.016666013546647485
